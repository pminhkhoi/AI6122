{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import PyPDF2 as pdf\n",
    "import seaborn as sns\n",
    "import random\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from pprint import pprint\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "data_path = '../Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_pdf_to_text(file_path, doc_read_counter = True):\n",
    "    '''\n",
    "    Uses PyPDF2\n",
    "    Function only takes in the directory of a folder anc converts all PDF files to text\n",
    "    '''\n",
    "    files = os.listdir(file_path)\n",
    "    pdf_ls = [i for i in files if i.endswith('.pdf')]\n",
    "\n",
    "    text_dic = {'No': [],\n",
    "                'PDF_title': [],\n",
    "                'Text': [],\n",
    "                'Page_length': [],\n",
    "                'Text_length': []\n",
    "               }\n",
    "\n",
    "    for idx, file in enumerate(pdf_ls):\n",
    "        max_files = len(pdf_ls)\n",
    "        with open(os.path.join(file_path, file), 'rb') as pdf_file:\n",
    "            # Create a PDF reader object\n",
    "            reader = pdf.PdfReader(pdf_file)\n",
    "\n",
    "            # Get the total number of pages in the PDF document\n",
    "            total_pages = len(reader.pages)\n",
    "\n",
    "            text = []\n",
    "            text_counter = 0\n",
    "            # Loop through all the pages and extract the text\n",
    "            for page_num in range(total_pages):\n",
    "                # Get the page object\n",
    "                page_obj = reader.pages[page_num]\n",
    "\n",
    "                # Extract the text from the page\n",
    "                page_text = page_obj.extract_text ()\n",
    "                text_counter +=len(page_text)\n",
    "                end_of_page = '=' * 30 + ' End of Page '+ str(page_num+1) + ' '+ '=' * 30\n",
    "\n",
    "                # Append pages of the same PDF together\n",
    "                text.append(page_text)\n",
    "                text.append(end_of_page)\n",
    "\n",
    "            if doc_read_counter:\n",
    "                print(f'# PDF read: {idx+1} | # PDF remaining: {max_files - idx - 1} | Total # PDF to be read: {max_files}')\n",
    "\n",
    "            # Append diff PDFs together\n",
    "            text_dic['No'].append(idx+1)\n",
    "            text_dic['PDF_title'].append(file)\n",
    "            text_dic['Text'].append('\\n\\n'.join(text))\n",
    "            text_dic['Page_length'].append(total_pages)\n",
    "            text_dic['Text_length'].append(text_counter)\n",
    "    return pd.DataFrame(text_dic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# PDF read: 1 | # PDF remaining: 9 | Total # PDF to be read: 10\n",
      "# PDF read: 2 | # PDF remaining: 8 | Total # PDF to be read: 10\n",
      "# PDF read: 3 | # PDF remaining: 7 | Total # PDF to be read: 10\n",
      "# PDF read: 4 | # PDF remaining: 6 | Total # PDF to be read: 10\n",
      "# PDF read: 5 | # PDF remaining: 5 | Total # PDF to be read: 10\n",
      "# PDF read: 6 | # PDF remaining: 4 | Total # PDF to be read: 10\n",
      "# PDF read: 7 | # PDF remaining: 3 | Total # PDF to be read: 10\n",
      "# PDF read: 8 | # PDF remaining: 2 | Total # PDF to be read: 10\n",
      "# PDF read: 9 | # PDF remaining: 1 | Total # PDF to be read: 10\n",
      "# PDF read: 10 | # PDF remaining: 0 | Total # PDF to be read: 10\n"
     ]
    }
   ],
   "source": [
    "# Extract from PDF\n",
    "pdf_dir = os.path.join(data_path, 'AI Papers')\n",
    "\n",
    "pdf_df_raw = dir_pdf_to_text(pdf_dir)\n",
    "pdf_df_processed = pdf_df_raw.copy()\n",
    "\n",
    "# Remove redundant columns\n",
    "pdf_df_processed = pdf_df_processed[['PDF_title', 'Text']]\n",
    "pdf_df_processed.rename(columns={'PDF_title': 'Title', 'Text':'Contents'}, inplace = True)\n",
    "pdf_df_processed['Source'] = 'PDF'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_news_df_raw = pd.read_excel(os.path.join(data_path, 'Google News Data.xlsx'))\n",
    "\n",
    "google_news_processed = google_news_df_raw.copy()\n",
    "\n",
    "# Remove redundant columns\n",
    "google_news_processed = google_news_processed[['Title', 'Article']]\n",
    "google_news_processed.rename(columns={ 'Article':'Contents'}, inplace = True)\n",
    "google_news_processed['Source'] = 'Google_News'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df_raw = pd.read_excel(os.path.join(data_path, 'Reddit Data.xlsx'))\n",
    "\n",
    "reddit_df_processed = reddit_df_raw.copy()\n",
    "\n",
    "# Remove redundant columns\n",
    "reddit_df_processed = reddit_df_processed[['Title', 'Article']]\n",
    "reddit_df_processed.rename(columns={ 'Article':'Contents'}, inplace = True)\n",
    "reddit_df_processed['Source'] = 'Reddit'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining dataset from 3 different domains\n",
    "\n",
    "- Each document is defined as an article/PDF/reddit post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.concat([google_news_processed, reddit_df_processed, pdf_df_processed]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw.to_csv('df_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run from here onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw = pd.read_csv('df_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenize all documents in each domain using a selected library (e.g., NLTK) and observe the tokens obtained. Discuss your observations from the following perspectives. Has the tokenizer correctly recognized the domain specific tokens? Use examples to illustrate what the expected tokens are, and what are not, particularly on the domain specific terms.\n",
    "\n",
    "- NLKT tokenizer tokenizes words by a mixture of blank space and punctuations. This function uses the Punkt tokenizer for tokenizing words. Punkt is a pre-trained unsupervised machine learning model for tokenizing sentences. It comes with pre-trained models for some languages, including English.\n",
    "\n",
    "- Some of the words are wrongly tokenized\n",
    "    1. Google News\n",
    "        - Numbers with commas in the middle are correctly tokenized as the same word e.g. 3,700 -> '3,700'\n",
    "        - Words with - are tokenized correctly into single tokens. e.g. co-founder -> 'co-founder'\n",
    "        - Words with apostrophe should not be tokenized. E.g. sector's -> 'tech', 'sector', \"'s\"\n",
    "            - Words with ' should not be split\n",
    "        - Names should be tokenized as a single token E.g Web Summit -> 'Web', 'Summit', Paddy Cosgrave -> 'Paddy', 'Cosgrave'\n",
    "            - Consecutive words with first alphabets in caps, excluding those at the start of a sentence, should not be split\n",
    "        - Dates should not be tokenized e.g. Oct 7 -> 'Oct', '7'\n",
    "            - Up to 2 sets of numbers to left and right of each month (long and short form) should be group together\n",
    "    2. Reddit\n",
    "        -  Team names should be tokenized as a single token e.g.G2 Esports -> 'G2', 'Esports'\n",
    "    3. PDF\n",
    "        - Figure X should be tokenized together  \n",
    "            - Figure X, followed by a number (regardless of .) should be groupped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "df_raw['token_words'] = df_raw['Contents'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Google_News', 'Reddit', 'PDF'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.Source.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highlight Results\n",
      "\n",
      "The introduction of the Swiss stage gives Western teams a 33% relative increase in the estimated chance of winning Worlds versus a Swissless simulation (up from 1.5% to 2.1%), and a further relative improvement of a Western team making it to finals by approximately 50% (from 6.8% to 10.24%). We also observe that the Swiss stage has improved the chances of a Western team making it through to the knockout stage, with the odds of at least one Western team making it out improving from 84.5% to 96.7%.\n",
      "Introduction\n",
      "\n",
      "Hiya all, I began putting together a Stochastic Simulation to explore this, and slowly it developed into a full-fledged side-project that I worked on occasionally alongside my PhD. Started LoL in Season 2 and have been consuming some flavour of Lol Esports since around then (the first match I watched was the CLG-EU vs M5 70min). I have never posted on Reddit before, but I wanted to share my work properly (even if between moving home and getting internet it's now about 2 weeks late). irl name is Jordan, and the account was just made to contribute my project to the community. Github link to view the full notebook https://github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave. Might put together a companion video explaining more and my thoughts as it's pretty hard to communicate everything here. I've linked the specific images where needed, and I will mark areas of cut content in brackets. An extra disclaimer before we start. The data isn't cleverly scraped, this project isn't perfect and the process is not as detailed as it could be. Even so, I am confident the method used provides a well-made stochastic simulation that meaningfully represents the format. I have done my utmost to outline my assumptions and justify my process below.\n",
      "Overview\n",
      "\n",
      "For the 2023 world championship, Riot updated their Groups stage from 4 groups of 4 teams with a double round robin within each group, to a Swiss stage. It is natural as a Western fan to ask what impact this might have for Western teams, as we typically perform better in volatile formats (such as the previous 'best of one' double round robin groups stage). We look to construct a stochastic simulation to investigate the new tournament system implemented by Riot.\n",
      "Previous Tournament(s) for data\n",
      "\n",
      "(Cut content explaining Major/Minor Regions) Throughout its history, League of Legends has had 4 primary international events. Worlds, the Mid-Season Invitational, Asian Games, and All-Stars. The All-Stars tournament is an off-season international tournament featuring fan-voted players, and was last seen in 2020 and will not be used in the data set. The Asian Games is an internationally recognised competition between countries that choose six players and coaching staff to send as their representatives. While League of Legends is used, it is not an exclusively Riot Games event. While it is a hyper-competitive event, players who play for one Region at Worlds or MSI may play for their home country's team in the Asian Games. Therefore as these teams do not represent those that would have had to fight through their regional league to be seeded into Worlds or MSI, and there are significant differences in how these teams are constructed, we will also be excluding the Asian Games from our data set. We therefore will only be considering the Mid-Season Invitational and Worlds matches as valid observations. The main issue we face is that as there are different numbers of representatives from different regions sent to each tournament, observations between the strongest seeds (Korea/China 1st seeds) and the weakest seeds (minor regions who struggle to make it to the main stage) are extremely limited / completely unobserved. As such, our first and most important assumption is that we will be measuring each Region's strength, rather than the strength of each region's seed individually (for example we will treat the 3rd seed of Europe the same as their 1st seed). In addition to increasing the pool of observations between different regions, this allows us a much simpler way to tackle changes in regions and regional construction throughout history.\n",
      "Blue-Team Advantage and Side Selection\n",
      "\n",
      "Blue has long been considered the stronger team and is chosen most often over the history of professional play. Typically the higher seed going into a match gets side selection in the first match (if the match is a best of 3 or 5), or teams will get to play once on both sides (in the double round robin mentioned above). The other way a team will have to choose a side is when they lose a match in a best of 3 or 5. We will calculate a global prior of Blue vs Red win chance, update the win chance for each region, and give whoever has side selection the choice of the better side for their region in the given circumstance.\n",
      "Data and League History\n",
      "\n",
      "(Cut content explaining that Metas changed, and explaining the different leagues). Some cut-off for the original seasons of League's history is required, as many of the institutions that now support teams in each region did not exist in the early years. I have chosen 2014 as the cut-off, so only data from season 5 (2015) and later will be considered. The precise cut-off does not have much justification, and there is an argument for formal analysis to see if there is a significant difference in the win rates of different teams around these years, however there is a lot of expert knowledge that would need to be provided to justify a specific cut-off date, and a rough estimate of 2015 seems satisfactory. The question is what to do with Oceania and Turkey? We can either re-classify their previous teams as they would appear in their new region, or we can remove them. Turkey is now a tier two region inside the LEC and as such none of their teams will be representing the LEC at Worlds. Further, the best performing Oceanian team placed 7/8th in the PCS, which implies the quality of Oceanian teams does not suitably represent the PCS, and as such both Regions should be excluded beyond calculating historic Blue advantage for our Prior.\n",
      "Assumptions\n",
      "\n",
      "    Regions have the same chance of winning regardless of the seed of the team from said region (see above)\n",
      "\n",
      "    Each region has a unique win probability vs each other region dependent on side selection (LEC vs LPL has different odds depending on if Europe is Blue or Red side)\n",
      "\n",
      "    The team with side selection will choose their best side vs the opposing region (we assume some prophetic perfect knowledge of the odds)\n",
      "\n",
      "    The Prior chosen will be a distribution for the Global win rate of Blue/Red side before observing any regional information\n",
      "\n",
      "    Matches between 2015-2023 (inclusive, so 8 and a half years of data) will be used, and that data further back doesn't accurately represent the current state of League of Legends in 2023.\n",
      "\n",
      "Outline\n",
      "\n",
      "The plan is as follows:\n",
      "\n",
      "\n",
      "    Collect every MSI and Worlds match since 2015 and store how many games each region won and against whom.\n",
      "\n",
      "    2) Construct a Prior distribution for the chance each side has to win before observing regional data which will be the average historical win-rates\n",
      "\n",
      "    3) Construct a Beta-Binomial Posterior distribution for the probability of each region winning on Blue against every other region.\n",
      "\n",
      "    4) Construct general functions we will require for the Simulations, sampling from each probability distribution based on who is playing.\n",
      "\n",
      "    5) Simulate a historical Worlds tournament\n",
      "\n",
      "    6) Simulate the new Swiss Structure at Worlds inside the new tournament structure\n",
      "\n",
      "    7) Repeat each simulation a substantial amount of times and observe how the West performs. How many times do we make it to the finals, etc.?\n",
      "\n",
      "Raw Match Information\n",
      "\n",
      "We collected every international match since the start of 2015 and broke them down into single-match observations. Below are the wins for each Region on Blue (First Column) by region:\n",
      "REGION\tCBL\tLCK\tLCS\tLEC\tLJL\tLLA\tLPL\tPCS\tVCS\tALL\n",
      "CBL\t0\t0\t2\t1\t8\t7\t1\t3\t2\t24\n",
      "LCK\t2\t28\t33\t49\t3\t3\t45\t18\t10\t191\n",
      "LCS\t4\t8\t2\t16\t4\t6\t9\t17\t10\t76\n",
      "LEC\t6\t12\t26\t2\t1\t5\t23\t17\t8\t100\n",
      "LJL\t3\t0\t3\t0\t0\t3\t0\t2\t2\t13\n",
      "LLA\t1\t0\t0\t0\t4\t1\t1\t2\t3\t12\n",
      "LPL\t2\t35\t30\t41\t5\t6\t14\t24\t11\t168\n",
      "PCS\t6\t8\t7\t9\t3\t6\t3\t1\t5\t48\n",
      "VCS\t0\t1\t1\t2\t2\t6\t2\t2\t0\t16\n",
      "ALL\t24\t92\t104\t120\t30\t43\t98\t86\t51\t648\n",
      "\n",
      "and the wins for each region on Red:\n",
      "REGION\tCBL\tLCK\tLCS\tLEC\tLJL\tLLA\tLPL\tPCS\tVCS\tALL\n",
      "CBL\t0\t0\t0\t0\t4\t3\t0\t0\t1\t8\n",
      "LCK\t3\t26\t27\t33\t3\t3\t35\t16\t12\t158\n",
      "LCS\t3\t3\t2\t9\t3\t1\t14\t14\t5\t54\n",
      "LEC\t2\t24\t13\t1\t1\t3\t16\t16\t9\t85\n",
      "LJL\t4\t0\t0\t1\t0\t3\t1\t2\t0\t11\n",
      "LLA\t0\t0\t0\t1\t2\t5\t1\t2\t3\t14\n",
      "LPL\t2\t32\t22\t28\t5\t4\t10\t18\t5\t126\n",
      "PCS\t4\t4\t8\t6\t2\t1\t5\t1\t6\t37\n",
      "VCS\t0\t0\t4\t7\t3\t7\t0\t3\t0\t24\n",
      "All\t18\t89\t76\t86\t23\t30\t82\t72\t41\t517\n",
      "Constructing Region Win Probabilities\n",
      "\n",
      "(I've done my best to get the little bit of stats across but I can't find a good way to put latex equations in a Reddit post :( ) As we can see from the data, there are some Regions which have NEVER won a match against another region. Obviously, they have some chance, even if the data we have observed doesn't include a win. As such, we're going to take a Bayesian approach and look to construct a posterior distribution for the probability a Region wins on blue. We can take the Maximum Posterior estimate probabilities as a win-chance for our simulation of the Swiss Stage. We could either model each game following the Bernoulli distribution, since whether a team wins or loses is a simple binary outcome, or we could model full matches following the Binomial Distribution. Since the Bernoulli distribution is a special case of the Binomial distribution, and therefore the Beta distribution can be used as a conjugate for both, there is minimal practical difference (as long as any side selection assumption is valid). Hence, as whoever wins each game determines side selection, it will be much simpler if we take the first approach. We assume that the probability a Region i wins a given match on Blue when against another Region j on Red is assumed to follow a Bernoulli distribution Ï‰~Bern(p). Second, we need to construct our prior distribution for the chance any Region wins on Blue. We know that the win rate Blue should roughly be 56%, and that this is going to depend a lot on the quality of the team more so than this prior information. Therefore, we want a relatively flat Beta distribution with an expected value E[X] = 0.56. The expected value of the Beta distribution is given by E[x] = a/(a+b). As a Beta(1,1) is equivalent to a Uniform distribution over [0,1], and a Beta(2,2) is a parabola centred on 0.5, we choose a prior distribution of Beta(1.4,1.1) (https://github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave/blob/main/Reddit%20Post/BetaDist.png). This gives a Posterior Probability for the chance a team wins vs any other region on both Blue and Red sides (See the Array here https://github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave/blob/main/Reddit%20Post/Individual%20Win%20Odds.png).\n",
      "Prefered Side: Calculate the Best side for each Region.\n",
      "\n",
      "Now we have each team's Posterior win rate on Blue, we can work out what side they should choose if given the choice. Simply they need to choose whatever is greater: their Blue win odds, or their opponent's Blue loss odds.\n",
      "Previous Tournament Structure - Do our Probabilities and Assumptions hold up?\n",
      "\n",
      "(Removed description of previous groups format) We need to test a simulation with the old Format to check our method isn't absurd, which is a typical test to run when constructing a simulation for analysis. The issue is, that some of the regions no longer exist, some existed at different points, and the play-ins seeding is significantly different. Therefore, we're going to make some assumptions and run a test simulation of the old Groups. We want to construct some Pools that are historically accurate, and since I'm a Bias EU fan, let's pick the structure and teams from 2019, where the Pools were:\n",
      "\n",
      "\n",
      "    The Groups Pool 1 consisted of the four 1st Seeds from the LCK1, LPL1, LEC1 and LCS1\n",
      "\n",
      "    2) The Groups Pool 2 consisted of 8 teams. LCK2, LEC2, LCS2, LPL2, LPL3, VCS1, PCS1, PCS2\n",
      "\n",
      "    3) The Groups Pool 3 consisted of the 4 teams advancing from the previous PlayIns structure. As the strongest teams we have available to us on paper (Looking at you MAD Lions), we look to seed in LCK3, LEC3, LCS3 and VCS2 from the historical Play-Ins bracket. In tie-breaker matches, side selection was given to the team with the lower average victory time (https://www.esportsbets.com/league-of-legends/worlds/rules, note: I couldn't find a Riot source here), but as we don't simulate match time, we will decide side selection via a coin flip. This is not a perfect simulation as we are not tracking head-to-head results, which for certain tie-breaker scenarios would create more situations to track, and we're using a simplification here as Worlds is around the corner (29th September 2023) we're almost at the Main stage (17th October 2023). The logic we need to consider follows:\n",
      "\n",
      "    If there are two highest-scoring teams with unique scores, we select them accordingly\n",
      "\n",
      "    If there are two highest-scoring teams with non-unique scores, we need to play a tie-breaker match to determine 1st and 2nd seeds\n",
      "\n",
      "    If there is a highest scoring team with a unique score and two teams tied for runner-up, the runner-up teams must play a tie-breaker for 2nd seed\n",
      "\n",
      "    If there are three highest-scoring teams sharing the same score, they would be seeded into a tie-breaker bracket. The team with a buy would usually be the team with the lowest average game time in their victories, however as we do not simulate game time, the team with a buy will be selected at random and therefore will receive side selection in their match as they have been chosen to have the lowest average game time\n",
      "\n",
      "    If there is a highest scoring team with a unique score and the other three teams share the same lowest score, they would be seeded into a tie-breaker bracket for runner-up. The team with a buy would usually be the team with the lowest average game time in their victories, however as we do not simulate game time, the team with a buy will be selected at random and therefore will receive side selection in their match as they have been chosen to have the lowest average game time\n",
      "\n",
      "    If all 4 teams share the same scoreline, then they are seeded into a single elimination bracket with seeding decided by victory time. We will randomly seed the 4 teams into the single elimination bracket with 1st and 2nd Seeds awarded to the Winners and Runners-up of the bracket with side selection decided randomly The successful 8 teams are finally seeded into a single elimination best of 5 bracket, where the only stipulations are that a 1st Seed team must face off against a 2nd Seed team in the quarter-finals and that teams from the same group cannot be seeded into the same half of the bracket.\n",
      "\n",
      "Old Format - Results (Historic Results Images)\n",
      "What does our simulation say about the Winner of Worlds?\n",
      "\n",
      "The headline estimate figures under the simulation are that there is an approximate 3% chance any Western team happens to be the one to win Worlds under the previous format, which as over the 8 and a half years of data used no Western team has won Worlds holds up in line with reality. Of the 8 actual winners in the data set, 5 were from South Korea (62.5%) and 3 were from China (37.5%), with the simulation following a similar suit giving China 33.3% of all victories and South Korea 63.5%. Therefore, at least for the overall winner, the simulation appears to be a good historical reflection of reality. Plus, shout-out to the 6/10000 scenarios where the PCS won! (https://github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave/blob/main/Reddit%20Post/Historic%20Win.png)\n",
      "How about the Runners Up?\n",
      "\n",
      "Collectively we have about a 12% chance of being eliminated as the Runners-up. Historically we've occupied 2/8 (25%) of the runner-up positions, which at first glance makes the simulation appear to be understating the West's ability to reach the Finals. However, considering that there are very few historical observations (only 8 observed runners-up), the simulated results are not an extreme divergence from history, and there is still a strong argument that the simulation is a fair representation of historic events. One could make the argument that one of these 2nd place positions was due to two Western teams meeting in the semi-finals (a very rare event, guaranteeing a Western top two position), and therefore the correct historic percentage should be the number of times a Western team has defeated an Eastern team in the semi-finals (G2 in 2019 is the only historical example, giving 12.5% observed vs 12% simulated estimate). This is not to make a comment on how strong Fnatic 2018 were in comparison to their peers at the time, as some considered them to potentially be the best Western team we had ever sent, and they finished their group as the 1st seed, winning against Invictus Gaming (the eventual winners) in their tie-breaker match. However, if we consider finals match occurrences, rather than winner or runners-up, the historic simulation looks even better justified.\n",
      "Finally (and most importantly), what about the number of scenarios where no one qualifies?\n",
      "\n",
      "Arguably more important is the number of simulated tournaments where no Western team makes it to groups. As the previous analysis looks at the proportion that a team appears in any randomly chosen position independent of other teams, we actually lose implicit information about those very teams. Worlds does not occur in a vacuum, and there are 16 teams all dependent on one another for positions. We see that under the conditions set out the chance no Western team made it to groups is approximately 1%. We also see how the chance of a Western team making it to the Semi-Finals and the Finals drops drastically as the Western teams now have to win a best of 5 (most likely against either China or Korea) to advance. While we appear in nearly 15% of finals (comparative to our 12.5% occupancy rate of total collective finals positions, where the difference occurs when the West has two teams in the Final), the final 3% chance to bring home the Worlds trophy still holds from the beginning of this analysis (https://github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave/blob/main/Reddit%20Post/Historic%20Appearence%20Chance.png).\n",
      "Old Format Critique - Improved Resolution\n",
      "\n",
      "One significant criticism of the simulation for the historic format is that we no longer live in 2019, we live in 2023. While it is important to test assumptions by simulating the historic structure our data originates from, one cannot claim that the differences between a 2019 structure and the modern 2023 structure are only because of the introduction of the Swiss stage. Therefore we look to simulate a hyperthetical 2023 groups stage to evaluate the impact the introduction of Swiss has had on Western teams. Therefore we will label our previous simulation that looked at a historic format as the 'Historic' simulation, our comparison simulation with a hypothetical bracket 'Swissless', and the final 2023 simulation as \"Swiss\". The important difference between the Historic and Swissless formats is the inclusion of a 4th seeds from the most competitive regions, which as we have identified from the 2019 simulation, will likely have a hugely significant impact on the results as the difference between the proportional 30% of knockouts slots and 99% chance to not miss knockouts is likely to change drastically if there are enough LPL and LCK teams to fill all 8 knockout slots. Therefore, in order to simulate a Swissless Worlds 2023, we look to base the simulation heavily on the 2022 Worlds format, and while the draw rules were slightly different with 4 pools rather than 3 because we do not consider the strength of each seed, it has no impact on the simulation.\n",
      "'Swissless' 2023 Simulation (Swissless Results Images)\n",
      "\n",
      "Worlds 2022 saw 4 LPL teams, 4 LCK teams, 4 LEC teams, 3 LCS teams, 2 PCS, 2 VCS, 1 LLA, 1 LJL, 1 CBL, 1 LCO (Oceania) and 1 TCL (Turkey). For our Swissless simulation, we follow Worlds 2022 with 3 LCK, 3 LPL, 2 LEC, 2 LCS, 1 PCS and 1 VCS teams automatically seeded into the groups stage, and will further seed the 4th LCK and LPL seeds, 3rd LEC and LCS seeds (as those are the teams that passed the 2022 play-ins stage). We see that while we estimated the West to win about 3% of Worlds under the 2019 rules, under a hypothetical Swissless structure we only estimate a chance of 1.4% for a Western team to win the tournament (https://github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave/blob/main/Reddit%20Post/Swissless%20Win.png). Further, we see how the probability of a Western team appearing at any stage in the knockouts has dropped significantly, with there now being a 10% chance no Western team makes knockouts, and only an approximate 6.8% chance of a Western team making a Finals appearance (https://github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave/blob/main/Reddit%20Post/Swissless%20Appearence%20Chance.png). As expected China and Korea are effectively guaranteed to appear in the knockout stage, with approximate 91% and 99% odds to make it to semi-finals, and a further 89% and 64% odds respectively to have a team appear in the finals. Ultimately, when considering a deep run at Worlds as a team from the West making it to the finals, the addition of a 4th seed from the LPL and LEC more than halved the Western chance.\n",
      "The New Tournament Structure\n",
      "Play-Ins\n",
      "\n",
      "First, we need to find the final Western representative, which is simply achieved by running a Bo5 between LEC4 and LCS4. The Play-Ins feature 8 teams facing off in two double elimination Bo3 brackets that were drawn at random after the conclusion of the LEC finals on 10th September 2023, so the bracket order and placement of teams are known to us. However, we will sample the bracket randomly just as Riot has done in order to fully simulate a Worlds run under the current rules. The 8 teams are seeded via two pools into the 2 double elimination brackets where two teams will qualify for the Swiss stage. The Play-Ins conclude with the upper-bracket finalist from one bracket facing off against the lower-bracket finalist from the opposing bracket in a best of 5. Seeding rules that we seed all Pool 1 teams (upper tier), then all Pool 2 (lower tier), where any team from the same Region must be in different brackets, and any team from the same Pool cannot face each other in the first round.\n",
      "\n",
      "    Pool 1: PCS1 (PSG), PCS2 (CFO), VCS1 (GAM) & WQS Winner (LEC4 vs LCS4 in a Bo5)\n",
      "\n",
      "    Pool 2: VCS2 (TW), LJL1 (DFM), LLA1 (R7) & CBL1 (LLL)\n",
      "\n",
      "Swiss Stage\n",
      "\n",
      "Teams in the Swiss stage will always face an opponent with an equivalent scoreline throughout (potentially) 5 rounds. Teams play in a single Bo1 (which has massive implications for the impact of Blue advantage) unless playing in a qualification (to the knockouts) or elimination (lose and you're removed from the tournament) match where they play a Bo3 (https://lolesports.com/article/state-of-the-game-lol-esports-in-2023/blt5d3bca31d1b39e0c). Update 12th Sep 2023: Following the Primer, we now know how the teams are seeded into the Swiss bracket. Seeds 1 must fight a 4th Seed, and a 2nd Seed must fight a 3rd Seed, where seeds 1,2 and 3 are from their respective major regions, and 4th Seeds are LCK4, LPL4 and the two Play-In winners. We need to match the pairs and then randomly allocate these pairs to matches to suitably explore the full state space as a requirement for valid results for a Monte Carlo estimate. Further, throughout the Swiss bracket, side selection is not random as I had first coded up The most challenging part of this is that Riot Games is very tight on information on how the teams will be matched up throughout the bracket. For example, we don't know if a team that wins in round 3 from 1|1 to have a scoreline of 2|1 is equivalent to a team that lost the upper bracket to now be at 2|1. Are these teams treated the same or do we randomly match these teams up? If we do fix the Upper and Middle bracket teams in their own pools, then who gets the higher seed if they win? I've thought it best to randomly sample both the Upper and Lower brackets in round 4, and we will perform an extra random draw step going into the Knockout stage.\n",
      "Defining and Running the Worlds 2023 Simulation\n",
      "\n",
      "We're going to run for 10,000 tournaments. I would like to run more but this running on god knows what free Google Cloud machine. For an industrial simulation, 10,000 would not usually be sufficient, however as it is already the 15th of October, there is a limited state space with the model to explore, we're not learning any parameters and we're looking at unlikely but not extreme events, it's perfectly long enough. For any results, one could estimate a likely absolute error interval of about 0.5%-1% in the most extreme cases.\n",
      "Results - Winners (Swiss Results Images)\n",
      "\n",
      "The headline figure to highlight is that the odds of a Western team winning Worlds have shot up by approximately just over 33% relative to their estimated chances from the Swissless simulation, to a tournament-winning chance of about 2.1%. This gives the West a real-term odds increase of about 0.6% for Worlds 2023, where we see the odds of Korea or China ending up victorious only decline in real terms by about 0.5% each. However, we specifically see the biggest benefit for Europe, with them appearing in roughly 1.5 times the number of victories when compared to the Swissless simulation (https://github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave/blob/main/Reddit%20Post/Swiss%20Win.png). For runner-up, we observe a similar change, where the Western chance of appearing here increases from approximately 5.4% to 8.3% (54% increase) at the expense of the Eastern teams under the new format. We further observe some impressive Minor region 2nd place finishes within our 10,000 simulated Worlds, implying there are better chances for a Minor region team making a deep run in the new format, although the simulation was not run long enough to make a reliable estimate for the exact percentage chance (https://github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave/blob/main/Reddit%20Post/Swiss%20Runners%20Up.png). Finally, we observe how the Swiss changes have improved the chances of seeing a Western team make it through to the knockout stage, with the odds of at least one Western team making it out improving from 84.5% to 96.7%, with a further improvement of a Western team making it to finals increasing from 6.8% to 10.24% (approximately a 50% increase) (https://github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave/blob/main/Reddit%20Post/Swiss%20Appearence%20Chance.png).\n",
      "Discussion\n",
      "\n",
      "While an extra 1/200 Worlds victories doesn't sound amazing, almost any team would jump at a chance to improve their odds of success by 33%! Meanwhile, while the Eastern teams do see their real-world chance of victory drop by about 0.2-0.5% each, this has only decreased their respective chances of winning Worlds by about 1% compared to their Swissless odds, which only goes to demonstrate the sheer dominance of the Eastern teams in this tournament. There is an interesting question to be answered as to why the Swiss format, which in theory should reward the superior regions (remember their win rate hasn't changed with the new format on a per-game basis) has resulted in a relatively considerable improvement for the Western teams' chances of making a meaningful run? I conjecture that the answer lies in how the Swiss stage is paired up. After the first round, where teams are drawn into matches based on their pools, the teams are then randomly paired with opponents of equal score. As there are likely to be some Korean vs Chinese matches in round 1, it simply takes some occurrence in certain rounds for an LCK or LPL team to be playing one another at both the 0|0, 1|0 and 0|1 scorelines, where the only block for a Western team is potentially to win a best of three. Further, with the West likely to fulfil slots 3 through 8, there will be multiple scenarios where say seed 4 and seed 7, as both Western teams, have the potential to be drawn together. While this is not likely, this scenario is more likely than a Western team in the Swissless structure drawn into say group A placing 1st Seed against both an LCK and LPL team, a Western team in any other group places 2nd seed beating out either their respective LCK or LPL team and both teams being drawn into a Quarter Finals match. I believe the reason for the improved Western odds this year is because as a part of the Swissless environment, with both 4 LCK and 4 LPL teams, every Western team is forced to overcome both a Korean and a Chinese team across 4 matches to obtain a beneficial Quarter-Finals match. This study shows that we can expect the West, which we model as objectively weaker than Chinese and Korean teams, to have a greater chance both to make it to the knockouts thereby eliminating one of those eight teams and making it all the way to the finals match, even winning the tournament. This certainly implies the Swiss format is far more volatile than the Groups system, and whether this is a worse format because of it is up for debate, as one could argue that increasing the possibility of engaging the entire Western fan base by enabling at least one Western team to make it to the Knockout stage is a good system. After all, everybody loves a good underdog storyline. Ultimately, Western fans should be quietly hopeful for one of our representatives to perform better than expected this year based on format changes alone. For anyone who has made it this far, thank you for taking the time to read this all. I'm very open to any and all feedback about this project. There is so much more that could be investigated! Ideas I have are:\n",
      "\n",
      "    Changes in the Number of Seeds (3 to 4 LPL and LCK seeds were MASSIVE)\n",
      "\n",
      "    Clear classification between China/Korea and Europe/North America (Major, Moderate and Minor Regions)\n",
      "\n",
      "    Strength of Seeds - some adjustment for 1st and 3/4th seeds\n",
      "\n",
      "    Warm-up and fatigue effects on different regions\n"
     ]
    }
   ],
   "source": [
    "print(df_raw.loc[df_raw.Source == 'Reddit'].reset_index(drop = True).Contents[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Highlight|Results|The|introduction|of|the|Swiss|stage|gives|Western|teams|a|33|%|relative|increase|in|the|estimated|chance|of|winning|Worlds|versus|a|Swissless|simulation|(|up|from|1.5|%|to|2.1|%|)|,|and|a|further|relative|improvement|of|a|Western|team|making|it|to|finals|by|approximately|50|%|(|from|6.8|%|to|10.24|%|)|.|We|also|observe|that|the|Swiss|stage|has|improved|the|chances|of|a|Western|team|making|it|through|to|the|knockout|stage|,|with|the|odds|of|at|least|one|Western|team|making|it|out|improving|from|84.5|%|to|96.7|%|.|Introduction|Hiya|all|,|I|began|putting|together|a|Stochastic|Simulation|to|explore|this|,|and|slowly|it|developed|into|a|full-fledged|side-project|that|I|worked|on|occasionally|alongside|my|PhD|.|Started|LoL|in|Season|2|and|have|been|consuming|some|flavour|of|Lol|Esports|since|around|then|(|the|first|match|I|watched|was|the|CLG-EU|vs|M5|70min|)|.|I|have|never|posted|on|Reddit|before|,|but|I|wanted|to|share|my|work|properly|(|even|if|between|moving|home|and|getting|internet|it|'s|now|about|2|weeks|late|)|.|irl|name|is|Jordan|,|and|the|account|was|just|made|to|contribute|my|project|to|the|community|.|Github|link|to|view|the|full|notebook|https|:|//github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave|.|Might|put|together|a|companion|video|explaining|more|and|my|thoughts|as|it|'s|pretty|hard|to|communicate|everything|here|.|I|'ve|linked|the|specific|images|where|needed|,|and|I|will|mark|areas|of|cut|content|in|brackets|.|An|extra|disclaimer|before|we|start|.|The|data|is|n't|cleverly|scraped|,|this|project|is|n't|perfect|and|the|process|is|not|as|detailed|as|it|could|be|.|Even|so|,|I|am|confident|the|method|used|provides|a|well-made|stochastic|simulation|that|meaningfully|represents|the|format|.|I|have|done|my|utmost|to|outline|my|assumptions|and|justify|my|process|below|.|Overview|For|the|2023|world|championship|,|Riot|updated|their|Groups|stage|from|4|groups|of|4|teams|with|a|double|round|robin|within|each|group|,|to|a|Swiss|stage|.|It|is|natural|as|a|Western|fan|to|ask|what|impact|this|might|have|for|Western|teams|,|as|we|typically|perform|better|in|volatile|formats|(|such|as|the|previous|'best|of|one|'|double|round|robin|groups|stage|)|.|We|look|to|construct|a|stochastic|simulation|to|investigate|the|new|tournament|system|implemented|by|Riot|.|Previous|Tournament|(|s|)|for|data|(|Cut|content|explaining|Major/Minor|Regions|)|Throughout|its|history|,|League|of|Legends|has|had|4|primary|international|events|.|Worlds|,|the|Mid-Season|Invitational|,|Asian|Games|,|and|All-Stars|.|The|All-Stars|tournament|is|an|off-season|international|tournament|featuring|fan-voted|players|,|and|was|last|seen|in|2020|and|will|not|be|used|in|the|data|set|.|The|Asian|Games|is|an|internationally|recognised|competition|between|countries|that|choose|six|players|and|coaching|staff|to|send|as|their|representatives|.|While|League|of|Legends|is|used|,|it|is|not|an|exclusively|Riot|Games|event|.|While|it|is|a|hyper-competitive|event|,|players|who|play|for|one|Region|at|Worlds|or|MSI|may|play|for|their|home|country|'s|team|in|the|Asian|Games|.|Therefore|as|these|teams|do|not|represent|those|that|would|have|had|to|fight|through|their|regional|league|to|be|seeded|into|Worlds|or|MSI|,|and|there|are|significant|differences|in|how|these|teams|are|constructed|,|we|will|also|be|excluding|the|Asian|Games|from|our|data|set|.|We|therefore|will|only|be|considering|the|Mid-Season|Invitational|and|Worlds|matches|as|valid|observations|.|The|main|issue|we|face|is|that|as|there|are|different|numbers|of|representatives|from|different|regions|sent|to|each|tournament|,|observations|between|the|strongest|seeds|(|Korea/China|1st|seeds|)|and|the|weakest|seeds|(|minor|regions|who|struggle|to|make|it|to|the|main|stage|)|are|extremely|limited|/|completely|unobserved|.|As|such|,|our|first|and|most|important|assumption|is|that|we|will|be|measuring|each|Region|'s|strength|,|rather|than|the|strength|of|each|region|'s|seed|individually|(|for|example|we|will|treat|the|3rd|seed|of|Europe|the|same|as|their|1st|seed|)|.|In|addition|to|increasing|the|pool|of|observations|between|different|regions|,|this|allows|us|a|much|simpler|way|to|tackle|changes|in|regions|and|regional|construction|throughout|history|.|Blue-Team|Advantage|and|Side|Selection|Blue|has|long|been|considered|the|stronger|team|and|is|chosen|most|often|over|the|history|of|professional|play|.|Typically|the|higher|seed|going|into|a|match|gets|side|selection|in|the|first|match|(|if|the|match|is|a|best|of|3|or|5|)|,|or|teams|will|get|to|play|once|on|both|sides|(|in|the|double|round|robin|mentioned|above|)|.|The|other|way|a|team|will|have|to|choose|a|side|is|when|they|lose|a|match|in|a|best|of|3|or|5|.|We|will|calculate|a|global|prior|of|Blue|vs|Red|win|chance|,|update|the|win|chance|for|each|region|,|and|give|whoever|has|side|selection|the|choice|of|the|better|side|for|their|region|in|the|given|circumstance|.|Data|and|League|History|(|Cut|content|explaining|that|Metas|changed|,|and|explaining|the|different|leagues|)|.|Some|cut-off|for|the|original|seasons|of|League|'s|history|is|required|,|as|many|of|the|institutions|that|now|support|teams|in|each|region|did|not|exist|in|the|early|years|.|I|have|chosen|2014|as|the|cut-off|,|so|only|data|from|season|5|(|2015|)|and|later|will|be|considered|.|The|precise|cut-off|does|not|have|much|justification|,|and|there|is|an|argument|for|formal|analysis|to|see|if|there|is|a|significant|difference|in|the|win|rates|of|different|teams|around|these|years|,|however|there|is|a|lot|of|expert|knowledge|that|would|need|to|be|provided|to|justify|a|specific|cut-off|date|,|and|a|rough|estimate|of|2015|seems|satisfactory|.|The|question|is|what|to|do|with|Oceania|and|Turkey|?|We|can|either|re-classify|their|previous|teams|as|they|would|appear|in|their|new|region|,|or|we|can|remove|them|.|Turkey|is|now|a|tier|two|region|inside|the|LEC|and|as|such|none|of|their|teams|will|be|representing|the|LEC|at|Worlds|.|Further|,|the|best|performing|Oceanian|team|placed|7/8th|in|the|PCS|,|which|implies|the|quality|of|Oceanian|teams|does|not|suitably|represent|the|PCS|,|and|as|such|both|Regions|should|be|excluded|beyond|calculating|historic|Blue|advantage|for|our|Prior|.|Assumptions|Regions|have|the|same|chance|of|winning|regardless|of|the|seed|of|the|team|from|said|region|(|see|above|)|Each|region|has|a|unique|win|probability|vs|each|other|region|dependent|on|side|selection|(|LEC|vs|LPL|has|different|odds|depending|on|if|Europe|is|Blue|or|Red|side|)|The|team|with|side|selection|will|choose|their|best|side|vs|the|opposing|region|(|we|assume|some|prophetic|perfect|knowledge|of|the|odds|)|The|Prior|chosen|will|be|a|distribution|for|the|Global|win|rate|of|Blue/Red|side|before|observing|any|regional|information|Matches|between|2015-2023|(|inclusive|,|so|8|and|a|half|years|of|data|)|will|be|used|,|and|that|data|further|back|does|n't|accurately|represent|the|current|state|of|League|of|Legends|in|2023|.|Outline|The|plan|is|as|follows|:|Collect|every|MSI|and|Worlds|match|since|2015|and|store|how|many|games|each|region|won|and|against|whom|.|2|)|Construct|a|Prior|distribution|for|the|chance|each|side|has|to|win|before|observing|regional|data|which|will|be|the|average|historical|win-rates|3|)|Construct|a|Beta-Binomial|Posterior|distribution|for|the|probability|of|each|region|winning|on|Blue|against|every|other|region|.|4|)|Construct|general|functions|we|will|require|for|the|Simulations|,|sampling|from|each|probability|distribution|based|on|who|is|playing|.|5|)|Simulate|a|historical|Worlds|tournament|6|)|Simulate|the|new|Swiss|Structure|at|Worlds|inside|the|new|tournament|structure|7|)|Repeat|each|simulation|a|substantial|amount|of|times|and|observe|how|the|West|performs|.|How|many|times|do|we|make|it|to|the|finals|,|etc.|?|Raw|Match|Information|We|collected|every|international|match|since|the|start|of|2015|and|broke|them|down|into|single-match|observations|.|Below|are|the|wins|for|each|Region|on|Blue|(|First|Column|)|by|region|:|REGION|CBL|LCK|LCS|LEC|LJL|LLA|LPL|PCS|VCS|ALL|CBL|0|0|2|1|8|7|1|3|2|24|LCK|2|28|33|49|3|3|45|18|10|191|LCS|4|8|2|16|4|6|9|17|10|76|LEC|6|12|26|2|1|5|23|17|8|100|LJL|3|0|3|0|0|3|0|2|2|13|LLA|1|0|0|0|4|1|1|2|3|12|LPL|2|35|30|41|5|6|14|24|11|168|PCS|6|8|7|9|3|6|3|1|5|48|VCS|0|1|1|2|2|6|2|2|0|16|ALL|24|92|104|120|30|43|98|86|51|648|and|the|wins|for|each|region|on|Red|:|REGION|CBL|LCK|LCS|LEC|LJL|LLA|LPL|PCS|VCS|ALL|CBL|0|0|0|0|4|3|0|0|1|8|LCK|3|26|27|33|3|3|35|16|12|158|LCS|3|3|2|9|3|1|14|14|5|54|LEC|2|24|13|1|1|3|16|16|9|85|LJL|4|0|0|1|0|3|1|2|0|11|LLA|0|0|0|1|2|5|1|2|3|14|LPL|2|32|22|28|5|4|10|18|5|126|PCS|4|4|8|6|2|1|5|1|6|37|VCS|0|0|4|7|3|7|0|3|0|24|All|18|89|76|86|23|30|82|72|41|517|Constructing|Region|Win|Probabilities|(|I|'ve|done|my|best|to|get|the|little|bit|of|stats|across|but|I|ca|n't|find|a|good|way|to|put|latex|equations|in|a|Reddit|post|:|(|)|As|we|can|see|from|the|data|,|there|are|some|Regions|which|have|NEVER|won|a|match|against|another|region|.|Obviously|,|they|have|some|chance|,|even|if|the|data|we|have|observed|does|n't|include|a|win|.|As|such|,|we|'re|going|to|take|a|Bayesian|approach|and|look|to|construct|a|posterior|distribution|for|the|probability|a|Region|wins|on|blue|.|We|can|take|the|Maximum|Posterior|estimate|probabilities|as|a|win-chance|for|our|simulation|of|the|Swiss|Stage|.|We|could|either|model|each|game|following|the|Bernoulli|distribution|,|since|whether|a|team|wins|or|loses|is|a|simple|binary|outcome|,|or|we|could|model|full|matches|following|the|Binomial|Distribution|.|Since|the|Bernoulli|distribution|is|a|special|case|of|the|Binomial|distribution|,|and|therefore|the|Beta|distribution|can|be|used|as|a|conjugate|for|both|,|there|is|minimal|practical|difference|(|as|long|as|any|side|selection|assumption|is|valid|)|.|Hence|,|as|whoever|wins|each|game|determines|side|selection|,|it|will|be|much|simpler|if|we|take|the|first|approach|.|We|assume|that|the|probability|a|Region|i|wins|a|given|match|on|Blue|when|against|another|Region|j|on|Red|is|assumed|to|follow|a|Bernoulli|distribution|Ï‰~Bern|(|p|)|.|Second|,|we|need|to|construct|our|prior|distribution|for|the|chance|any|Region|wins|on|Blue|.|We|know|that|the|win|rate|Blue|should|roughly|be|56|%|,|and|that|this|is|going|to|depend|a|lot|on|the|quality|of|the|team|more|so|than|this|prior|information|.|Therefore|,|we|want|a|relatively|flat|Beta|distribution|with|an|expected|value|E|[|X|]|=|0.56|.|The|expected|value|of|the|Beta|distribution|is|given|by|E|[|x|]|=|a/|(|a+b|)|.|As|a|Beta|(|1,1|)|is|equivalent|to|a|Uniform|distribution|over|[|0,1|]|,|and|a|Beta|(|2,2|)|is|a|parabola|centred|on|0.5|,|we|choose|a|prior|distribution|of|Beta|(|1.4,1.1|)|(|https|:|//github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave/blob/main/Reddit|%|20Post/BetaDist.png|)|.|This|gives|a|Posterior|Probability|for|the|chance|a|team|wins|vs|any|other|region|on|both|Blue|and|Red|sides|(|See|the|Array|here|https|:|//github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave/blob/main/Reddit|%|20Post/Individual|%|20Win|%|20Odds.png|)|.|Prefered|Side|:|Calculate|the|Best|side|for|each|Region|.|Now|we|have|each|team|'s|Posterior|win|rate|on|Blue|,|we|can|work|out|what|side|they|should|choose|if|given|the|choice|.|Simply|they|need|to|choose|whatever|is|greater|:|their|Blue|win|odds|,|or|their|opponent|'s|Blue|loss|odds|.|Previous|Tournament|Structure|-|Do|our|Probabilities|and|Assumptions|hold|up|?|(|Removed|description|of|previous|groups|format|)|We|need|to|test|a|simulation|with|the|old|Format|to|check|our|method|is|n't|absurd|,|which|is|a|typical|test|to|run|when|constructing|a|simulation|for|analysis|.|The|issue|is|,|that|some|of|the|regions|no|longer|exist|,|some|existed|at|different|points|,|and|the|play-ins|seeding|is|significantly|different|.|Therefore|,|we|'re|going|to|make|some|assumptions|and|run|a|test|simulation|of|the|old|Groups|.|We|want|to|construct|some|Pools|that|are|historically|accurate|,|and|since|I|'m|a|Bias|EU|fan|,|let|'s|pick|the|structure|and|teams|from|2019|,|where|the|Pools|were|:|The|Groups|Pool|1|consisted|of|the|four|1st|Seeds|from|the|LCK1|,|LPL1|,|LEC1|and|LCS1|2|)|The|Groups|Pool|2|consisted|of|8|teams|.|LCK2|,|LEC2|,|LCS2|,|LPL2|,|LPL3|,|VCS1|,|PCS1|,|PCS2|3|)|The|Groups|Pool|3|consisted|of|the|4|teams|advancing|from|the|previous|PlayIns|structure|.|As|the|strongest|teams|we|have|available|to|us|on|paper|(|Looking|at|you|MAD|Lions|)|,|we|look|to|seed|in|LCK3|,|LEC3|,|LCS3|and|VCS2|from|the|historical|Play-Ins|bracket|.|In|tie-breaker|matches|,|side|selection|was|given|to|the|team|with|the|lower|average|victory|time|(|https|:|//www.esportsbets.com/league-of-legends/worlds/rules|,|note|:|I|could|n't|find|a|Riot|source|here|)|,|but|as|we|do|n't|simulate|match|time|,|we|will|decide|side|selection|via|a|coin|flip|.|This|is|not|a|perfect|simulation|as|we|are|not|tracking|head-to-head|results|,|which|for|certain|tie-breaker|scenarios|would|create|more|situations|to|track|,|and|we|'re|using|a|simplification|here|as|Worlds|is|around|the|corner|(|29th|September|2023|)|we|'re|almost|at|the|Main|stage|(|17th|October|2023|)|.|The|logic|we|need|to|consider|follows|:|If|there|are|two|highest-scoring|teams|with|unique|scores|,|we|select|them|accordingly|If|there|are|two|highest-scoring|teams|with|non-unique|scores|,|we|need|to|play|a|tie-breaker|match|to|determine|1st|and|2nd|seeds|If|there|is|a|highest|scoring|team|with|a|unique|score|and|two|teams|tied|for|runner-up|,|the|runner-up|teams|must|play|a|tie-breaker|for|2nd|seed|If|there|are|three|highest-scoring|teams|sharing|the|same|score|,|they|would|be|seeded|into|a|tie-breaker|bracket|.|The|team|with|a|buy|would|usually|be|the|team|with|the|lowest|average|game|time|in|their|victories|,|however|as|we|do|not|simulate|game|time|,|the|team|with|a|buy|will|be|selected|at|random|and|therefore|will|receive|side|selection|in|their|match|as|they|have|been|chosen|to|have|the|lowest|average|game|time|If|there|is|a|highest|scoring|team|with|a|unique|score|and|the|other|three|teams|share|the|same|lowest|score|,|they|would|be|seeded|into|a|tie-breaker|bracket|for|runner-up|.|The|team|with|a|buy|would|usually|be|the|team|with|the|lowest|average|game|time|in|their|victories|,|however|as|we|do|not|simulate|game|time|,|the|team|with|a|buy|will|be|selected|at|random|and|therefore|will|receive|side|selection|in|their|match|as|they|have|been|chosen|to|have|the|lowest|average|game|time|If|all|4|teams|share|the|same|scoreline|,|then|they|are|seeded|into|a|single|elimination|bracket|with|seeding|decided|by|victory|time|.|We|will|randomly|seed|the|4|teams|into|the|single|elimination|bracket|with|1st|and|2nd|Seeds|awarded|to|the|Winners|and|Runners-up|of|the|bracket|with|side|selection|decided|randomly|The|successful|8|teams|are|finally|seeded|into|a|single|elimination|best|of|5|bracket|,|where|the|only|stipulations|are|that|a|1st|Seed|team|must|face|off|against|a|2nd|Seed|team|in|the|quarter-finals|and|that|teams|from|the|same|group|can|not|be|seeded|into|the|same|half|of|the|bracket|.|Old|Format|-|Results|(|Historic|Results|Images|)|What|does|our|simulation|say|about|the|Winner|of|Worlds|?|The|headline|estimate|figures|under|the|simulation|are|that|there|is|an|approximate|3|%|chance|any|Western|team|happens|to|be|the|one|to|win|Worlds|under|the|previous|format|,|which|as|over|the|8|and|a|half|years|of|data|used|no|Western|team|has|won|Worlds|holds|up|in|line|with|reality|.|Of|the|8|actual|winners|in|the|data|set|,|5|were|from|South|Korea|(|62.5|%|)|and|3|were|from|China|(|37.5|%|)|,|with|the|simulation|following|a|similar|suit|giving|China|33.3|%|of|all|victories|and|South|Korea|63.5|%|.|Therefore|,|at|least|for|the|overall|winner|,|the|simulation|appears|to|be|a|good|historical|reflection|of|reality|.|Plus|,|shout-out|to|the|6/10000|scenarios|where|the|PCS|won|!|(|https|:|//github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave/blob/main/Reddit|%|20Post/Historic|%|20Win.png|)|How|about|the|Runners|Up|?|Collectively|we|have|about|a|12|%|chance|of|being|eliminated|as|the|Runners-up|.|Historically|we|'ve|occupied|2/8|(|25|%|)|of|the|runner-up|positions|,|which|at|first|glance|makes|the|simulation|appear|to|be|understating|the|West|'s|ability|to|reach|the|Finals|.|However|,|considering|that|there|are|very|few|historical|observations|(|only|8|observed|runners-up|)|,|the|simulated|results|are|not|an|extreme|divergence|from|history|,|and|there|is|still|a|strong|argument|that|the|simulation|is|a|fair|representation|of|historic|events|.|One|could|make|the|argument|that|one|of|these|2nd|place|positions|was|due|to|two|Western|teams|meeting|in|the|semi-finals|(|a|very|rare|event|,|guaranteeing|a|Western|top|two|position|)|,|and|therefore|the|correct|historic|percentage|should|be|the|number|of|times|a|Western|team|has|defeated|an|Eastern|team|in|the|semi-finals|(|G2|in|2019|is|the|only|historical|example|,|giving|12.5|%|observed|vs|12|%|simulated|estimate|)|.|This|is|not|to|make|a|comment|on|how|strong|Fnatic|2018|were|in|comparison|to|their|peers|at|the|time|,|as|some|considered|them|to|potentially|be|the|best|Western|team|we|had|ever|sent|,|and|they|finished|their|group|as|the|1st|seed|,|winning|against|Invictus|Gaming|(|the|eventual|winners|)|in|their|tie-breaker|match|.|However|,|if|we|consider|finals|match|occurrences|,|rather|than|winner|or|runners-up|,|the|historic|simulation|looks|even|better|justified|.|Finally|(|and|most|importantly|)|,|what|about|the|number|of|scenarios|where|no|one|qualifies|?|Arguably|more|important|is|the|number|of|simulated|tournaments|where|no|Western|team|makes|it|to|groups|.|As|the|previous|analysis|looks|at|the|proportion|that|a|team|appears|in|any|randomly|chosen|position|independent|of|other|teams|,|we|actually|lose|implicit|information|about|those|very|teams|.|Worlds|does|not|occur|in|a|vacuum|,|and|there|are|16|teams|all|dependent|on|one|another|for|positions|.|We|see|that|under|the|conditions|set|out|the|chance|no|Western|team|made|it|to|groups|is|approximately|1|%|.|We|also|see|how|the|chance|of|a|Western|team|making|it|to|the|Semi-Finals|and|the|Finals|drops|drastically|as|the|Western|teams|now|have|to|win|a|best|of|5|(|most|likely|against|either|China|or|Korea|)|to|advance|.|While|we|appear|in|nearly|15|%|of|finals|(|comparative|to|our|12.5|%|occupancy|rate|of|total|collective|finals|positions|,|where|the|difference|occurs|when|the|West|has|two|teams|in|the|Final|)|,|the|final|3|%|chance|to|bring|home|the|Worlds|trophy|still|holds|from|the|beginning|of|this|analysis|(|https|:|//github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave/blob/main/Reddit|%|20Post/Historic|%|20Appearence|%|20Chance.png|)|.|Old|Format|Critique|-|Improved|Resolution|One|significant|criticism|of|the|simulation|for|the|historic|format|is|that|we|no|longer|live|in|2019|,|we|live|in|2023|.|While|it|is|important|to|test|assumptions|by|simulating|the|historic|structure|our|data|originates|from|,|one|can|not|claim|that|the|differences|between|a|2019|structure|and|the|modern|2023|structure|are|only|because|of|the|introduction|of|the|Swiss|stage|.|Therefore|we|look|to|simulate|a|hyperthetical|2023|groups|stage|to|evaluate|the|impact|the|introduction|of|Swiss|has|had|on|Western|teams|.|Therefore|we|will|label|our|previous|simulation|that|looked|at|a|historic|format|as|the|'Historic|'|simulation|,|our|comparison|simulation|with|a|hypothetical|bracket|'Swissless|'|,|and|the|final|2023|simulation|as|``|Swiss|''|.|The|important|difference|between|the|Historic|and|Swissless|formats|is|the|inclusion|of|a|4th|seeds|from|the|most|competitive|regions|,|which|as|we|have|identified|from|the|2019|simulation|,|will|likely|have|a|hugely|significant|impact|on|the|results|as|the|difference|between|the|proportional|30|%|of|knockouts|slots|and|99|%|chance|to|not|miss|knockouts|is|likely|to|change|drastically|if|there|are|enough|LPL|and|LCK|teams|to|fill|all|8|knockout|slots|.|Therefore|,|in|order|to|simulate|a|Swissless|Worlds|2023|,|we|look|to|base|the|simulation|heavily|on|the|2022|Worlds|format|,|and|while|the|draw|rules|were|slightly|different|with|4|pools|rather|than|3|because|we|do|not|consider|the|strength|of|each|seed|,|it|has|no|impact|on|the|simulation|.|'Swissless|'|2023|Simulation|(|Swissless|Results|Images|)|Worlds|2022|saw|4|LPL|teams|,|4|LCK|teams|,|4|LEC|teams|,|3|LCS|teams|,|2|PCS|,|2|VCS|,|1|LLA|,|1|LJL|,|1|CBL|,|1|LCO|(|Oceania|)|and|1|TCL|(|Turkey|)|.|For|our|Swissless|simulation|,|we|follow|Worlds|2022|with|3|LCK|,|3|LPL|,|2|LEC|,|2|LCS|,|1|PCS|and|1|VCS|teams|automatically|seeded|into|the|groups|stage|,|and|will|further|seed|the|4th|LCK|and|LPL|seeds|,|3rd|LEC|and|LCS|seeds|(|as|those|are|the|teams|that|passed|the|2022|play-ins|stage|)|.|We|see|that|while|we|estimated|the|West|to|win|about|3|%|of|Worlds|under|the|2019|rules|,|under|a|hypothetical|Swissless|structure|we|only|estimate|a|chance|of|1.4|%|for|a|Western|team|to|win|the|tournament|(|https|:|//github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave/blob/main/Reddit|%|20Post/Swissless|%|20Win.png|)|.|Further|,|we|see|how|the|probability|of|a|Western|team|appearing|at|any|stage|in|the|knockouts|has|dropped|significantly|,|with|there|now|being|a|10|%|chance|no|Western|team|makes|knockouts|,|and|only|an|approximate|6.8|%|chance|of|a|Western|team|making|a|Finals|appearance|(|https|:|//github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave/blob/main/Reddit|%|20Post/Swissless|%|20Appearence|%|20Chance.png|)|.|As|expected|China|and|Korea|are|effectively|guaranteed|to|appear|in|the|knockout|stage|,|with|approximate|91|%|and|99|%|odds|to|make|it|to|semi-finals|,|and|a|further|89|%|and|64|%|odds|respectively|to|have|a|team|appear|in|the|finals|.|Ultimately|,|when|considering|a|deep|run|at|Worlds|as|a|team|from|the|West|making|it|to|the|finals|,|the|addition|of|a|4th|seed|from|the|LPL|and|LEC|more|than|halved|the|Western|chance|.|The|New|Tournament|Structure|Play-Ins|First|,|we|need|to|find|the|final|Western|representative|,|which|is|simply|achieved|by|running|a|Bo5|between|LEC4|and|LCS4|.|The|Play-Ins|feature|8|teams|facing|off|in|two|double|elimination|Bo3|brackets|that|were|drawn|at|random|after|the|conclusion|of|the|LEC|finals|on|10th|September|2023|,|so|the|bracket|order|and|placement|of|teams|are|known|to|us|.|However|,|we|will|sample|the|bracket|randomly|just|as|Riot|has|done|in|order|to|fully|simulate|a|Worlds|run|under|the|current|rules|.|The|8|teams|are|seeded|via|two|pools|into|the|2|double|elimination|brackets|where|two|teams|will|qualify|for|the|Swiss|stage|.|The|Play-Ins|conclude|with|the|upper-bracket|finalist|from|one|bracket|facing|off|against|the|lower-bracket|finalist|from|the|opposing|bracket|in|a|best|of|5|.|Seeding|rules|that|we|seed|all|Pool|1|teams|(|upper|tier|)|,|then|all|Pool|2|(|lower|tier|)|,|where|any|team|from|the|same|Region|must|be|in|different|brackets|,|and|any|team|from|the|same|Pool|can|not|face|each|other|in|the|first|round|.|Pool|1|:|PCS1|(|PSG|)|,|PCS2|(|CFO|)|,|VCS1|(|GAM|)|&|WQS|Winner|(|LEC4|vs|LCS4|in|a|Bo5|)|Pool|2|:|VCS2|(|TW|)|,|LJL1|(|DFM|)|,|LLA1|(|R7|)|&|CBL1|(|LLL|)|Swiss|Stage|Teams|in|the|Swiss|stage|will|always|face|an|opponent|with|an|equivalent|scoreline|throughout|(|potentially|)|5|rounds|.|Teams|play|in|a|single|Bo1|(|which|has|massive|implications|for|the|impact|of|Blue|advantage|)|unless|playing|in|a|qualification|(|to|the|knockouts|)|or|elimination|(|lose|and|you|'re|removed|from|the|tournament|)|match|where|they|play|a|Bo3|(|https|:|//lolesports.com/article/state-of-the-game-lol-esports-in-2023/blt5d3bca31d1b39e0c|)|.|Update|12th|Sep|2023|:|Following|the|Primer|,|we|now|know|how|the|teams|are|seeded|into|the|Swiss|bracket|.|Seeds|1|must|fight|a|4th|Seed|,|and|a|2nd|Seed|must|fight|a|3rd|Seed|,|where|seeds|1,2|and|3|are|from|their|respective|major|regions|,|and|4th|Seeds|are|LCK4|,|LPL4|and|the|two|Play-In|winners|.|We|need|to|match|the|pairs|and|then|randomly|allocate|these|pairs|to|matches|to|suitably|explore|the|full|state|space|as|a|requirement|for|valid|results|for|a|Monte|Carlo|estimate|.|Further|,|throughout|the|Swiss|bracket|,|side|selection|is|not|random|as|I|had|first|coded|up|The|most|challenging|part|of|this|is|that|Riot|Games|is|very|tight|on|information|on|how|the|teams|will|be|matched|up|throughout|the|bracket|.|For|example|,|we|do|n't|know|if|a|team|that|wins|in|round|3|from|1|1|to|have|a|scoreline|of|2|1|is|equivalent|to|a|team|that|lost|the|upper|bracket|to|now|be|at|2|1|.|Are|these|teams|treated|the|same|or|do|we|randomly|match|these|teams|up|?|If|we|do|fix|the|Upper|and|Middle|bracket|teams|in|their|own|pools|,|then|who|gets|the|higher|seed|if|they|win|?|I|'ve|thought|it|best|to|randomly|sample|both|the|Upper|and|Lower|brackets|in|round|4|,|and|we|will|perform|an|extra|random|draw|step|going|into|the|Knockout|stage|.|Defining|and|Running|the|Worlds|2023|Simulation|We|'re|going|to|run|for|10,000|tournaments|.|I|would|like|to|run|more|but|this|running|on|god|knows|what|free|Google|Cloud|machine|.|For|an|industrial|simulation|,|10,000|would|not|usually|be|sufficient|,|however|as|it|is|already|the|15th|of|October|,|there|is|a|limited|state|space|with|the|model|to|explore|,|we|'re|not|learning|any|parameters|and|we|'re|looking|at|unlikely|but|not|extreme|events|,|it|'s|perfectly|long|enough|.|For|any|results|,|one|could|estimate|a|likely|absolute|error|interval|of|about|0.5|%|-1|%|in|the|most|extreme|cases|.|Results|-|Winners|(|Swiss|Results|Images|)|The|headline|figure|to|highlight|is|that|the|odds|of|a|Western|team|winning|Worlds|have|shot|up|by|approximately|just|over|33|%|relative|to|their|estimated|chances|from|the|Swissless|simulation|,|to|a|tournament-winning|chance|of|about|2.1|%|.|This|gives|the|West|a|real-term|odds|increase|of|about|0.6|%|for|Worlds|2023|,|where|we|see|the|odds|of|Korea|or|China|ending|up|victorious|only|decline|in|real|terms|by|about|0.5|%|each|.|However|,|we|specifically|see|the|biggest|benefit|for|Europe|,|with|them|appearing|in|roughly|1.5|times|the|number|of|victories|when|compared|to|the|Swissless|simulation|(|https|:|//github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave/blob/main/Reddit|%|20Post/Swiss|%|20Win.png|)|.|For|runner-up|,|we|observe|a|similar|change|,|where|the|Western|chance|of|appearing|here|increases|from|approximately|5.4|%|to|8.3|%|(|54|%|increase|)|at|the|expense|of|the|Eastern|teams|under|the|new|format|.|We|further|observe|some|impressive|Minor|region|2nd|place|finishes|within|our|10,000|simulated|Worlds|,|implying|there|are|better|chances|for|a|Minor|region|team|making|a|deep|run|in|the|new|format|,|although|the|simulation|was|not|run|long|enough|to|make|a|reliable|estimate|for|the|exact|percentage|chance|(|https|:|//github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave/blob/main/Reddit|%|20Post/Swiss|%|20Runners|%|20Up.png|)|.|Finally|,|we|observe|how|the|Swiss|changes|have|improved|the|chances|of|seeing|a|Western|team|make|it|through|to|the|knockout|stage|,|with|the|odds|of|at|least|one|Western|team|making|it|out|improving|from|84.5|%|to|96.7|%|,|with|a|further|improvement|of|a|Western|team|making|it|to|finals|increasing|from|6.8|%|to|10.24|%|(|approximately|a|50|%|increase|)|(|https|:|//github.com/JordanJHood/StochasticSim-LeagueOfLegends-Worlds2023-WhatImpactWillSwissHave/blob/main/Reddit|%|20Post/Swiss|%|20Appearence|%|20Chance.png|)|.|Discussion|While|an|extra|1/200|Worlds|victories|does|n't|sound|amazing|,|almost|any|team|would|jump|at|a|chance|to|improve|their|odds|of|success|by|33|%|!|Meanwhile|,|while|the|Eastern|teams|do|see|their|real-world|chance|of|victory|drop|by|about|0.2-0.5|%|each|,|this|has|only|decreased|their|respective|chances|of|winning|Worlds|by|about|1|%|compared|to|their|Swissless|odds|,|which|only|goes|to|demonstrate|the|sheer|dominance|of|the|Eastern|teams|in|this|tournament|.|There|is|an|interesting|question|to|be|answered|as|to|why|the|Swiss|format|,|which|in|theory|should|reward|the|superior|regions|(|remember|their|win|rate|has|n't|changed|with|the|new|format|on|a|per-game|basis|)|has|resulted|in|a|relatively|considerable|improvement|for|the|Western|teams|'|chances|of|making|a|meaningful|run|?|I|conjecture|that|the|answer|lies|in|how|the|Swiss|stage|is|paired|up|.|After|the|first|round|,|where|teams|are|drawn|into|matches|based|on|their|pools|,|the|teams|are|then|randomly|paired|with|opponents|of|equal|score|.|As|there|are|likely|to|be|some|Korean|vs|Chinese|matches|in|round|1|,|it|simply|takes|some|occurrence|in|certain|rounds|for|an|LCK|or|LPL|team|to|be|playing|one|another|at|both|the|0|0|,|1|0|and|0|1|scorelines|,|where|the|only|block|for|a|Western|team|is|potentially|to|win|a|best|of|three|.|Further|,|with|the|West|likely|to|fulfil|slots|3|through|8|,|there|will|be|multiple|scenarios|where|say|seed|4|and|seed|7|,|as|both|Western|teams|,|have|the|potential|to|be|drawn|together|.|While|this|is|not|likely|,|this|scenario|is|more|likely|than|a|Western|team|in|the|Swissless|structure|drawn|into|say|group|A|placing|1st|Seed|against|both|an|LCK|and|LPL|team|,|a|Western|team|in|any|other|group|places|2nd|seed|beating|out|either|their|respective|LCK|or|LPL|team|and|both|teams|being|drawn|into|a|Quarter|Finals|match|.|I|believe|the|reason|for|the|improved|Western|odds|this|year|is|because|as|a|part|of|the|Swissless|environment|,|with|both|4|LCK|and|4|LPL|teams|,|every|Western|team|is|forced|to|overcome|both|a|Korean|and|a|Chinese|team|across|4|matches|to|obtain|a|beneficial|Quarter-Finals|match|.|This|study|shows|that|we|can|expect|the|West|,|which|we|model|as|objectively|weaker|than|Chinese|and|Korean|teams|,|to|have|a|greater|chance|both|to|make|it|to|the|knockouts|thereby|eliminating|one|of|those|eight|teams|and|making|it|all|the|way|to|the|finals|match|,|even|winning|the|tournament|.|This|certainly|implies|the|Swiss|format|is|far|more|volatile|than|the|Groups|system|,|and|whether|this|is|a|worse|format|because|of|it|is|up|for|debate|,|as|one|could|argue|that|increasing|the|possibility|of|engaging|the|entire|Western|fan|base|by|enabling|at|least|one|Western|team|to|make|it|to|the|Knockout|stage|is|a|good|system|.|After|all|,|everybody|loves|a|good|underdog|storyline|.|Ultimately|,|Western|fans|should|be|quietly|hopeful|for|one|of|our|representatives|to|perform|better|than|expected|this|year|based|on|format|changes|alone|.|For|anyone|who|has|made|it|this|far|,|thank|you|for|taking|the|time|to|read|this|all|.|I|'m|very|open|to|any|and|all|feedback|about|this|project|.|There|is|so|much|more|that|could|be|investigated|!|Ideas|I|have|are|:|Changes|in|the|Number|of|Seeds|(|3|to|4|LPL|and|LCK|seeds|were|MASSIVE|)|Clear|classification|between|China/Korea|and|Europe/North|America|(|Major|,|Moderate|and|Minor|Regions|)|Strength|of|Seeds|-|some|adjustment|for|1st|and|3/4th|seeds|Warm-up|and|fatigue|effects|on|different|regions\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'|'.join(df_raw.loc[df_raw.Source == 'Reddit'].reset_index(drop = True).token_words[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Provided', 'proper', 'attribution', 'is', 'provided', ',', 'Google', 'hereby', 'grants', 'permission', 'to', 'reproduce', 'the', 'tables', 'and', 'figures', 'in', 'this', 'paper', 'solely', 'for', 'use', 'in', 'journalistic', 'or', 'scholarly', 'works', '.', 'Attention', 'Is', 'All', 'You', 'Need', 'Ashish', 'Vaswaniâˆ—', 'Google', 'Brain', 'avaswani', '@', 'google.comNoam', 'Shazeerâˆ—', 'Google', 'Brain', 'noam', '@', 'google.comNiki', 'Parmarâˆ—', 'Google', 'Research', 'nikip', '@', 'google.comJakob', 'Uszkoreitâˆ—', 'Google', 'Research', 'usz', '@', 'google.com', 'Llion', 'Jonesâˆ—', 'Google', 'Research', 'llion', '@', 'google.comAidan', 'N.', 'Gomezâˆ—', 'â€ ', 'University', 'of', 'Toronto', 'aidan', '@', 'cs.toronto.eduÅukasz', 'Kaiserâˆ—', 'Google', 'Brain', 'lukaszkaiser', '@', 'google.com', 'Illia', 'Polosukhinâˆ—', 'â€¡', 'illia.polosukhin', '@', 'gmail.com', 'Abstract', 'The', 'dominant', 'sequence', 'transduction', 'models', 'are', 'based', 'on', 'complex', 'recurrent', 'or', 'convolutional', 'neural', 'networks', 'that', 'include', 'an', 'encoder', 'and', 'a', 'decoder', '.', 'The', 'best', 'performing', 'models', 'also', 'connect', 'the', 'encoder', 'and', 'decoder', 'through', 'an', 'attention', 'mechanism', '.', 'We', 'propose', 'a', 'new', 'simple', 'network', 'architecture', ',', 'the', 'Transformer', ',', 'based', 'solely', 'on', 'attention', 'mechanisms', ',', 'dispensing', 'with', 'recurrence', 'and', 'convolutions', 'entirely', '.', 'Experiments', 'on', 'two', 'machine', 'translation', 'tasks', 'show', 'these', 'models', 'to', 'be', 'superior', 'in', 'quality', 'while', 'being', 'more', 'parallelizable', 'and', 'requiring', 'significantly', 'less', 'time', 'to', 'train', '.', 'Our', 'model', 'achieves', '28.4', 'BLEU', 'on', 'the', 'WMT', '2014', 'English-', 'to-German', 'translation', 'task', ',', 'improving', 'over', 'the', 'existing', 'best', 'results', ',', 'including', 'ensembles', ',', 'by', 'over', '2', 'BLEU', '.', 'On', 'the', 'WMT', '2014', 'English-to-French', 'translation', 'task', ',', 'our', 'model', 'establishes', 'a', 'new', 'single-model', 'state-of-the-art', 'BLEU', 'score', 'of', '41.8', 'after', 'training', 'for', '3.5', 'days', 'on', 'eight', 'GPUs', ',', 'a', 'small', 'fraction', 'of', 'the', 'training', 'costs', 'of', 'the', 'best', 'models', 'from', 'the', 'literature', '.', 'We', 'show', 'that', 'the', 'Transformer', 'generalizes', 'well', 'to', 'other', 'tasks', 'by', 'applying', 'it', 'successfully', 'to', 'English', 'constituency', 'parsing', 'both', 'with', 'large', 'and', 'limited', 'training', 'data', '.', 'âˆ—Equal', 'contribution', '.', 'Listing', 'order', 'is', 'random', '.', 'Jakob', 'proposed', 'replacing', 'RNNs', 'with', 'self-attention', 'and', 'started', 'the', 'effort', 'to', 'evaluate', 'this', 'idea', '.', 'Ashish', ',', 'with', 'Illia', ',', 'designed', 'and', 'implemented', 'the', 'first', 'Transformer', 'models', 'and', 'has', 'been', 'crucially', 'involved', 'in', 'every', 'aspect', 'of', 'this', 'work', '.', 'Noam', 'proposed', 'scaled', 'dot-product', 'attention', ',', 'multi-head', 'attention', 'and', 'the', 'parameter-free', 'position', 'representation', 'and', 'became', 'the', 'other', 'person', 'involved', 'in', 'nearly', 'every', 'detail', '.', 'Niki', 'designed', ',', 'implemented', ',', 'tuned', 'and', 'evaluated', 'countless', 'model', 'variants', 'in', 'our', 'original', 'codebase', 'and', 'tensor2tensor', '.', 'Llion', 'also', 'experimented', 'with', 'novel', 'model', 'variants', ',', 'was', 'responsible', 'for', 'our', 'initial', 'codebase', ',', 'and', 'efficient', 'inference', 'and', 'visualizations', '.', 'Lukasz', 'and', 'Aidan', 'spent', 'countless', 'long', 'days', 'designing', 'various', 'parts', 'of', 'and', 'implementing', 'tensor2tensor', ',', 'replacing', 'our', 'earlier', 'codebase', ',', 'greatly', 'improving', 'results', 'and', 'massively', 'accelerating', 'our', 'research', '.', 'â€ Work', 'performed', 'while', 'at', 'Google', 'Brain', '.', 'â€¡Work', 'performed', 'while', 'at', 'Google', 'Research', '.', '31st', 'Conference', 'on', 'Neural', 'Information', 'Processing', 'Systems', '(', 'NIPS', '2017', ')', ',', 'Long', 'Beach', ',', 'CA', ',', 'USA.arXiv:1706.03762v7', '[', 'cs.CL', ']', '2', 'Aug', '2023', '==============================', 'End', 'of', 'Page', '1', '==============================', '1', 'Introduction', 'Recurrent', 'neural', 'networks', ',', 'long', 'short-term', 'memory', '[', '13', ']', 'and', 'gated', 'recurrent', '[', '7', ']', 'neural', 'networks', 'in', 'particular', ',', 'have', 'been', 'firmly', 'established', 'as', 'state', 'of', 'the', 'art', 'approaches', 'in', 'sequence', 'modeling', 'and', 'transduction', 'problems', 'such', 'as', 'language', 'modeling', 'and', 'machine', 'translation', '[', '35,2,5', ']', '.', 'Numerous', 'efforts', 'have', 'since', 'continued', 'to', 'push', 'the', 'boundaries', 'of', 'recurrent', 'language', 'models', 'and', 'encoder-decoder', 'architectures', '[', '38', ',', '24', ',', '15', ']', '.', 'Recurrent', 'models', 'typically', 'factor', 'computation', 'along', 'the', 'symbol', 'positions', 'of', 'the', 'input', 'and', 'output', 'sequences', '.', 'Aligning', 'the', 'positions', 'to', 'steps', 'in', 'computation', 'time', ',', 'they', 'generate', 'a', 'sequence', 'of', 'hidden', 'states', 'ht', ',', 'as', 'a', 'function', 'of', 'the', 'previous', 'hidden', 'state', 'htâˆ’1and', 'the', 'input', 'for', 'position', 't.', 'This', 'inherently', 'sequential', 'nature', 'precludes', 'parallelization', 'within', 'training', 'examples', ',', 'which', 'becomes', 'critical', 'at', 'longer', 'sequence', 'lengths', ',', 'as', 'memory', 'constraints', 'limit', 'batching', 'across', 'examples', '.', 'Recent', 'work', 'has', 'achieved', 'significant', 'improvements', 'in', 'computational', 'efficiency', 'through', 'factorization', 'tricks', '[', '21', ']', 'and', 'conditional', 'computation', '[', '32', ']', ',', 'while', 'also', 'improving', 'model', 'performance', 'in', 'case', 'of', 'the', 'latter', '.', 'The', 'fundamental', 'constraint', 'of', 'sequential', 'computation', ',', 'however', ',', 'remains', '.', 'Attention', 'mechanisms', 'have', 'become', 'an', 'integral', 'part', 'of', 'compelling', 'sequence', 'modeling', 'and', 'transduc-', 'tion', 'models', 'in', 'various', 'tasks', ',', 'allowing', 'modeling', 'of', 'dependencies', 'without', 'regard', 'to', 'their', 'distance', 'in', 'the', 'input', 'or', 'output', 'sequences', '[', '2,19', ']', '.', 'In', 'all', 'but', 'a', 'few', 'cases', '[', '27', ']', ',', 'however', ',', 'such', 'attention', 'mechanisms', 'are', 'used', 'in', 'conjunction', 'with', 'a', 'recurrent', 'network', '.', 'In', 'this', 'work', 'we', 'propose', 'the', 'Transformer', ',', 'a', 'model', 'architecture', 'eschewing', 'recurrence', 'and', 'instead', 'relying', 'entirely', 'on', 'an', 'attention', 'mechanism', 'to', 'draw', 'global', 'dependencies', 'between', 'input', 'and', 'output', '.', 'The', 'Transformer', 'allows', 'for', 'significantly', 'more', 'parallelization', 'and', 'can', 'reach', 'a', 'new', 'state', 'of', 'the', 'art', 'in', 'translation', 'quality', 'after', 'being', 'trained', 'for', 'as', 'little', 'as', 'twelve', 'hours', 'on', 'eight', 'P100', 'GPUs', '.', '2', 'Background', 'The', 'goal', 'of', 'reducing', 'sequential', 'computation', 'also', 'forms', 'the', 'foundation', 'of', 'the', 'Extended', 'Neural', 'GPU', '[', '16', ']', ',', 'ByteNet', '[', '18', ']', 'and', 'ConvS2S', '[', '9', ']', ',', 'all', 'of', 'which', 'use', 'convolutional', 'neural', 'networks', 'as', 'basic', 'building', 'block', ',', 'computing', 'hidden', 'representations', 'in', 'parallel', 'for', 'all', 'input', 'and', 'output', 'positions', '.', 'In', 'these', 'models', ',', 'the', 'number', 'of', 'operations', 'required', 'to', 'relate', 'signals', 'from', 'two', 'arbitrary', 'input', 'or', 'output', 'positions', 'grows', 'in', 'the', 'distance', 'between', 'positions', ',', 'linearly', 'for', 'ConvS2S', 'and', 'logarithmically', 'for', 'ByteNet', '.', 'This', 'makes', 'it', 'more', 'difficult', 'to', 'learn', 'dependencies', 'between', 'distant', 'positions', '[', '12', ']', '.', 'In', 'the', 'Transformer', 'this', 'is', 'reduced', 'to', 'a', 'constant', 'number', 'of', 'operations', ',', 'albeit', 'at', 'the', 'cost', 'of', 'reduced', 'effective', 'resolution', 'due', 'to', 'averaging', 'attention-weighted', 'positions', ',', 'an', 'effect', 'we', 'counteract', 'with', 'Multi-Head', 'Attention', 'as', 'described', 'in', 'section', '3.2', '.', 'Self-attention', ',', 'sometimes', 'called', 'intra-attention', 'is', 'an', 'attention', 'mechanism', 'relating', 'different', 'positions', 'of', 'a', 'single', 'sequence', 'in', 'order', 'to', 'compute', 'a', 'representation', 'of', 'the', 'sequence', '.', 'Self-attention', 'has', 'been', 'used', 'successfully', 'in', 'a', 'variety', 'of', 'tasks', 'including', 'reading', 'comprehension', ',', 'abstractive', 'summarization', ',', 'textual', 'entailment', 'and', 'learning', 'task-independent', 'sentence', 'representations', '[', '4', ',', '27', ',', '28', ',', '22', ']', '.', 'End-to-end', 'memory', 'networks', 'are', 'based', 'on', 'a', 'recurrent', 'attention', 'mechanism', 'instead', 'of', 'sequence-', 'aligned', 'recurrence', 'and', 'have', 'been', 'shown', 'to', 'perform', 'well', 'on', 'simple-language', 'question', 'answering', 'and', 'language', 'modeling', 'tasks', '[', '34', ']', '.', 'To', 'the', 'best', 'of', 'our', 'knowledge', ',', 'however', ',', 'the', 'Transformer', 'is', 'the', 'first', 'transduction', 'model', 'relying', 'entirely', 'on', 'self-attention', 'to', 'compute', 'representations', 'of', 'its', 'input', 'and', 'output', 'without', 'using', 'sequence-', 'aligned', 'RNNs', 'or', 'convolution', '.', 'In', 'the', 'following', 'sections', ',', 'we', 'will', 'describe', 'the', 'Transformer', ',', 'motivate', 'self-attention', 'and', 'discuss', 'its', 'advantages', 'over', 'models', 'such', 'as', '[', '17', ',', '18', ']', 'and', '[', '9', ']', '.', '3', 'Model', 'Architecture', 'Most', 'competitive', 'neural', 'sequence', 'transduction', 'models', 'have', 'an', 'encoder-decoder', 'structure', '[', '5,2,35', ']', '.', 'Here', ',', 'the', 'encoder', 'maps', 'an', 'input', 'sequence', 'of', 'symbol', 'representations', '(', 'x1', ',', '...', ',', 'x', 'n', ')', 'to', 'a', 'sequence', 'of', 'continuous', 'representations', 'z=', '(', 'z1', ',', '...', ',', 'z', 'n', ')', '.', 'Given', 'z', ',', 'the', 'decoder', 'then', 'generates', 'an', 'output', 'sequence', '(', 'y1', ',', '...', ',', 'y', 'm', ')', 'of', 'symbols', 'one', 'element', 'at', 'a', 'time', '.', 'At', 'each', 'step', 'the', 'model', 'is', 'auto-regressive', '[', '10', ']', ',', 'consuming', 'the', 'previously', 'generated', 'symbols', 'as', 'additional', 'input', 'when', 'generating', 'the', 'next', '.', '2', '==============================', 'End', 'of', 'Page', '2', '==============================', 'Figure', '1', ':', 'The', 'Transformer', '-', 'model', 'architecture', '.', 'The', 'Transformer', 'follows', 'this', 'overall', 'architecture', 'using', 'stacked', 'self-attention', 'and', 'point-wise', ',', 'fully', 'connected', 'layers', 'for', 'both', 'the', 'encoder', 'and', 'decoder', ',', 'shown', 'in', 'the', 'left', 'and', 'right', 'halves', 'of', 'Figure', '1', ',', 'respectively', '.', '3.1', 'Encoder', 'and', 'Decoder', 'Stacks', 'Encoder', ':', 'The', 'encoder', 'is', 'composed', 'of', 'a', 'stack', 'of', 'N=', '6', 'identical', 'layers', '.', 'Each', 'layer', 'has', 'two', 'sub-layers', '.', 'The', 'first', 'is', 'a', 'multi-head', 'self-attention', 'mechanism', ',', 'and', 'the', 'second', 'is', 'a', 'simple', ',', 'position-', 'wise', 'fully', 'connected', 'feed-forward', 'network', '.', 'We', 'employ', 'a', 'residual', 'connection', '[', '11', ']', 'around', 'each', 'of', 'the', 'two', 'sub-layers', ',', 'followed', 'by', 'layer', 'normalization', '[', '1', ']', '.', 'That', 'is', ',', 'the', 'output', 'of', 'each', 'sub-layer', 'is', 'LayerNorm', '(', 'x+', 'Sublayer', '(', 'x', ')', ')', ',', 'where', 'Sublayer', '(', 'x', ')', 'is', 'the', 'function', 'implemented', 'by', 'the', 'sub-layer', 'itself', '.', 'To', 'facilitate', 'these', 'residual', 'connections', ',', 'all', 'sub-layers', 'in', 'the', 'model', ',', 'as', 'well', 'as', 'the', 'embedding', 'layers', ',', 'produce', 'outputs', 'of', 'dimension', 'dmodel', '=', '512', '.', 'Decoder', ':', 'The', 'decoder', 'is', 'also', 'composed', 'of', 'a', 'stack', 'of', 'N=', '6identical', 'layers', '.', 'In', 'addition', 'to', 'the', 'two', 'sub-layers', 'in', 'each', 'encoder', 'layer', ',', 'the', 'decoder', 'inserts', 'a', 'third', 'sub-layer', ',', 'which', 'performs', 'multi-head', 'attention', 'over', 'the', 'output', 'of', 'the', 'encoder', 'stack', '.', 'Similar', 'to', 'the', 'encoder', ',', 'we', 'employ', 'residual', 'connections', 'around', 'each', 'of', 'the', 'sub-layers', ',', 'followed', 'by', 'layer', 'normalization', '.', 'We', 'also', 'modify', 'the', 'self-attention', 'sub-layer', 'in', 'the', 'decoder', 'stack', 'to', 'prevent', 'positions', 'from', 'attending', 'to', 'subsequent', 'positions', '.', 'This', 'masking', ',', 'combined', 'with', 'fact', 'that', 'the', 'output', 'embeddings', 'are', 'offset', 'by', 'one', 'position', ',', 'ensures', 'that', 'the', 'predictions', 'for', 'position', 'ican', 'depend', 'only', 'on', 'the', 'known', 'outputs', 'at', 'positions', 'less', 'than', 'i', '.', '3.2', 'Attention', 'An', 'attention', 'function', 'can', 'be', 'described', 'as', 'mapping', 'a', 'query', 'and', 'a', 'set', 'of', 'key-value', 'pairs', 'to', 'an', 'output', ',', 'where', 'the', 'query', ',', 'keys', ',', 'values', ',', 'and', 'output', 'are', 'all', 'vectors', '.', 'The', 'output', 'is', 'computed', 'as', 'a', 'weighted', 'sum', '3', '==============================', 'End', 'of', 'Page', '3', '==============================', 'Scaled', 'Dot-Product', 'Attention', 'Multi-Head', 'Attention', 'Figure', '2', ':', '(', 'left', ')', 'Scaled', 'Dot-Product', 'Attention', '.', '(', 'right', ')', 'Multi-Head', 'Attention', 'consists', 'of', 'several', 'attention', 'layers', 'running', 'in', 'parallel', '.', 'of', 'the', 'values', ',', 'where', 'the', 'weight', 'assigned', 'to', 'each', 'value', 'is', 'computed', 'by', 'a', 'compatibility', 'function', 'of', 'the', 'query', 'with', 'the', 'corresponding', 'key', '.', '3.2.1', 'Scaled', 'Dot-Product', 'Attention', 'We', 'call', 'our', 'particular', 'attention', '``', 'Scaled', 'Dot-Product', 'Attention', \"''\", '(', 'Figure', '2', ')', '.', 'The', 'input', 'consists', 'of', 'queries', 'and', 'keys', 'of', 'dimension', 'dk', ',', 'and', 'values', 'of', 'dimension', 'dv', '.', 'We', 'compute', 'the', 'dot', 'products', 'of', 'the', 'query', 'with', 'all', 'keys', ',', 'divide', 'each', 'byâˆšdk', ',', 'and', 'apply', 'a', 'softmax', 'function', 'to', 'obtain', 'the', 'weights', 'on', 'the', 'values', '.', 'In', 'practice', ',', 'we', 'compute', 'the', 'attention', 'function', 'on', 'a', 'set', 'of', 'queries', 'simultaneously', ',', 'packed', 'together', 'into', 'a', 'matrix', 'Q', '.', 'The', 'keys', 'and', 'values', 'are', 'also', 'packed', 'together', 'into', 'matrices', 'KandV', '.', 'We', 'compute', 'the', 'matrix', 'of', 'outputs', 'as', ':', 'Attention', '(', 'Q', ',', 'K', ',', 'V', ')', '=', 'softmax', '(', 'QKT', 'âˆšdk', ')', 'V', '(', '1', ')', 'The', 'two', 'most', 'commonly', 'used', 'attention', 'functions', 'are', 'additive', 'attention', '[', '2', ']', ',', 'and', 'dot-product', '(', 'multi-', 'plicative', ')', 'attention', '.', 'Dot-product', 'attention', 'is', 'identical', 'to', 'our', 'algorithm', ',', 'except', 'for', 'the', 'scaling', 'factor', 'of1âˆšdk', '.', 'Additive', 'attention', 'computes', 'the', 'compatibility', 'function', 'using', 'a', 'feed-forward', 'network', 'with', 'a', 'single', 'hidden', 'layer', '.', 'While', 'the', 'two', 'are', 'similar', 'in', 'theoretical', 'complexity', ',', 'dot-product', 'attention', 'is', 'much', 'faster', 'and', 'more', 'space-efficient', 'in', 'practice', ',', 'since', 'it', 'can', 'be', 'implemented', 'using', 'highly', 'optimized', 'matrix', 'multiplication', 'code', '.', 'While', 'for', 'small', 'values', 'of', 'dkthe', 'two', 'mechanisms', 'perform', 'similarly', ',', 'additive', 'attention', 'outperforms', 'dot', 'product', 'attention', 'without', 'scaling', 'for', 'larger', 'values', 'of', 'dk', '[', '3', ']', '.', 'We', 'suspect', 'that', 'for', 'large', 'values', 'of', 'dk', ',', 'the', 'dot', 'products', 'grow', 'large', 'in', 'magnitude', ',', 'pushing', 'the', 'softmax', 'function', 'into', 'regions', 'where', 'it', 'has', 'extremely', 'small', 'gradients4', '.', 'To', 'counteract', 'this', 'effect', ',', 'we', 'scale', 'the', 'dot', 'products', 'by1âˆšdk', '.', '3.2.2', 'Multi-Head', 'Attention', 'Instead', 'of', 'performing', 'a', 'single', 'attention', 'function', 'with', 'dmodel-dimensional', 'keys', ',', 'values', 'and', 'queries', ',', 'we', 'found', 'it', 'beneficial', 'to', 'linearly', 'project', 'the', 'queries', ',', 'keys', 'and', 'values', 'htimes', 'with', 'different', ',', 'learned', 'linear', 'projections', 'to', 'dk', ',', 'dkanddvdimensions', ',', 'respectively', '.', 'On', 'each', 'of', 'these', 'projected', 'versions', 'of', 'queries', ',', 'keys', 'and', 'values', 'we', 'then', 'perform', 'the', 'attention', 'function', 'in', 'parallel', ',', 'yielding', 'dv-dimensional', '4To', 'illustrate', 'why', 'the', 'dot', 'products', 'get', 'large', ',', 'assume', 'that', 'the', 'components', 'of', 'qandkare', 'independent', 'random', 'variables', 'with', 'mean', '0and', 'variance', '1', '.', 'Then', 'their', 'dot', 'product', ',', 'qÂ·k=Pdk', 'i=1qiki', ',', 'has', 'mean', '0and', 'variance', 'dk', '.', '4', '==============================', 'End', 'of', 'Page', '4', '==============================', 'output', 'values', '.', 'These', 'are', 'concatenated', 'and', 'once', 'again', 'projected', ',', 'resulting', 'in', 'the', 'final', 'values', ',', 'as', 'depicted', 'in', 'Figure', '2', '.', 'Multi-head', 'attention', 'allows', 'the', 'model', 'to', 'jointly', 'attend', 'to', 'information', 'from', 'different', 'representation', 'subspaces', 'at', 'different', 'positions', '.', 'With', 'a', 'single', 'attention', 'head', ',', 'averaging', 'inhibits', 'this', '.', 'MultiHead', '(', 'Q', ',', 'K', ',', 'V', ')', '=', 'Concat', '(', 'head', '1', ',', '...', ',', 'head', 'h', ')', 'WO', 'where', 'head', 'i=', 'Attention', '(', 'QWQ', 'i', ',', 'KWK', 'i', ',', 'V', 'WV', 'i', ')', 'Where', 'the', 'projections', 'are', 'parameter', 'matrices', 'WQ', 'iâˆˆRdmodelÃ—dk', ',', 'WK', 'iâˆˆRdmodelÃ—dk', ',', 'WV', 'iâˆˆRdmodelÃ—dv', 'andWOâˆˆRhdvÃ—dmodel', '.', 'In', 'this', 'work', 'we', 'employ', 'h=', '8', 'parallel', 'attention', 'layers', ',', 'or', 'heads', '.', 'For', 'each', 'of', 'these', 'we', 'use', 'dk=dv=dmodel/h=', '64', '.', 'Due', 'to', 'the', 'reduced', 'dimension', 'of', 'each', 'head', ',', 'the', 'total', 'computational', 'cost', 'is', 'similar', 'to', 'that', 'of', 'single-head', 'attention', 'with', 'full', 'dimensionality', '.', '3.2.3', 'Applications', 'of', 'Attention', 'in', 'our', 'Model', 'The', 'Transformer', 'uses', 'multi-head', 'attention', 'in', 'three', 'different', 'ways', ':', 'â€¢In', '``', 'encoder-decoder', 'attention', \"''\", 'layers', ',', 'the', 'queries', 'come', 'from', 'the', 'previous', 'decoder', 'layer', ',', 'and', 'the', 'memory', 'keys', 'and', 'values', 'come', 'from', 'the', 'output', 'of', 'the', 'encoder', '.', 'This', 'allows', 'every', 'position', 'in', 'the', 'decoder', 'to', 'attend', 'over', 'all', 'positions', 'in', 'the', 'input', 'sequence', '.', 'This', 'mimics', 'the', 'typical', 'encoder-decoder', 'attention', 'mechanisms', 'in', 'sequence-to-sequence', 'models', 'such', 'as', '[', '38', ',', '2', ',', '9', ']', '.', 'â€¢The', 'encoder', 'contains', 'self-attention', 'layers', '.', 'In', 'a', 'self-attention', 'layer', 'all', 'of', 'the', 'keys', ',', 'values', 'and', 'queries', 'come', 'from', 'the', 'same', 'place', ',', 'in', 'this', 'case', ',', 'the', 'output', 'of', 'the', 'previous', 'layer', 'in', 'the', 'encoder', '.', 'Each', 'position', 'in', 'the', 'encoder', 'can', 'attend', 'to', 'all', 'positions', 'in', 'the', 'previous', 'layer', 'of', 'the', 'encoder', '.', 'â€¢Similarly', ',', 'self-attention', 'layers', 'in', 'the', 'decoder', 'allow', 'each', 'position', 'in', 'the', 'decoder', 'to', 'attend', 'to', 'all', 'positions', 'in', 'the', 'decoder', 'up', 'to', 'and', 'including', 'that', 'position', '.', 'We', 'need', 'to', 'prevent', 'leftward', 'information', 'flow', 'in', 'the', 'decoder', 'to', 'preserve', 'the', 'auto-regressive', 'property', '.', 'We', 'implement', 'this', 'inside', 'of', 'scaled', 'dot-product', 'attention', 'by', 'masking', 'out', '(', 'setting', 'to', 'âˆ’âˆž', ')', 'all', 'values', 'in', 'the', 'input', 'of', 'the', 'softmax', 'which', 'correspond', 'to', 'illegal', 'connections', '.', 'See', 'Figure', '2', '.', '3.3', 'Position-wise', 'Feed-Forward', 'Networks', 'In', 'addition', 'to', 'attention', 'sub-layers', ',', 'each', 'of', 'the', 'layers', 'in', 'our', 'encoder', 'and', 'decoder', 'contains', 'a', 'fully', 'connected', 'feed-forward', 'network', ',', 'which', 'is', 'applied', 'to', 'each', 'position', 'separately', 'and', 'identically', '.', 'This', 'consists', 'of', 'two', 'linear', 'transformations', 'with', 'a', 'ReLU', 'activation', 'in', 'between', '.', 'FFN', '(', 'x', ')', '=', 'max', '(', '0', ',', 'xW', '1+b1', ')', 'W2+b2', '(', '2', ')', 'While', 'the', 'linear', 'transformations', 'are', 'the', 'same', 'across', 'different', 'positions', ',', 'they', 'use', 'different', 'parameters', 'from', 'layer', 'to', 'layer', '.', 'Another', 'way', 'of', 'describing', 'this', 'is', 'as', 'two', 'convolutions', 'with', 'kernel', 'size', '1', '.', 'The', 'dimensionality', 'of', 'input', 'and', 'output', 'is', 'dmodel', '=', '512', ',', 'and', 'the', 'inner-layer', 'has', 'dimensionality', 'dff=', '2048', '.', '3.4', 'Embeddings', 'and', 'Softmax', 'Similarly', 'to', 'other', 'sequence', 'transduction', 'models', ',', 'we', 'use', 'learned', 'embeddings', 'to', 'convert', 'the', 'input', 'tokens', 'and', 'output', 'tokens', 'to', 'vectors', 'of', 'dimension', 'dmodel', '.', 'We', 'also', 'use', 'the', 'usual', 'learned', 'linear', 'transfor-', 'mation', 'and', 'softmax', 'function', 'to', 'convert', 'the', 'decoder', 'output', 'to', 'predicted', 'next-token', 'probabilities', '.', 'In', 'our', 'model', ',', 'we', 'share', 'the', 'same', 'weight', 'matrix', 'between', 'the', 'two', 'embedding', 'layers', 'and', 'the', 'pre-softmax', 'linear', 'transformation', ',', 'similar', 'to', '[', '30', ']', '.', 'In', 'the', 'embedding', 'layers', ',', 'we', 'multiply', 'those', 'weights', 'byâˆšdmodel', '.', '5', '==============================', 'End', 'of', 'Page', '5', '==============================', 'Table', '1', ':', 'Maximum', 'path', 'lengths', ',', 'per-layer', 'complexity', 'and', 'minimum', 'number', 'of', 'sequential', 'operations', 'for', 'different', 'layer', 'types', '.', 'nis', 'the', 'sequence', 'length', ',', 'dis', 'the', 'representation', 'dimension', ',', 'kis', 'the', 'kernel', 'size', 'of', 'convolutions', 'and', 'rthe', 'size', 'of', 'the', 'neighborhood', 'in', 'restricted', 'self-attention', '.', 'Layer', 'Type', 'Complexity', 'per', 'Layer', 'Sequential', 'Maximum', 'Path', 'Length', 'Operations', 'Self-Attention', 'O', '(', 'n2Â·d', ')', 'O', '(', '1', ')', 'O', '(', '1', ')', 'Recurrent', 'O', '(', 'nÂ·d2', ')', 'O', '(', 'n', ')', 'O', '(', 'n', ')', 'Convolutional', 'O', '(', 'kÂ·nÂ·d2', ')', 'O', '(', '1', ')', 'O', '(', 'logk', '(', 'n', ')', ')', 'Self-Attention', '(', 'restricted', ')', 'O', '(', 'rÂ·nÂ·d', ')', 'O', '(', '1', ')', 'O', '(', 'n/r', ')', '3.5', 'Positional', 'Encoding', 'Since', 'our', 'model', 'contains', 'no', 'recurrence', 'and', 'no', 'convolution', ',', 'in', 'order', 'for', 'the', 'model', 'to', 'make', 'use', 'of', 'the', 'order', 'of', 'the', 'sequence', ',', 'we', 'must', 'inject', 'some', 'information', 'about', 'the', 'relative', 'or', 'absolute', 'position', 'of', 'the', 'tokens', 'in', 'the', 'sequence', '.', 'To', 'this', 'end', ',', 'we', 'add', '``', 'positional', 'encodings', \"''\", 'to', 'the', 'input', 'embeddings', 'at', 'the', 'bottoms', 'of', 'the', 'encoder', 'and', 'decoder', 'stacks', '.', 'The', 'positional', 'encodings', 'have', 'the', 'same', 'dimension', 'dmodel', 'as', 'the', 'embeddings', ',', 'so', 'that', 'the', 'two', 'can', 'be', 'summed', '.', 'There', 'are', 'many', 'choices', 'of', 'positional', 'encodings', ',', 'learned', 'and', 'fixed', '[', '9', ']', '.', 'In', 'this', 'work', ',', 'we', 'use', 'sine', 'and', 'cosine', 'functions', 'of', 'different', 'frequencies', ':', 'PE', '(', 'pos,2i', ')', '=sin', '(', 'pos/100002i/d', 'model', ')', 'PE', '(', 'pos,2i+1', ')', '=cos', '(', 'pos/100002i/d', 'model', ')', 'where', 'posis', 'the', 'position', 'and', 'iis', 'the', 'dimension', '.', 'That', 'is', ',', 'each', 'dimension', 'of', 'the', 'positional', 'encoding', 'corresponds', 'to', 'a', 'sinusoid', '.', 'The', 'wavelengths', 'form', 'a', 'geometric', 'progression', 'from', '2Ï€to10000', 'Â·2Ï€', '.', 'We', 'chose', 'this', 'function', 'because', 'we', 'hypothesized', 'it', 'would', 'allow', 'the', 'model', 'to', 'easily', 'learn', 'to', 'attend', 'by', 'relative', 'positions', ',', 'since', 'for', 'any', 'fixed', 'offset', 'k', ',', 'PEpos+kcan', 'be', 'represented', 'as', 'a', 'linear', 'function', 'of', 'PEpos', '.', 'We', 'also', 'experimented', 'with', 'using', 'learned', 'positional', 'embeddings', '[', '9', ']', 'instead', ',', 'and', 'found', 'that', 'the', 'two', 'versions', 'produced', 'nearly', 'identical', 'results', '(', 'see', 'Table', '3', 'row', '(', 'E', ')', ')', '.', 'We', 'chose', 'the', 'sinusoidal', 'version', 'because', 'it', 'may', 'allow', 'the', 'model', 'to', 'extrapolate', 'to', 'sequence', 'lengths', 'longer', 'than', 'the', 'ones', 'encountered', 'during', 'training', '.', '4', 'Why', 'Self-Attention', 'In', 'this', 'section', 'we', 'compare', 'various', 'aspects', 'of', 'self-attention', 'layers', 'to', 'the', 'recurrent', 'and', 'convolu-', 'tional', 'layers', 'commonly', 'used', 'for', 'mapping', 'one', 'variable-length', 'sequence', 'of', 'symbol', 'representations', '(', 'x1', ',', '...', ',', 'x', 'n', ')', 'to', 'another', 'sequence', 'of', 'equal', 'length', '(', 'z1', ',', '...', ',', 'z', 'n', ')', ',', 'with', 'xi', ',', 'ziâˆˆRd', ',', 'such', 'as', 'a', 'hidden', 'layer', 'in', 'a', 'typical', 'sequence', 'transduction', 'encoder', 'or', 'decoder', '.', 'Motivating', 'our', 'use', 'of', 'self-attention', 'we', 'consider', 'three', 'desiderata', '.', 'One', 'is', 'the', 'total', 'computational', 'complexity', 'per', 'layer', '.', 'Another', 'is', 'the', 'amount', 'of', 'computation', 'that', 'can', 'be', 'parallelized', ',', 'as', 'measured', 'by', 'the', 'minimum', 'number', 'of', 'sequential', 'operations', 'required', '.', 'The', 'third', 'is', 'the', 'path', 'length', 'between', 'long-range', 'dependencies', 'in', 'the', 'network', '.', 'Learning', 'long-range', 'dependencies', 'is', 'a', 'key', 'challenge', 'in', 'many', 'sequence', 'transduction', 'tasks', '.', 'One', 'key', 'factor', 'affecting', 'the', 'ability', 'to', 'learn', 'such', 'dependencies', 'is', 'the', 'length', 'of', 'the', 'paths', 'forward', 'and', 'backward', 'signals', 'have', 'to', 'traverse', 'in', 'the', 'network', '.', 'The', 'shorter', 'these', 'paths', 'between', 'any', 'combination', 'of', 'positions', 'in', 'the', 'input', 'and', 'output', 'sequences', ',', 'the', 'easier', 'it', 'is', 'to', 'learn', 'long-range', 'dependencies', '[', '12', ']', '.', 'Hence', 'we', 'also', 'compare', 'the', 'maximum', 'path', 'length', 'between', 'any', 'two', 'input', 'and', 'output', 'positions', 'in', 'networks', 'composed', 'of', 'the', 'different', 'layer', 'types', '.', 'As', 'noted', 'in', 'Table', '1', ',', 'a', 'self-attention', 'layer', 'connects', 'all', 'positions', 'with', 'a', 'constant', 'number', 'of', 'sequentially', 'executed', 'operations', ',', 'whereas', 'a', 'recurrent', 'layer', 'requires', 'O', '(', 'n', ')', 'sequential', 'operations', '.', 'In', 'terms', 'of', 'computational', 'complexity', ',', 'self-attention', 'layers', 'are', 'faster', 'than', 'recurrent', 'layers', 'when', 'the', 'sequence', '6', '==============================', 'End', 'of', 'Page', '6', '==============================', 'length', 'nis', 'smaller', 'than', 'the', 'representation', 'dimensionality', 'd', ',', 'which', 'is', 'most', 'often', 'the', 'case', 'with', 'sentence', 'representations', 'used', 'by', 'state-of-the-art', 'models', 'in', 'machine', 'translations', ',', 'such', 'as', 'word-piece', '[', '38', ']', 'and', 'byte-pair', '[', '31', ']', 'representations', '.', 'To', 'improve', 'computational', 'performance', 'for', 'tasks', 'involving', 'very', 'long', 'sequences', ',', 'self-attention', 'could', 'be', 'restricted', 'to', 'considering', 'only', 'a', 'neighborhood', 'of', 'size', 'rin', 'the', 'input', 'sequence', 'centered', 'around', 'the', 'respective', 'output', 'position', '.', 'This', 'would', 'increase', 'the', 'maximum', 'path', 'length', 'to', 'O', '(', 'n/r', ')', '.', 'We', 'plan', 'to', 'investigate', 'this', 'approach', 'further', 'in', 'future', 'work', '.', 'A', 'single', 'convolutional', 'layer', 'with', 'kernel', 'width', 'k', '<', 'n', 'does', 'not', 'connect', 'all', 'pairs', 'of', 'input', 'and', 'output', 'positions', '.', 'Doing', 'so', 'requires', 'a', 'stack', 'of', 'O', '(', 'n/k', ')', 'convolutional', 'layers', 'in', 'the', 'case', 'of', 'contiguous', 'kernels', ',', 'orO', '(', 'logk', '(', 'n', ')', ')', 'in', 'the', 'case', 'of', 'dilated', 'convolutions', '[', '18', ']', ',', 'increasing', 'the', 'length', 'of', 'the', 'longest', 'paths', 'between', 'any', 'two', 'positions', 'in', 'the', 'network', '.', 'Convolutional', 'layers', 'are', 'generally', 'more', 'expensive', 'than', 'recurrent', 'layers', ',', 'by', 'a', 'factor', 'of', 'k.', 'Separable', 'convolutions', '[', '6', ']', ',', 'however', ',', 'decrease', 'the', 'complexity', 'considerably', ',', 'to', 'O', '(', 'kÂ·nÂ·d+nÂ·d2', ')', '.', 'Even', 'with', 'k=n', ',', 'however', ',', 'the', 'complexity', 'of', 'a', 'separable', 'convolution', 'is', 'equal', 'to', 'the', 'combination', 'of', 'a', 'self-attention', 'layer', 'and', 'a', 'point-wise', 'feed-forward', 'layer', ',', 'the', 'approach', 'we', 'take', 'in', 'our', 'model', '.', 'As', 'side', 'benefit', ',', 'self-attention', 'could', 'yield', 'more', 'interpretable', 'models', '.', 'We', 'inspect', 'attention', 'distributions', 'from', 'our', 'models', 'and', 'present', 'and', 'discuss', 'examples', 'in', 'the', 'appendix', '.', 'Not', 'only', 'do', 'individual', 'attention', 'heads', 'clearly', 'learn', 'to', 'perform', 'different', 'tasks', ',', 'many', 'appear', 'to', 'exhibit', 'behavior', 'related', 'to', 'the', 'syntactic', 'and', 'semantic', 'structure', 'of', 'the', 'sentences', '.', '5', 'Training', 'This', 'section', 'describes', 'the', 'training', 'regime', 'for', 'our', 'models', '.', '5.1', 'Training', 'Data', 'and', 'Batching', 'We', 'trained', 'on', 'the', 'standard', 'WMT', '2014', 'English-German', 'dataset', 'consisting', 'of', 'about', '4.5', 'million', 'sentence', 'pairs', '.', 'Sentences', 'were', 'encoded', 'using', 'byte-pair', 'encoding', '[', '3', ']', ',', 'which', 'has', 'a', 'shared', 'source-', 'target', 'vocabulary', 'of', 'about', '37000', 'tokens', '.', 'For', 'English-French', ',', 'we', 'used', 'the', 'significantly', 'larger', 'WMT', '2014', 'English-French', 'dataset', 'consisting', 'of', '36M', 'sentences', 'and', 'split', 'tokens', 'into', 'a', '32000', 'word-piece', 'vocabulary', '[', '38', ']', '.', 'Sentence', 'pairs', 'were', 'batched', 'together', 'by', 'approximate', 'sequence', 'length', '.', 'Each', 'training', 'batch', 'contained', 'a', 'set', 'of', 'sentence', 'pairs', 'containing', 'approximately', '25000', 'source', 'tokens', 'and', '25000', 'target', 'tokens', '.', '5.2', 'Hardware', 'and', 'Schedule', 'We', 'trained', 'our', 'models', 'on', 'one', 'machine', 'with', '8', 'NVIDIA', 'P100', 'GPUs', '.', 'For', 'our', 'base', 'models', 'using', 'the', 'hyperparameters', 'described', 'throughout', 'the', 'paper', ',', 'each', 'training', 'step', 'took', 'about', '0.4', 'seconds', '.', 'We', 'trained', 'the', 'base', 'models', 'for', 'a', 'total', 'of', '100,000', 'steps', 'or', '12', 'hours', '.', 'For', 'our', 'big', 'models', ',', '(', 'described', 'on', 'the', 'bottom', 'line', 'of', 'table', '3', ')', ',', 'step', 'time', 'was', '1.0', 'seconds', '.', 'The', 'big', 'models', 'were', 'trained', 'for', '300,000', 'steps', '(', '3.5', 'days', ')', '.', '5.3', 'Optimizer', 'We', 'used', 'the', 'Adam', 'optimizer', '[', '20', ']', 'with', 'Î²1=', '0.9', ',', 'Î²2=', '0.98andÏµ=', '10âˆ’9', '.', 'We', 'varied', 'the', 'learning', 'rate', 'over', 'the', 'course', 'of', 'training', ',', 'according', 'to', 'the', 'formula', ':', 'lrate', '=dâˆ’0.5', 'modelÂ·min', '(', 'step_numâˆ’0.5', ',', 'step', '_numÂ·warmup', '_stepsâˆ’1.5', ')', '(', '3', ')', 'This', 'corresponds', 'to', 'increasing', 'the', 'learning', 'rate', 'linearly', 'for', 'the', 'first', 'warmup', '_steps', 'training', 'steps', ',', 'and', 'decreasing', 'it', 'thereafter', 'proportionally', 'to', 'the', 'inverse', 'square', 'root', 'of', 'the', 'step', 'number', '.', 'We', 'used', 'warmup', '_steps', '=', '4000', '.', '5.4', 'Regularization', 'We', 'employ', 'three', 'types', 'of', 'regularization', 'during', 'training', ':', '7', '==============================', 'End', 'of', 'Page', '7', '==============================', 'Table', '2', ':', 'The', 'Transformer', 'achieves', 'better', 'BLEU', 'scores', 'than', 'previous', 'state-of-the-art', 'models', 'on', 'the', 'English-to-German', 'and', 'English-to-French', 'newstest2014', 'tests', 'at', 'a', 'fraction', 'of', 'the', 'training', 'cost', '.', 'ModelBLEU', 'Training', 'Cost', '(', 'FLOPs', ')', 'EN-DE', 'EN-FR', 'EN-DE', 'EN-FR', 'ByteNet', '[', '18', ']', '23.75', 'Deep-Att', '+', 'PosUnk', '[', '39', ']', '39.2', '1.0Â·1020', 'GNMT', '+', 'RL', '[', '38', ']', '24.6', '39.92', '2.3Â·10191.4Â·1020', 'ConvS2S', '[', '9', ']', '25.16', '40.46', '9.6Â·10181.5Â·1020', 'MoE', '[', '32', ']', '26.03', '40.56', '2.0Â·10191.2Â·1020', 'Deep-Att', '+', 'PosUnk', 'Ensemble', '[', '39', ']', '40.4', '8.0Â·1020', 'GNMT', '+', 'RL', 'Ensemble', '[', '38', ']', '26.30', '41.16', '1.8Â·10201.1Â·1021', 'ConvS2S', 'Ensemble', '[', '9', ']', '26.36', '41.29', '7.7Â·10191.2Â·1021', 'Transformer', '(', 'base', 'model', ')', '27.3', '38.1', '3.3Â·1018', 'Transformer', '(', 'big', ')', '28.4', '41.8', '2.3Â·1019', 'Residual', 'Dropout', 'We', 'apply', 'dropout', '[', '33', ']', 'to', 'the', 'output', 'of', 'each', 'sub-layer', ',', 'before', 'it', 'is', 'added', 'to', 'the', 'sub-layer', 'input', 'and', 'normalized', '.', 'In', 'addition', ',', 'we', 'apply', 'dropout', 'to', 'the', 'sums', 'of', 'the', 'embeddings', 'and', 'the', 'positional', 'encodings', 'in', 'both', 'the', 'encoder', 'and', 'decoder', 'stacks', '.', 'For', 'the', 'base', 'model', ',', 'we', 'use', 'a', 'rate', 'of', 'Pdrop=', '0.1', '.', 'Label', 'Smoothing', 'During', 'training', ',', 'we', 'employed', 'label', 'smoothing', 'of', 'value', 'Ïµls=', '0.1', '[', '36', ']', '.', 'This', 'hurts', 'perplexity', ',', 'as', 'the', 'model', 'learns', 'to', 'be', 'more', 'unsure', ',', 'but', 'improves', 'accuracy', 'and', 'BLEU', 'score', '.', '6', 'Results', '6.1', 'Machine', 'Translation', 'On', 'the', 'WMT', '2014', 'English-to-German', 'translation', 'task', ',', 'the', 'big', 'transformer', 'model', '(', 'Transformer', '(', 'big', ')', 'in', 'Table', '2', ')', 'outperforms', 'the', 'best', 'previously', 'reported', 'models', '(', 'including', 'ensembles', ')', 'by', 'more', 'than', '2.0', 'BLEU', ',', 'establishing', 'a', 'new', 'state-of-the-art', 'BLEU', 'score', 'of', '28.4', '.', 'The', 'configuration', 'of', 'this', 'model', 'is', 'listed', 'in', 'the', 'bottom', 'line', 'of', 'Table', '3', '.', 'Training', 'took', '3.5days', 'on', '8P100', 'GPUs', '.', 'Even', 'our', 'base', 'model', 'surpasses', 'all', 'previously', 'published', 'models', 'and', 'ensembles', ',', 'at', 'a', 'fraction', 'of', 'the', 'training', 'cost', 'of', 'any', 'of', 'the', 'competitive', 'models', '.', 'On', 'the', 'WMT', '2014', 'English-to-French', 'translation', 'task', ',', 'our', 'big', 'model', 'achieves', 'a', 'BLEU', 'score', 'of', '41.0', ',', 'outperforming', 'all', 'of', 'the', 'previously', 'published', 'single', 'models', ',', 'at', 'less', 'than', '1/4the', 'training', 'cost', 'of', 'the', 'previous', 'state-of-the-art', 'model', '.', 'The', 'Transformer', '(', 'big', ')', 'model', 'trained', 'for', 'English-to-French', 'used', 'dropout', 'rate', 'Pdrop=', '0.1', ',', 'instead', 'of', '0.3', '.', 'For', 'the', 'base', 'models', ',', 'we', 'used', 'a', 'single', 'model', 'obtained', 'by', 'averaging', 'the', 'last', '5', 'checkpoints', ',', 'which', 'were', 'written', 'at', '10-minute', 'intervals', '.', 'For', 'the', 'big', 'models', ',', 'we', 'averaged', 'the', 'last', '20', 'checkpoints', '.', 'We', 'used', 'beam', 'search', 'with', 'a', 'beam', 'size', 'of', '4and', 'length', 'penalty', 'Î±=', '0.6', '[', '38', ']', '.', 'These', 'hyperparameters', 'were', 'chosen', 'after', 'experimentation', 'on', 'the', 'development', 'set', '.', 'We', 'set', 'the', 'maximum', 'output', 'length', 'during', 'inference', 'to', 'input', 'length', '+', '50', ',', 'but', 'terminate', 'early', 'when', 'possible', '[', '38', ']', '.', 'Table', '2', 'summarizes', 'our', 'results', 'and', 'compares', 'our', 'translation', 'quality', 'and', 'training', 'costs', 'to', 'other', 'model', 'architectures', 'from', 'the', 'literature', '.', 'We', 'estimate', 'the', 'number', 'of', 'floating', 'point', 'operations', 'used', 'to', 'train', 'a', 'model', 'by', 'multiplying', 'the', 'training', 'time', ',', 'the', 'number', 'of', 'GPUs', 'used', ',', 'and', 'an', 'estimate', 'of', 'the', 'sustained', 'single-precision', 'floating-point', 'capacity', 'of', 'each', 'GPU5', '.', '6.2', 'Model', 'Variations', 'To', 'evaluate', 'the', 'importance', 'of', 'different', 'components', 'of', 'the', 'Transformer', ',', 'we', 'varied', 'our', 'base', 'model', 'in', 'different', 'ways', ',', 'measuring', 'the', 'change', 'in', 'performance', 'on', 'English-to-German', 'translation', 'on', 'the', '5We', 'used', 'values', 'of', '2.8', ',', '3.7', ',', '6.0', 'and', '9.5', 'TFLOPS', 'for', 'K80', ',', 'K40', ',', 'M40', 'and', 'P100', ',', 'respectively', '.', '8', '==============================', 'End', 'of', 'Page', '8', '==============================', 'Table', '3', ':', 'Variations', 'on', 'the', 'Transformer', 'architecture', '.', 'Unlisted', 'values', 'are', 'identical', 'to', 'those', 'of', 'the', 'base', 'model', '.', 'All', 'metrics', 'are', 'on', 'the', 'English-to-German', 'translation', 'development', 'set', ',', 'newstest2013', '.', 'Listed', 'perplexities', 'are', 'per-wordpiece', ',', 'according', 'to', 'our', 'byte-pair', 'encoding', ',', 'and', 'should', 'not', 'be', 'compared', 'to', 'per-word', 'perplexities', '.', 'N', 'd', 'model', 'dff', 'h', 'd', 'k', 'dvPdrop', 'Ïµlstrain', 'PPL', 'BLEU', 'params', 'steps', '(', 'dev', ')', '(', 'dev', ')', 'Ã—106', 'base', '6', '512', '2048', '8', '64', '64', '0.1', '0.1', '100K', '4.92', '25.8', '65', '(', 'A', ')', '1', '512', '512', '5.29', '24.9', '4', '128', '128', '5.00', '25.5', '16', '32', '32', '4.91', '25.8', '32', '16', '16', '5.01', '25.4', '(', 'B', ')', '16', '5.16', '25.1', '58', '32', '5.01', '25.4', '60', '(', 'C', ')', '2', '6.11', '23.7', '36', '4', '5.19', '25.3', '50', '8', '4.88', '25.5', '80', '256', '32', '32', '5.75', '24.5', '28', '1024', '128', '128', '4.66', '26.0', '168', '1024', '5.12', '25.4', '53', '4096', '4.75', '26.2', '90', '(', 'D', ')', '0.0', '5.77', '24.6', '0.2', '4.95', '25.5', '0.0', '4.67', '25.3', '0.2', '5.47', '25.7', '(', 'E', ')', 'positional', 'embedding', 'instead', 'of', 'sinusoids', '4.92', '25.7', 'big', '6', '1024', '4096', '16', '0.3', '300K', '4.33', '26.4', '213', 'development', 'set', ',', 'newstest2013', '.', 'We', 'used', 'beam', 'search', 'as', 'described', 'in', 'the', 'previous', 'section', ',', 'but', 'no', 'checkpoint', 'averaging', '.', 'We', 'present', 'these', 'results', 'in', 'Table', '3', '.', 'In', 'Table', '3', 'rows', '(', 'A', ')', ',', 'we', 'vary', 'the', 'number', 'of', 'attention', 'heads', 'and', 'the', 'attention', 'key', 'and', 'value', 'dimensions', ',', 'keeping', 'the', 'amount', 'of', 'computation', 'constant', ',', 'as', 'described', 'in', 'Section', '3.2.2', '.', 'While', 'single-head', 'attention', 'is', '0.9', 'BLEU', 'worse', 'than', 'the', 'best', 'setting', ',', 'quality', 'also', 'drops', 'off', 'with', 'too', 'many', 'heads', '.', 'In', 'Table', '3', 'rows', '(', 'B', ')', ',', 'we', 'observe', 'that', 'reducing', 'the', 'attention', 'key', 'size', 'dkhurts', 'model', 'quality', '.', 'This', 'suggests', 'that', 'determining', 'compatibility', 'is', 'not', 'easy', 'and', 'that', 'a', 'more', 'sophisticated', 'compatibility', 'function', 'than', 'dot', 'product', 'may', 'be', 'beneficial', '.', 'We', 'further', 'observe', 'in', 'rows', '(', 'C', ')', 'and', '(', 'D', ')', 'that', ',', 'as', 'expected', ',', 'bigger', 'models', 'are', 'better', ',', 'and', 'dropout', 'is', 'very', 'helpful', 'in', 'avoiding', 'over-fitting', '.', 'In', 'row', '(', 'E', ')', 'we', 'replace', 'our', 'sinusoidal', 'positional', 'encoding', 'with', 'learned', 'positional', 'embeddings', '[', '9', ']', ',', 'and', 'observe', 'nearly', 'identical', 'results', 'to', 'the', 'base', 'model', '.', '6.3', 'English', 'Constituency', 'Parsing', 'To', 'evaluate', 'if', 'the', 'Transformer', 'can', 'generalize', 'to', 'other', 'tasks', 'we', 'performed', 'experiments', 'on', 'English', 'constituency', 'parsing', '.', 'This', 'task', 'presents', 'specific', 'challenges', ':', 'the', 'output', 'is', 'subject', 'to', 'strong', 'structural', 'constraints', 'and', 'is', 'significantly', 'longer', 'than', 'the', 'input', '.', 'Furthermore', ',', 'RNN', 'sequence-to-sequence', 'models', 'have', 'not', 'been', 'able', 'to', 'attain', 'state-of-the-art', 'results', 'in', 'small-data', 'regimes', '[', '37', ']', '.', 'We', 'trained', 'a', '4-layer', 'transformer', 'with', 'dmodel', '=', '1024', 'on', 'the', 'Wall', 'Street', 'Journal', '(', 'WSJ', ')', 'portion', 'of', 'the', 'Penn', 'Treebank', '[', '25', ']', ',', 'about', '40K', 'training', 'sentences', '.', 'We', 'also', 'trained', 'it', 'in', 'a', 'semi-supervised', 'setting', ',', 'using', 'the', 'larger', 'high-confidence', 'and', 'BerkleyParser', 'corpora', 'from', 'with', 'approximately', '17M', 'sentences', '[', '37', ']', '.', 'We', 'used', 'a', 'vocabulary', 'of', '16K', 'tokens', 'for', 'the', 'WSJ', 'only', 'setting', 'and', 'a', 'vocabulary', 'of', '32K', 'tokens', 'for', 'the', 'semi-supervised', 'setting', '.', 'We', 'performed', 'only', 'a', 'small', 'number', 'of', 'experiments', 'to', 'select', 'the', 'dropout', ',', 'both', 'attention', 'and', 'residual', '(', 'section', '5.4', ')', ',', 'learning', 'rates', 'and', 'beam', 'size', 'on', 'the', 'Section', '22', 'development', 'set', ',', 'all', 'other', 'parameters', 'remained', 'unchanged', 'from', 'the', 'English-to-German', 'base', 'translation', 'model', '.', 'During', 'inference', ',', 'we', '9', '==============================', 'End', 'of', 'Page', '9', '==============================', 'Table', '4', ':', 'The', 'Transformer', 'generalizes', 'well', 'to', 'English', 'constituency', 'parsing', '(', 'Results', 'are', 'on', 'Section', '23', 'of', 'WSJ', ')', 'Parser', 'Training', 'WSJ', '23', 'F1', 'Vinyals', '&', 'Kaiser', 'el', 'al', '.', '(', '2014', ')', '[', '37', ']', 'WSJ', 'only', ',', 'discriminative', '88.3', 'Petrov', 'et', 'al', '.', '(', '2006', ')', '[', '29', ']', 'WSJ', 'only', ',', 'discriminative', '90.4', 'Zhu', 'et', 'al', '.', '(', '2013', ')', '[', '40', ']', 'WSJ', 'only', ',', 'discriminative', '90.4', 'Dyer', 'et', 'al', '.', '(', '2016', ')', '[', '8', ']', 'WSJ', 'only', ',', 'discriminative', '91.7', 'Transformer', '(', '4', 'layers', ')', 'WSJ', 'only', ',', 'discriminative', '91.3', 'Zhu', 'et', 'al', '.', '(', '2013', ')', '[', '40', ']', 'semi-supervised', '91.3', 'Huang', '&', 'Harper', '(', '2009', ')', '[', '14', ']', 'semi-supervised', '91.3', 'McClosky', 'et', 'al', '.', '(', '2006', ')', '[', '26', ']', 'semi-supervised', '92.1', 'Vinyals', '&', 'Kaiser', 'el', 'al', '.', '(', '2014', ')', '[', '37', ']', 'semi-supervised', '92.1', 'Transformer', '(', '4', 'layers', ')', 'semi-supervised', '92.7', 'Luong', 'et', 'al', '.', '(', '2015', ')', '[', '23', ']', 'multi-task', '93.0', 'Dyer', 'et', 'al', '.', '(', '2016', ')', '[', '8', ']', 'generative', '93.3', 'increased', 'the', 'maximum', 'output', 'length', 'to', 'input', 'length', '+', '300', '.', 'We', 'used', 'a', 'beam', 'size', 'of', '21andÎ±=', '0.3', 'for', 'both', 'WSJ', 'only', 'and', 'the', 'semi-supervised', 'setting', '.', 'Our', 'results', 'in', 'Table', '4', 'show', 'that', 'despite', 'the', 'lack', 'of', 'task-specific', 'tuning', 'our', 'model', 'performs', 'sur-', 'prisingly', 'well', ',', 'yielding', 'better', 'results', 'than', 'all', 'previously', 'reported', 'models', 'with', 'the', 'exception', 'of', 'the', 'Recurrent', 'Neural', 'Network', 'Grammar', '[', '8', ']', '.', 'In', 'contrast', 'to', 'RNN', 'sequence-to-sequence', 'models', '[', '37', ']', ',', 'the', 'Transformer', 'outperforms', 'the', 'Berkeley-', 'Parser', '[', '29', ']', 'even', 'when', 'training', 'only', 'on', 'the', 'WSJ', 'training', 'set', 'of', '40K', 'sentences', '.', '7', 'Conclusion', 'In', 'this', 'work', ',', 'we', 'presented', 'the', 'Transformer', ',', 'the', 'first', 'sequence', 'transduction', 'model', 'based', 'entirely', 'on', 'attention', ',', 'replacing', 'the', 'recurrent', 'layers', 'most', 'commonly', 'used', 'in', 'encoder-decoder', 'architectures', 'with', 'multi-headed', 'self-attention', '.', 'For', 'translation', 'tasks', ',', 'the', 'Transformer', 'can', 'be', 'trained', 'significantly', 'faster', 'than', 'architectures', 'based', 'on', 'recurrent', 'or', 'convolutional', 'layers', '.', 'On', 'both', 'WMT', '2014', 'English-to-German', 'and', 'WMT', '2014', 'English-to-French', 'translation', 'tasks', ',', 'we', 'achieve', 'a', 'new', 'state', 'of', 'the', 'art', '.', 'In', 'the', 'former', 'task', 'our', 'best', 'model', 'outperforms', 'even', 'all', 'previously', 'reported', 'ensembles', '.', 'We', 'are', 'excited', 'about', 'the', 'future', 'of', 'attention-based', 'models', 'and', 'plan', 'to', 'apply', 'them', 'to', 'other', 'tasks', '.', 'We', 'plan', 'to', 'extend', 'the', 'Transformer', 'to', 'problems', 'involving', 'input', 'and', 'output', 'modalities', 'other', 'than', 'text', 'and', 'to', 'investigate', 'local', ',', 'restricted', 'attention', 'mechanisms', 'to', 'efficiently', 'handle', 'large', 'inputs', 'and', 'outputs', 'such', 'as', 'images', ',', 'audio', 'and', 'video', '.', 'Making', 'generation', 'less', 'sequential', 'is', 'another', 'research', 'goals', 'of', 'ours', '.', 'The', 'code', 'we', 'used', 'to', 'train', 'and', 'evaluate', 'our', 'models', 'is', 'available', 'at', 'https', ':', '//github.com/', 'tensorflow/tensor2tensor', '.', 'Acknowledgements', 'We', 'are', 'grateful', 'to', 'Nal', 'Kalchbrenner', 'and', 'Stephan', 'Gouws', 'for', 'their', 'fruitful', 'comments', ',', 'corrections', 'and', 'inspiration', '.', 'References', '[', '1', ']', 'Jimmy', 'Lei', 'Ba', ',', 'Jamie', 'Ryan', 'Kiros', ',', 'and', 'Geoffrey', 'E', 'Hinton', '.', 'Layer', 'normalization', '.', 'arXiv', 'preprint', 'arXiv:1607.06450', ',', '2016', '.', '[', '2', ']', 'Dzmitry', 'Bahdanau', ',', 'Kyunghyun', 'Cho', ',', 'and', 'Yoshua', 'Bengio', '.', 'Neural', 'machine', 'translation', 'by', 'jointly', 'learning', 'to', 'align', 'and', 'translate', '.', 'CoRR', ',', 'abs/1409.0473', ',', '2014', '.', '[', '3', ']', 'Denny', 'Britz', ',', 'Anna', 'Goldie', ',', 'Minh-Thang', 'Luong', ',', 'and', 'Quoc', 'V', '.', 'Le', '.', 'Massive', 'exploration', 'of', 'neural', 'machine', 'translation', 'architectures', '.', 'CoRR', ',', 'abs/1703.03906', ',', '2017', '.', '[', '4', ']', 'Jianpeng', 'Cheng', ',', 'Li', 'Dong', ',', 'and', 'Mirella', 'Lapata', '.', 'Long', 'short-term', 'memory-networks', 'for', 'machine', 'reading', '.', 'arXiv', 'preprint', 'arXiv:1601.06733', ',', '2016', '.', '10', '==============================', 'End', 'of', 'Page', '10', '==============================', '[', '5', ']', 'Kyunghyun', 'Cho', ',', 'Bart', 'van', 'Merrienboer', ',', 'Caglar', 'Gulcehre', ',', 'Fethi', 'Bougares', ',', 'Holger', 'Schwenk', ',', 'and', 'Yoshua', 'Bengio', '.', 'Learning', 'phrase', 'representations', 'using', 'rnn', 'encoder-decoder', 'for', 'statistical', 'machine', 'translation', '.', 'CoRR', ',', 'abs/1406.1078', ',', '2014', '.', '[', '6', ']', 'Francois', 'Chollet', '.', 'Xception', ':', 'Deep', 'learning', 'with', 'depthwise', 'separable', 'convolutions', '.', 'arXiv', 'preprint', 'arXiv:1610.02357', ',', '2016', '.', '[', '7', ']', 'Junyoung', 'Chung', ',', 'Ã‡aglar', 'GÃ¼lÃ§ehre', ',', 'Kyunghyun', 'Cho', ',', 'and', 'Yoshua', 'Bengio', '.', 'Empirical', 'evaluation', 'of', 'gated', 'recurrent', 'neural', 'networks', 'on', 'sequence', 'modeling', '.', 'CoRR', ',', 'abs/1412.3555', ',', '2014', '.', '[', '8', ']', 'Chris', 'Dyer', ',', 'Adhiguna', 'Kuncoro', ',', 'Miguel', 'Ballesteros', ',', 'and', 'Noah', 'A.', 'Smith', '.', 'Recurrent', 'neural', 'network', 'grammars', '.', 'In', 'Proc', '.', 'of', 'NAACL', ',', '2016', '.', '[', '9', ']', 'Jonas', 'Gehring', ',', 'Michael', 'Auli', ',', 'David', 'Grangier', ',', 'Denis', 'Yarats', ',', 'and', 'Yann', 'N.', 'Dauphin', '.', 'Convolu-', 'tional', 'sequence', 'to', 'sequence', 'learning', '.', 'arXiv', 'preprint', 'arXiv:1705.03122v2', ',', '2017', '.', '[', '10', ']', 'Alex', 'Graves', '.', 'Generating', 'sequences', 'with', 'recurrent', 'neural', 'networks', '.', 'arXiv', 'preprint', 'arXiv:1308.0850', ',', '2013', '.', '[', '11', ']', 'Kaiming', 'He', ',', 'Xiangyu', 'Zhang', ',', 'Shaoqing', 'Ren', ',', 'and', 'Jian', 'Sun', '.', 'Deep', 'residual', 'learning', 'for', 'im-', 'age', 'recognition', '.', 'In', 'Proceedings', 'of', 'the', 'IEEE', 'Conference', 'on', 'Computer', 'Vision', 'and', 'Pattern', 'Recognition', ',', 'pages', '770â€“778', ',', '2016', '.', '[', '12', ']', 'Sepp', 'Hochreiter', ',', 'Yoshua', 'Bengio', ',', 'Paolo', 'Frasconi', ',', 'and', 'JÃ¼rgen', 'Schmidhuber', '.', 'Gradient', 'flow', 'in', 'recurrent', 'nets', ':', 'the', 'difficulty', 'of', 'learning', 'long-term', 'dependencies', ',', '2001', '.', '[', '13', ']', 'Sepp', 'Hochreiter', 'and', 'JÃ¼rgen', 'Schmidhuber', '.', 'Long', 'short-term', 'memory', '.', 'Neural', 'computation', ',', '9', '(', '8', ')', ':1735â€“1780', ',', '1997', '.', '[', '14', ']', 'Zhongqiang', 'Huang', 'and', 'Mary', 'Harper', '.', 'Self-training', 'PCFG', 'grammars', 'with', 'latent', 'annotations', 'across', 'languages', '.', 'In', 'Proceedings', 'of', 'the', '2009', 'Conference', 'on', 'Empirical', 'Methods', 'in', 'Natural', 'Language', 'Processing', ',', 'pages', '832â€“841', '.', 'ACL', ',', 'August', '2009', '.', '[', '15', ']', 'Rafal', 'Jozefowicz', ',', 'Oriol', 'Vinyals', ',', 'Mike', 'Schuster', ',', 'Noam', 'Shazeer', ',', 'and', 'Yonghui', 'Wu', '.', 'Exploring', 'the', 'limits', 'of', 'language', 'modeling', '.', 'arXiv', 'preprint', 'arXiv:1602.02410', ',', '2016', '.', '[', '16', ']', 'Åukasz', 'Kaiser', 'and', 'Samy', 'Bengio', '.', 'Can', 'active', 'memory', 'replace', 'attention', '?', 'In', 'Advances', 'in', 'Neural', 'Information', 'Processing', 'Systems', ',', '(', 'NIPS', ')', ',', '2016', '.', '[', '17', ']', 'Åukasz', 'Kaiser', 'and', 'Ilya', 'Sutskever', '.', 'Neural', 'GPUs', 'learn', 'algorithms', '.', 'In', 'International', 'Conference', 'on', 'Learning', 'Representations', '(', 'ICLR', ')', ',', '2016', '.', '[', '18', ']', 'Nal', 'Kalchbrenner', ',', 'Lasse', 'Espeholt', ',', 'Karen', 'Simonyan', ',', 'Aaron', 'van', 'den', 'Oord', ',', 'Alex', 'Graves', ',', 'and', 'Ko-', 'ray', 'Kavukcuoglu', '.', 'Neural', 'machine', 'translation', 'in', 'linear', 'time', '.', 'arXiv', 'preprint', 'arXiv:1610.10099v2', ',', '2017', '.', '[', '19', ']', 'Yoon', 'Kim', ',', 'Carl', 'Denton', ',', 'Luong', 'Hoang', ',', 'and', 'Alexander', 'M.', 'Rush', '.', 'Structured', 'attention', 'networks', '.', 'InInternational', 'Conference', 'on', 'Learning', 'Representations', ',', '2017', '.', '[', '20', ']', 'Diederik', 'Kingma', 'and', 'Jimmy', 'Ba', '.', 'Adam', ':', 'A', 'method', 'for', 'stochastic', 'optimization', '.', 'In', 'ICLR', ',', '2015', '.', '[', '21', ']', 'Oleksii', 'Kuchaiev', 'and', 'Boris', 'Ginsburg', '.', 'Factorization', 'tricks', 'for', 'LSTM', 'networks', '.', 'arXiv', 'preprint', 'arXiv:1703.10722', ',', '2017', '.', '[', '22', ']', 'Zhouhan', 'Lin', ',', 'Minwei', 'Feng', ',', 'Cicero', 'Nogueira', 'dos', 'Santos', ',', 'Mo', 'Yu', ',', 'Bing', 'Xiang', ',', 'Bowen', 'Zhou', ',', 'and', 'Yoshua', 'Bengio', '.', 'A', 'structured', 'self-attentive', 'sentence', 'embedding', '.', 'arXiv', 'preprint', 'arXiv:1703.03130', ',', '2017', '.', '[', '23', ']', 'Minh-Thang', 'Luong', ',', 'Quoc', 'V', '.', 'Le', ',', 'Ilya', 'Sutskever', ',', 'Oriol', 'Vinyals', ',', 'and', 'Lukasz', 'Kaiser', '.', 'Multi-task', 'sequence', 'to', 'sequence', 'learning', '.', 'arXiv', 'preprint', 'arXiv:1511.06114', ',', '2015', '.', '[', '24', ']', 'Minh-Thang', 'Luong', ',', 'Hieu', 'Pham', ',', 'and', 'Christopher', 'D', 'Manning', '.', 'Effective', 'approaches', 'to', 'attention-', 'based', 'neural', 'machine', 'translation', '.', 'arXiv', 'preprint', 'arXiv:1508.04025', ',', '2015', '.', '11', '==============================', 'End', 'of', 'Page', '11', '==============================', '[', '25', ']', 'Mitchell', 'P', 'Marcus', ',', 'Mary', 'Ann', 'Marcinkiewicz', ',', 'and', 'Beatrice', 'Santorini', '.', 'Building', 'a', 'large', 'annotated', 'corpus', 'of', 'english', ':', 'The', 'penn', 'treebank', '.', 'Computational', 'linguistics', ',', '19', '(', '2', ')', ':313â€“330', ',', '1993', '.', '[', '26', ']', 'David', 'McClosky', ',', 'Eugene', 'Charniak', ',', 'and', 'Mark', 'Johnson', '.', 'Effective', 'self-training', 'for', 'parsing', '.', 'In', 'Proceedings', 'of', 'the', 'Human', 'Language', 'Technology', 'Conference', 'of', 'the', 'NAACL', ',', 'Main', 'Conference', ',', 'pages', '152â€“159', '.', 'ACL', ',', 'June', '2006', '.', '[', '27', ']', 'Ankur', 'Parikh', ',', 'Oscar', 'TÃ¤ckstrÃ¶m', ',', 'Dipanjan', 'Das', ',', 'and', 'Jakob', 'Uszkoreit', '.', 'A', 'decomposable', 'attention', 'model', '.', 'In', 'Empirical', 'Methods', 'in', 'Natural', 'Language', 'Processing', ',', '2016', '.', '[', '28', ']', 'Romain', 'Paulus', ',', 'Caiming', 'Xiong', ',', 'and', 'Richard', 'Socher', '.', 'A', 'deep', 'reinforced', 'model', 'for', 'abstractive', 'summarization', '.', 'arXiv', 'preprint', 'arXiv:1705.04304', ',', '2017', '.', '[', '29', ']', 'Slav', 'Petrov', ',', 'Leon', 'Barrett', ',', 'Romain', 'Thibaux', ',', 'and', 'Dan', 'Klein', '.', 'Learning', 'accurate', ',', 'compact', ',', 'and', 'interpretable', 'tree', 'annotation', '.', 'In', 'Proceedings', 'of', 'the', '21st', 'International', 'Conference', 'on', 'Computational', 'Linguistics', 'and', '44th', 'Annual', 'Meeting', 'of', 'the', 'ACL', ',', 'pages', '433â€“440', '.', 'ACL', ',', 'July', '2006', '.', '[', '30', ']', 'Ofir', 'Press', 'and', 'Lior', 'Wolf', '.', 'Using', 'the', 'output', 'embedding', 'to', 'improve', 'language', 'models', '.', 'arXiv', 'preprint', 'arXiv:1608.05859', ',', '2016', '.', '[', '31', ']', 'Rico', 'Sennrich', ',', 'Barry', 'Haddow', ',', 'and', 'Alexandra', 'Birch', '.', 'Neural', 'machine', 'translation', 'of', 'rare', 'words', 'with', 'subword', 'units', '.', 'arXiv', 'preprint', 'arXiv:1508.07909', ',', '2015', '.', '[', '32', ']', 'Noam', 'Shazeer', ',', 'Azalia', 'Mirhoseini', ',', 'Krzysztof', 'Maziarz', ',', 'Andy', 'Davis', ',', 'Quoc', 'Le', ',', 'Geoffrey', 'Hinton', ',', 'and', 'Jeff', 'Dean', '.', 'Outrageously', 'large', 'neural', 'networks', ':', 'The', 'sparsely-gated', 'mixture-of-experts', 'layer', '.', 'arXiv', 'preprint', 'arXiv:1701.06538', ',', '2017', '.', '[', '33', ']', 'Nitish', 'Srivastava', ',', 'Geoffrey', 'E', 'Hinton', ',', 'Alex', 'Krizhevsky', ',', 'Ilya', 'Sutskever', ',', 'and', 'Ruslan', 'Salakhutdi-', 'nov', '.', 'Dropout', ':', 'a', 'simple', 'way', 'to', 'prevent', 'neural', 'networks', 'from', 'overfitting', '.', 'Journal', 'of', 'Machine', 'Learning', 'Research', ',', '15', '(', '1', ')', ':1929â€“1958', ',', '2014', '.', '[', '34', ']', 'Sainbayar', 'Sukhbaatar', ',', 'Arthur', 'Szlam', ',', 'Jason', 'Weston', ',', 'and', 'Rob', 'Fergus', '.', 'End-to-end', 'memory', 'networks', '.', 'In', 'C.', 'Cortes', ',', 'N.', 'D.', 'Lawrence', ',', 'D.', 'D.', 'Lee', ',', 'M.', 'Sugiyama', ',', 'and', 'R.', 'Garnett', ',', 'editors', ',', 'Advances', 'in', 'Neural', 'Information', 'Processing', 'Systems', '28', ',', 'pages', '2440â€“2448', '.', 'Curran', 'Associates', ',', 'Inc.', ',', '2015', '.', '[', '35', ']', 'Ilya', 'Sutskever', ',', 'Oriol', 'Vinyals', ',', 'and', 'Quoc', 'VV', 'Le', '.', 'Sequence', 'to', 'sequence', 'learning', 'with', 'neural', 'networks', '.', 'In', 'Advances', 'in', 'Neural', 'Information', 'Processing', 'Systems', ',', 'pages', '3104â€“3112', ',', '2014', '.', '[', '36', ']', 'Christian', 'Szegedy', ',', 'Vincent', 'Vanhoucke', ',', 'Sergey', 'Ioffe', ',', 'Jonathon', 'Shlens', ',', 'and', 'Zbigniew', 'Wojna', '.', 'Rethinking', 'the', 'inception', 'architecture', 'for', 'computer', 'vision', '.', 'CoRR', ',', 'abs/1512.00567', ',', '2015', '.', '[', '37', ']', 'Vinyals', '&', 'Kaiser', ',', 'Koo', ',', 'Petrov', ',', 'Sutskever', ',', 'and', 'Hinton', '.', 'Grammar', 'as', 'a', 'foreign', 'language', '.', 'In', 'Advances', 'in', 'Neural', 'Information', 'Processing', 'Systems', ',', '2015', '.', '[', '38', ']', 'Yonghui', 'Wu', ',', 'Mike', 'Schuster', ',', 'Zhifeng', 'Chen', ',', 'Quoc', 'V', 'Le', ',', 'Mohammad', 'Norouzi', ',', 'Wolfgang', 'Macherey', ',', 'Maxim', 'Krikun', ',', 'Yuan', 'Cao', ',', 'Qin', 'Gao', ',', 'Klaus', 'Macherey', ',', 'et', 'al', '.', 'Google', 'â€™', 's', 'neural', 'machine', 'translation', 'system', ':', 'Bridging', 'the', 'gap', 'between', 'human', 'and', 'machine', 'translation', '.', 'arXiv', 'preprint', 'arXiv:1609.08144', ',', '2016', '.', '[', '39', ']', 'Jie', 'Zhou', ',', 'Ying', 'Cao', ',', 'Xuguang', 'Wang', ',', 'Peng', 'Li', ',', 'and', 'Wei', 'Xu', '.', 'Deep', 'recurrent', 'models', 'with', 'fast-forward', 'connections', 'for', 'neural', 'machine', 'translation', '.', 'CoRR', ',', 'abs/1606.04199', ',', '2016', '.', '[', '40', ']', 'Muhua', 'Zhu', ',', 'Yue', 'Zhang', ',', 'Wenliang', 'Chen', ',', 'Min', 'Zhang', ',', 'and', 'Jingbo', 'Zhu', '.', 'Fast', 'and', 'accurate', 'shift-reduce', 'constituent', 'parsing', '.', 'In', 'Proceedings', 'of', 'the', '51st', 'Annual', 'Meeting', 'of', 'the', 'ACL', '(', 'Volume', '1', ':', 'Long', 'Papers', ')', ',', 'pages', '434â€“443', '.', 'ACL', ',', 'August', '2013', '.', '12', '==============================', 'End', 'of', 'Page', '12', '==============================', 'Attention', 'Visualizations', 'Input-Input', 'Layer5', 'It', 'is', 'in', 'this', 'spirit', 'that', 'a', 'majority', 'of', 'American', 'governments', 'have', 'passed', 'new', 'laws', 'since', '2009', 'making', 'the', 'registration', 'or', 'voting', 'process', 'more', 'difficult', '.', '<', 'EOS', '>', '<', 'pad', '>', '<', 'pad', '>', '<', 'pad', '>', '<', 'pad', '>', '<', 'pad', '>', '<', 'pad', '>', 'It', 'is', 'in', 'this', 'spirit', 'that', 'a', 'majority', 'of', 'American', 'governments', 'have', 'passed', 'new', 'laws', 'since', '2009', 'making', 'the', 'registration', 'or', 'voting', 'process', 'more', 'difficult', '.', '<', 'EOS', '>', '<', 'pad', '>', '<', 'pad', '>', '<', 'pad', '>', '<', 'pad', '>', '<', 'pad', '>', '<', 'pad', '>', 'Figure', '3', ':', 'An', 'example', 'of', 'the', 'attention', 'mechanism', 'following', 'long-distance', 'dependencies', 'in', 'the', 'encoder', 'self-attention', 'in', 'layer', '5', 'of', '6', '.', 'Many', 'of', 'the', 'attention', 'heads', 'attend', 'to', 'a', 'distant', 'dependency', 'of', 'the', 'verb', 'â€˜', 'making', 'â€™', ',', 'completing', 'the', 'phrase', 'â€˜', 'making', '...', 'more', 'difficult', 'â€™', '.', 'Attentions', 'here', 'shown', 'only', 'for', 'the', 'word', 'â€˜', 'making', 'â€™', '.', 'Different', 'colors', 'represent', 'different', 'heads', '.', 'Best', 'viewed', 'in', 'color', '.', '13', '==============================', 'End', 'of', 'Page', '13', '==============================', 'Input-Input', 'Layer5', 'The', 'Law', 'will', 'never', 'be', 'perfect', ',', 'but', 'its', 'application', 'should', 'be', 'just', '-', 'this', 'is', 'what', 'we', 'are', 'missing', ',', 'in', 'my', 'opinion', '.', '<', 'EOS', '>', '<', 'pad', '>', 'The', 'Law', 'will', 'never', 'be', 'perfect', ',', 'but', 'its', 'application', 'should', 'be', 'just', '-', 'this', 'is', 'what', 'we', 'are', 'missing', ',', 'in', 'my', 'opinion', '.', '<', 'EOS', '>', '<', 'pad', '>', 'Input-Input', 'Layer5', 'The', 'Law', 'will', 'never', 'be', 'perfect', ',', 'but', 'its', 'application', 'should', 'be', 'just', '-', 'this', 'is', 'what', 'we', 'are', 'missing', ',', 'in', 'my', 'opinion', '.', '<', 'EOS', '>', '<', 'pad', '>', 'The', 'Law', 'will', 'never', 'be', 'perfect', ',', 'but', 'its', 'application', 'should', 'be', 'just', '-', 'this', 'is', 'what', 'we', 'are', 'missing', ',', 'in', 'my', 'opinion', '.', '<', 'EOS', '>', '<', 'pad', '>', 'Figure', '4', ':', 'Two', 'attention', 'heads', ',', 'also', 'in', 'layer', '5', 'of', '6', ',', 'apparently', 'involved', 'in', 'anaphora', 'resolution', '.', 'Top', ':', 'Full', 'attentions', 'for', 'head', '5', '.', 'Bottom', ':', 'Isolated', 'attentions', 'from', 'just', 'the', 'word', 'â€˜', 'its', 'â€™', 'for', 'attention', 'heads', '5', 'and', '6', '.', 'Note', 'that', 'the', 'attentions', 'are', 'very', 'sharp', 'for', 'this', 'word', '.', '14', '==============================', 'End', 'of', 'Page', '14', '==============================', 'Input-Input', 'Layer5', 'The', 'Law', 'will', 'never', 'be', 'perfect', ',', 'but', 'its', 'application', 'should', 'be', 'just', '-', 'this', 'is', 'what', 'we', 'are', 'missing', ',', 'in', 'my', 'opinion', '.', '<', 'EOS', '>', '<', 'pad', '>', 'The', 'Law', 'will', 'never', 'be', 'perfect', ',', 'but', 'its', 'application', 'should', 'be', 'just', '-', 'this', 'is', 'what', 'we', 'are', 'missing', ',', 'in', 'my', 'opinion', '.', '<', 'EOS', '>', '<', 'pad', '>', 'Input-Input', 'Layer5', 'The', 'Law', 'will', 'never', 'be', 'perfect', ',', 'but', 'its', 'application', 'should', 'be', 'just', '-', 'this', 'is', 'what', 'we', 'are', 'missing', ',', 'in', 'my', 'opinion', '.', '<', 'EOS', '>', '<', 'pad', '>', 'The', 'Law', 'will', 'never', 'be', 'perfect', ',', 'but', 'its', 'application', 'should', 'be', 'just', '-', 'this', 'is', 'what', 'we', 'are', 'missing', ',', 'in', 'my', 'opinion', '.', '<', 'EOS', '>', '<', 'pad', '>', 'Figure', '5', ':', 'Many', 'of', 'the', 'attention', 'heads', 'exhibit', 'behaviour', 'that', 'seems', 'related', 'to', 'the', 'structure', 'of', 'the', 'sentence', '.', 'We', 'give', 'two', 'such', 'examples', 'above', ',', 'from', 'two', 'different', 'heads', 'from', 'the', 'encoder', 'self-attention', 'at', 'layer', '5', 'of', '6', '.', 'The', 'heads', 'clearly', 'learned', 'to', 'perform', 'different', 'tasks', '.', '15', '==============================', 'End', 'of', 'Page', '15', '==============================']\n"
     ]
    }
   ],
   "source": [
    "print(df_raw.loc[df_raw.Source == 'PDF'].reset_index(drop = True).token_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Perform stemming and compare the token distributions before and after the stemming (you may choose any stemming algorithm implemented in any toolkit).\n",
    "\n",
    "You may compare:\n",
    "    - The number of distinct tokens\n",
    "    - The length distribution of the token (The length distribution can be compared in a plot: the x-axis is the length of a token in number of characters, and the y-axis is the number of tokens of each length)\n",
    "    \n",
    "Discuss your findings\n",
    "    - Number of distinct tokens significantly reduced from 16,856 to 12,855 (~24% reduction)\n",
    "    - Words between the length of 10 to 15 experience the highest number of frequency reduction (refer to table)\n",
    "        - Most of the more common words lie between this length\n",
    "        - Words > length of 20 tend to be rare words/dirty data which should be removed during data cleaning phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove tokens with only punctuations\n",
    "def no_punc(tokens):\n",
    "    punctuation_set = set(string.punctuation)\n",
    "    filtered_tokens = [token for token in tokens if not all(char in punctuation_set for char in token)]\n",
    "    return filtered_tokens\n",
    "\n",
    "df_raw['no_punc'] = df_raw.token_words.apply(no_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "df_raw['stemmed_text'] = df_raw.no_punc.apply(lambda x: [stemmer.stem(token) for token in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comparing before and after stemming:\n",
    "before_stemming_col = 'no_punc'\n",
    "\n",
    "stem_comparison = pd.DataFrame({'before_stem': [item for sublist in df_raw[before_stemming_col] for item in sublist]})\n",
    "stem_comparison['after_stem'] = stem_comparison.before_stem.apply(stemmer.stem)\n",
    "stem_comparison.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinct tokens before and after stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of distinct token before stemming: 16856\n",
      " Number of distinct token after stemming: 12855\n"
     ]
    }
   ],
   "source": [
    "# Multiple words are stemmed into the same word\n",
    "print(f' Number of distinct token before stemming: {stem_comparison.before_stem.nunique()}')\n",
    "print(f' Number of distinct token after stemming: {stem_comparison.after_stem.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting word distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_comparison['len_before_stem'] = stem_comparison.before_stem.apply(len)\n",
    "stem_comparison['len_after_stem'] = stem_comparison.after_stem.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>before_stem</th>\n",
       "      <th>after_stem</th>\n",
       "      <th>len_before_stem</th>\n",
       "      <th>len_after_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86016</th>\n",
       "      <td>//aclanthology.org/2022.wmt-1.2</td>\n",
       "      <td>//aclanthology.org/2022.wmt-1.2</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104688</th>\n",
       "      <td>training2530354045505560ImageNet</td>\n",
       "      <td>training2530354045505560imagenet</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53540</th>\n",
       "      <td>Re-computationStreamingLLMMemory</td>\n",
       "      <td>re-computationstreamingllmmemori</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53527</th>\n",
       "      <td>040080012001600256512102420484096</td>\n",
       "      <td>040080012001600256512102420484096</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93714</th>\n",
       "      <td>Length50100150200250300Throughput</td>\n",
       "      <td>length50100150200250300throughput</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              before_stem                         after_stem  \\\n",
       "86016     //aclanthology.org/2022.wmt-1.2    //aclanthology.org/2022.wmt-1.2   \n",
       "104688   training2530354045505560ImageNet   training2530354045505560imagenet   \n",
       "53540    Re-computationStreamingLLMMemory   re-computationstreamingllmmemori   \n",
       "53527   040080012001600256512102420484096  040080012001600256512102420484096   \n",
       "93714   Length50100150200250300Throughput  length50100150200250300throughput   \n",
       "\n",
       "        len_before_stem  len_after_stem  \n",
       "86016                31              31  \n",
       "104688               32              32  \n",
       "53540                32              32  \n",
       "53527                33              33  \n",
       "93714                33              33  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_comparison.loc[stem_comparison.len_after_stem>30].sort_values(['len_before_stem']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2888ebc1948>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "before_stemmming = stem_comparison[['before_stem','len_before_stem']].drop_duplicates()\n",
    "after_stemmming = stem_comparison[['after_stem','len_after_stem']].drop_duplicates()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.hist(before_stemmming['len_before_stem']\n",
    "         , bins=range(min(before_stemmming['len_before_stem']), 30, 1)\n",
    "         , edgecolor='black'\n",
    "         , alpha = 0.3\n",
    "         , label = 'Before Stemming')\n",
    "\n",
    "plt.hist(after_stemmming['len_after_stem']\n",
    "         , bins=range(min(after_stemmming['len_after_stem']), 30, 1)\n",
    "         , edgecolor='black'\n",
    "         , alpha = 0.3\n",
    "         , label = 'After Stemming')\n",
    "\n",
    "plt.xlabel('Token Length in number of characters')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Word Lengths')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check % reduction of words by character length\n",
    "\n",
    "len_after_stem_df = pd.DataFrame(after_stemmming.len_after_stem.value_counts())\\\n",
    "    .reset_index().rename(columns={'index': 'Token_len', 'len_after_stem': 'Count_after_stem'})\n",
    "\n",
    "len_before_stem_df = pd.DataFrame(before_stemmming.len_before_stem.value_counts())\\\n",
    "    .reset_index().rename(columns={'index': 'Token_len', 'len_before_stem': 'Count_before_stem'})\n",
    "\n",
    "len_comparison_df = pd.merge(len_before_stem_df, len_after_stem_df, how = 'left', on = 'Token_len')\n",
    "len_comparison_df.sort_values('Token_len', inplace = True)\n",
    "\n",
    "len_comparison_df['change'] = (len_comparison_df.Count_before_stem - len_comparison_df.Count_after_stem)\n",
    "\n",
    "len_comparison_df['per_change'] = (len_comparison_df.Count_before_stem - len_comparison_df.Count_after_stem)\\\n",
    "    /len_comparison_df.Count_before_stem * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token_len</th>\n",
       "      <th>Count_before_stem</th>\n",
       "      <th>Count_after_stem</th>\n",
       "      <th>change</th>\n",
       "      <th>per_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>764</td>\n",
       "      <td>288.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>62.303665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>522</td>\n",
       "      <td>208.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>60.153257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>335</td>\n",
       "      <td>139.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>58.507463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>189</td>\n",
       "      <td>83.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>56.084656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>1038</td>\n",
       "      <td>456.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>56.069364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Token_len  Count_before_stem  Count_after_stem  change  per_change\n",
       "38         52                  3               1.0     2.0   66.666667\n",
       "37         31                  3               1.0     2.0   66.666667\n",
       "36         37                  3               1.0     2.0   66.666667\n",
       "8          11                764             288.0   476.0   62.303665\n",
       "10         12                522             208.0   314.0   60.153257\n",
       "11         13                335             139.0   196.0   58.507463\n",
       "14         15                189              83.0   106.0   56.084656\n",
       "7          10               1038             456.0   582.0   56.069364\n",
       "18         19                 25              12.0    13.0   52.000000\n",
       "42         42                  2               1.0     1.0   50.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_comparison_df.sort_values('per_change', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Change in token length (absolute number)')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(len_comparison_df.Token_len, len_comparison_df.change)\n",
    "\n",
    "plt.xlim(0,30)\n",
    "plt.xlabel('Length of token (number of characters)')\n",
    "plt.ylabel('Change in token (absolute number)')\n",
    "plt.title('Change in token length (absolute number)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Change in token length (% change)')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(len_comparison_df.Token_len, len_comparison_df.per_change)\n",
    "\n",
    "plt.xlim(0,30)\n",
    "plt.xlabel('Length of token (number of characters)')\n",
    "plt.ylabel('Change in token (% change)')\n",
    "plt.title('Change in token length (% change)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence segmentation\n",
    "\n",
    "Perform sentence segmentation on all documents in each domain. \n",
    "- Compare the distribution of the sentence length in the three domains. \n",
    "    - Here, the x-axis is the length of a sentence in number of words/tokens, and the y-axis is the number of sentences of such length. \n",
    "- Discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw['sentence_tokens'] = df_raw.Contents.apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENEVA : MSC continues to monitor and assess the developing situation in Israel to ensure the safety of its personnel and minimize disruption to customer supply chains.\n",
      "================== end of token 0 ==================\n",
      "Currently, Israelâ€™s major ports continue to function and both road and rail are fully operational in and around the country.\n",
      "================== end of token 1 ==================\n",
      "Bookings continue to be accepted to and from Israel, including selected IMO goods subject to the new government restrictions.\n",
      "================== end of token 2 ==================\n",
      "To ensure continued flexibility and facilitation of our customersâ€™ logistics operations, we have developed a suite of assistance for customers with cargo in Israel or bound for Israel:\n",
      "\n",
      "Detention and Demurrage (D&D) â€“ For import containers discharged in ASH or HFA, MSC will stop the clock for the period between 8 October 2023 and 8 November 2023, which will be considered as free days*.\n",
      "================== end of token 3 ==================\n",
      "*Valid until further notice and will be reviewed according to the situation.\n",
      "================== end of token 4 ==================\n",
      "MSC will continue to analyze and evaluate events as they develop and will communicate all new information and any changes to our customers as they occur.\n",
      "================== end of token 5 ==================\n",
      "Our staff are continuing to work remotely to ensure continuity of our operations and customer service and we thank them for their ongoing commitment.\n",
      "================== end of token 6 ==================\n",
      "If you have any further questions or require assistance, please contact your local MSC representatives.\n",
      "================== end of token 7 ==================\n"
     ]
    }
   ],
   "source": [
    "for idx, token in enumerate(df_raw.sentence_tokens[6]):\n",
    "    print(token)\n",
    "    print(f'================== end of token {idx} ==================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Contents</th>\n",
       "      <th>Source</th>\n",
       "      <th>token_words</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>sentence_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meta, Google quit tech summit over organizer's...</td>\n",
       "      <td>WASHINGTON: Meta and Google have pulled out of...</td>\n",
       "      <td>Google_News</td>\n",
       "      <td>[WASHINGTON, :, Meta, and, Google, have, pulle...</td>\n",
       "      <td>[WASHINGTON, Meta, and, Google, have, pulled, ...</td>\n",
       "      <td>[washington, meta, and, googl, have, pull, out...</td>\n",
       "      <td>[WASHINGTON: Meta and Google have pulled out o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>srael-Gaza live updates: Father of freed US ho...</td>\n",
       "      <td>Thousands of people have died and thousands mo...</td>\n",
       "      <td>Google_News</td>\n",
       "      <td>[Thousands, of, people, have, died, and, thous...</td>\n",
       "      <td>[Thousands, of, people, have, died, and, thous...</td>\n",
       "      <td>[thousand, of, peopl, have, die, and, thousand...</td>\n",
       "      <td>[Thousands of people have died and thousands m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asean and GCC leaders condemn attacks on civil...</td>\n",
       "      <td>RIYADH - Leaders of Asean and the Gulf Coopera...</td>\n",
       "      <td>Google_News</td>\n",
       "      <td>[RIYADH, -, Leaders, of, Asean, and, the, Gulf...</td>\n",
       "      <td>[RIYADH, Leaders, of, Asean, and, the, Gulf, C...</td>\n",
       "      <td>[riyadh, leader, of, asean, and, the, gulf, co...</td>\n",
       "      <td>[RIYADH - Leaders of Asean and the Gulf Cooper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meta, Google quit tech summit over organizer's...</td>\n",
       "      <td>Meta and Google have pulled out of the Web Sum...</td>\n",
       "      <td>Google_News</td>\n",
       "      <td>[Meta, and, Google, have, pulled, out, of, the...</td>\n",
       "      <td>[Meta, and, Google, have, pulled, out, of, the...</td>\n",
       "      <td>[meta, and, googl, have, pull, out, of, the, w...</td>\n",
       "      <td>[Meta and Google have pulled out of the Web Su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Live Updates | Current US assessment is Israel...</td>\n",
       "      <td>The office of Israeli Prime Minister Benjamin ...</td>\n",
       "      <td>Google_News</td>\n",
       "      <td>[The, office, of, Israeli, Prime, Minister, Be...</td>\n",
       "      <td>[The, office, of, Israeli, Prime, Minister, Be...</td>\n",
       "      <td>[the, offic, of, isra, prime, minist, benjamin...</td>\n",
       "      <td>[The office of Israeli Prime Minister Benjamin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Meta, Google quit tech summit over organizer's...   \n",
       "1  srael-Gaza live updates: Father of freed US ho...   \n",
       "2  Asean and GCC leaders condemn attacks on civil...   \n",
       "3  Meta, Google quit tech summit over organizer's...   \n",
       "4  Live Updates | Current US assessment is Israel...   \n",
       "\n",
       "                                            Contents       Source  \\\n",
       "0  WASHINGTON: Meta and Google have pulled out of...  Google_News   \n",
       "1  Thousands of people have died and thousands mo...  Google_News   \n",
       "2  RIYADH - Leaders of Asean and the Gulf Coopera...  Google_News   \n",
       "3  Meta and Google have pulled out of the Web Sum...  Google_News   \n",
       "4  The office of Israeli Prime Minister Benjamin ...  Google_News   \n",
       "\n",
       "                                         token_words  \\\n",
       "0  [WASHINGTON, :, Meta, and, Google, have, pulle...   \n",
       "1  [Thousands, of, people, have, died, and, thous...   \n",
       "2  [RIYADH, -, Leaders, of, Asean, and, the, Gulf...   \n",
       "3  [Meta, and, Google, have, pulled, out, of, the...   \n",
       "4  [The, office, of, Israeli, Prime, Minister, Be...   \n",
       "\n",
       "                                             no_punc  \\\n",
       "0  [WASHINGTON, Meta, and, Google, have, pulled, ...   \n",
       "1  [Thousands, of, people, have, died, and, thous...   \n",
       "2  [RIYADH, Leaders, of, Asean, and, the, Gulf, C...   \n",
       "3  [Meta, and, Google, have, pulled, out, of, the...   \n",
       "4  [The, office, of, Israeli, Prime, Minister, Be...   \n",
       "\n",
       "                                        stemmed_text  \\\n",
       "0  [washington, meta, and, googl, have, pull, out...   \n",
       "1  [thousand, of, peopl, have, die, and, thousand...   \n",
       "2  [riyadh, leader, of, asean, and, the, gulf, co...   \n",
       "3  [meta, and, googl, have, pull, out, of, the, w...   \n",
       "4  [the, offic, of, isra, prime, minist, benjamin...   \n",
       "\n",
       "                                     sentence_tokens  \n",
       "0  [WASHINGTON: Meta and Google have pulled out o...  \n",
       "1  [Thousands of people have died and thousands m...  \n",
       "2  [RIYADH - Leaders of Asean and the Gulf Cooper...  \n",
       "3  [Meta and Google have pulled out of the Web Su...  \n",
       "4  [The office of Israeli Prime Minister Benjamin...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Preprint\\nEFFICIENT STREAMING LANGUAGE MODELS\\nWITH ATTENTION SINKS\\nGuangxuan Xiao1âˆ—Yuandong Tian2Beidi Chen3Song Han1Mike Lewis2\\n1Massachusetts Institute of Technology\\n2Meta AI\\n3Carnegie Mellon University\\nhttps://github.com/mit-han-lab/streaming-llm\\nABSTRACT\\nDeploying Large Language Models (LLMs) in streaming applications such as\\nmulti-round dialogue, where long interactions are expected, is urgently needed but\\nposes two major challenges.',\n",
       " 'Firstly, during the decoding stage, caching previous\\ntokensâ€™ Key and Value states (KV) consumes extensive memory.',\n",
       " 'Secondly, popular\\nLLMs cannot generalize to longer texts than the training sequence length.',\n",
       " 'Window\\nattention, where only the most recent KVs are cached, is a natural approach â€” but\\nwe show that it fails when the text length surpasses the cache size.',\n",
       " 'We observe\\nan interesting phenomenon, namely attention sink , that keeping the KV of initial\\ntokens will largely recover the performance of window attention.',\n",
       " 'In this paper, we\\nfirst demonstrate that the emergence of attention sink is due to the strong attention\\nscores towards initial tokens as a â€œsinkâ€ even if they are not semantically important.',\n",
       " 'Based on the above analysis, we introduce StreamingLLM, an efficient framework\\nthat enables LLMs trained with a finite length attention window to generalize to\\ninfinite sequence length without any fine-tuning.',\n",
       " 'We show that StreamingLLM can\\nenable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language\\nmodeling with up to 4 million tokens and more.',\n",
       " 'In addition, we discover that\\nadding a placeholder token as a dedicated attention sink during pre-training can\\nfurther improve streaming deployment.',\n",
       " 'In streaming settings, StreamingLLM\\noutperforms the sliding window recomputation baseline by up to 22.2 Ã—speedup.',\n",
       " 'Code and datasets are provided in the link.',\n",
       " '1 I NTRODUCTION\\nLarge Language Models (LLMs) (Radford et al., 2018; Brown et al., 2020; Zhang et al., 2022;\\nOpenAI, 2023; Touvron et al., 2023a;b) are becoming ubiquitous, powering many natural language\\nprocessing applications such as dialog systems (Schulman et al., 2022; Taori et al., 2023; Chiang et al.,\\n2023), document summarization (Goyal & Durrett, 2020; Zhang et al., 2023a), code completion (Chen\\net al., 2021; RoziÃ¨re et al., 2023) and question answering (Kamalloo et al., 2023).',\n",
       " 'To unleash the\\nfull potential of pretrained LLMs, they should be able to efficiently and accurately perform long\\nsequence generation.',\n",
       " 'For example, an ideal ChatBot assistant can stably work over the content of\\nrecent day-long conversations.',\n",
       " 'However, it is very challenging for LLM to generalize to longer\\nsequence lengths than they have been pretrained on, e.g., 4K for Llama-2 Touvron et al.',\n",
       " '(2023b).',\n",
       " 'The reason is that LLMs are constrained by the attention window during pre-training.',\n",
       " 'Despite\\nsubstantial efforts to expand this window size (Chen et al., 2023; kaiokendev, 2023; Peng et al., 2023)\\nand improve training (Dao et al., 2022; Dao, 2023) and inference (Pope et al., 2022; Xiao et al., 2023;\\nAnagnostidis et al., 2023; Zhang et al., 2023b) efficiency for lengthy inputs, the acceptable sequence\\nlength remains intrinsically finite , which doesnâ€™t allow persistent deployments.',\n",
       " 'In this paper, we first introduce the concept of LLM streaming applications and ask the question:\\nCan we deploy an LLM for infinite-length inputs without sacrificing efficiency and performance?',\n",
       " 'âˆ—Part of the work done during an internship at Meta AI.',\n",
       " '1arXiv:2309.17453v1  [cs.CL]  29 Sep 2023\\n\\n============================== End of Page 1 ==============================\\n\\nPreprint\\n(a) Dense Attention(b) Window Attention(d) StreamingLLM (ours)\\nAttention Sinkâ‹¯T cached tokensâ‹¯L cached tokensâ‹¯T-L evicted tokensCurrent Token(c) Sliding Window  w/ Re-computation\\nL re-computed tokensâ‹¯previous tokens are truncatedâ‹¯L cached tokensâ‹¯evicted tokensO(T2)O(TL)O(TL2)O(TL)PPL: 5158PPL: 5641PPL: 5.43PPL: 5.40\\nHas poor efficiency and performance on long text.Breaks when initial tokens are evicted.Has to re-compute cache for each incoming token.Can perform efficient and stable language modeling on long texts.',\n",
       " 'Figure 1: Illustration of StreamingLLM vs. existing methods.',\n",
       " 'The language model, pre-trained on\\ntexts of length L, predicts the Tth token ( Tâ‰«L).',\n",
       " '(a) Dense Attention has O(T2)time complexity\\nand an increasing cache size.',\n",
       " 'Its performance decreases when the text length exceeds the pre-training\\ntext length.',\n",
       " '(b) Window Attention caches the most recent Ltokensâ€™ KV .',\n",
       " 'While efficient in inference,\\nperformance declines sharply once the starting tokensâ€™ keys and values are evicted.',\n",
       " '(c) Sliding\\nWindow with Re-computation rebuilds the KV states from the Lrecent tokens for each new token.',\n",
       " 'While it performs well on long texts, its O(TL2)complexity, stemming from quadratic attention\\nin context re-computation, makes it considerably slow.',\n",
       " '(d) StreamingLLM keeps the attention sink\\n(several initial tokens) for stable attention computation, combined with the recent tokens.',\n",
       " 'Itâ€™s efficient\\nand offers stable performance on extended texts.',\n",
       " 'Perplexities are measured using the Llama-2-13B\\nmodel on the first book (65K tokens) in the PG-19 test set.',\n",
       " 'When applying LLMs for infinite input streams, two primary challenges arise:\\n1.During the decoding stage, Transformer-based LLMs cache the Key and Value states (KV)\\nof all previous tokens, as illustrated in Figure 1 (a), which can lead to excessive memory\\nusage and increasing decoding latency (Pope et al., 2022).',\n",
       " '2.Existing models have limited length extrapolation abilities, i.e., their performance de-\\ngrades (Press et al., 2022; Chen et al., 2023) when the sequence length goes beyond the\\nattention window size set during pre-training.',\n",
       " 'An intuitive approach, known as window attention (Beltagy et al., 2020) (Figure 1 b), maintains only\\na fixed-size sliding window on the KV states of most recent tokens.',\n",
       " 'Although it ensures constant\\nmemory usage and decoding speed after the cache is initially filled, the model collapses once the\\nsequence length exceeds the cache size, i.e., even just evicting the KV of the first token , as illustrated\\nin Figure 3.',\n",
       " 'Another strategy is the sliding window with re-computation (shown in Figure 1 c), which\\nrebuilds the KV states of recent tokens for each generated token.',\n",
       " 'While it offers strong performance,\\nthis approach is significantly slower due to the computation of quadratic attention within its window,\\nmaking this method impractical for real-world streaming applications.',\n",
       " 'To understand the failure of window attention, we find an interesting phenomenon of autoregressive\\nLLMs: a surprisingly large amount of attention score is allocated to the initial tokens, irrespective\\nof their relevance to the language modeling task, as visualized in Figure 2.',\n",
       " 'We term these tokens\\nâ€œattention sinks \".',\n",
       " 'Despite their lack of semantic significance, they collect significant attention scores.',\n",
       " 'We attribute the reason to the Softmax operation, which requires attention scores to sum up to one\\nfor all contextual tokens.',\n",
       " 'Thus, even when the current query does not have a strong match in many\\nprevious tokens, the model still needs to allocate these unneeded attention values somewhere so it\\nsums up to one.',\n",
       " 'The reason behind initial tokens as sink tokens is intuitive: initial tokens are visible\\nto almost all subsequent tokens because of the autoregressive language modeling nature, making\\nthem more readily trained to serve as attention sinks.',\n",
       " 'Based on the above insights, we propose StreamingLLM, a simple and efficient framework that\\nenables LLMs trained with a finite attention window to work on text of infinite length without fine-\\ntuning.',\n",
       " 'StreamingLLM exploits the fact that attention sinks have high attention values, and preserving\\nthem can maintain the attention score distribution close to normal.',\n",
       " 'Therefore, StreamingLLM simply\\nkeeps the attention sink tokensâ€™ KV (with just 4 initial tokens sufficing) together with the sliding\\nwindowâ€™s KV to anchor the attention computation and stabilize the modelâ€™s performance.',\n",
       " 'With\\nStreamingLLM, models including Llama-2-[7, 13, 70]B, MPT-[7, 30]B, Falcon-[7, 40]B, and Pythia-\\n2\\n\\n============================== End of Page 2 ==============================\\n\\nPreprint\\nLayer 0 Head 0Layer 1 Head 0Layer 2 Head 0\\nLayer 23 Head 0Layer 31 Head 0Layer 16 Head 0Layer 9 Head 0\\nFigure 2: Visualization of the average attention logits in Llama-2-7B over 256 sentences, each with a length\\nof 16.',\n",
       " 'Observations include: (1) The attention maps in the first two layers (layers 0 and 1) exhibit the \"local\"\\npattern, with recent tokens receiving more attention.',\n",
       " '(2) Beyond the bottom two layers, the model heavily attends\\nto the initial token across all layers and heads.',\n",
       " '[2.9,6.9,12]B can reliably model 4 million tokens, and potentially even more.',\n",
       " 'Compared with the only\\nviable baseline, sliding window with recomputation, StreamingLLM achieves up to 22.2 Ã—speedup,\\nrealizing the streaming use of LLMs.',\n",
       " 'Finally, we confirm our attention sink hypothesis and demonstrate that language models can be\\npre-trained to require only a single attention sink token for streaming deployment.',\n",
       " 'Specifically, we\\nsuggest that an extra learnable token at the beginning of all training samples can serve as a designated\\nattention sink.',\n",
       " 'By pre-training 160-million parameter language models from scratch, we demonstrate\\nthat adding this single sink token preserves the modelâ€™s performance in streaming cases.',\n",
       " 'This stands in\\ncontrast to vanilla models, which necessitate the reintroduction of multiple initial tokens as attention\\nsinks to achieve the same performance level.',\n",
       " '2 R ELATED WORK\\nExtensive research has been done on applying LLMs to lengthy texts, with three main areas of focus:\\nLength Extrapolation ,Context Window Extension , and Improving LLMsâ€™ Utilization of Long\\nText.',\n",
       " 'While seemingly related, itâ€™s worth noting that progress in one direction doesnâ€™t necessarily\\nlead to progress in the other.',\n",
       " 'For example, extending the context size of LLMs doesnâ€™t improve the\\nmodelâ€™s performance beyond the context size, and neither approach ensures effective use of the long\\ncontext.',\n",
       " 'Our StreamingLLM framework primarily lies in the first category, where LLMs are applied\\nto text significantly exceeding the pre-training window size, potentially even of infinite length.',\n",
       " 'We do\\nnot expand the attention window size of LLMs or enhance the modelâ€™s memory and usage on long\\ntexts.',\n",
       " 'The last two categories are orthogonal to our focus and could be integrated with our techniques.',\n",
       " 'Length extrapolation aims to enable language models trained on shorter texts to handle longer\\nones during testing.',\n",
       " 'A predominant avenue of research targets the development of relative position\\nencoding methods for Transformer models, enabling them to function beyond their training window.',\n",
       " 'One such initiative is Rotary Position Embeddings (RoPE) (Su et al., 2021), which transforms\\nthe queries and keys in every attention layer for relative position integration.',\n",
       " 'Despite its promise,\\nsubsequent research (Press et al., 2022; Chen et al., 2023) indicated its underperformance on text\\nthat exceeds the training window.',\n",
       " 'Another approach, ALiBi (Press et al., 2022), biases the query-key\\nattention scores based on their distance, thereby introducing relative positional information.',\n",
       " 'While\\nthis exhibited improved extrapolation, our tests on MPT models highlighted a breakdown when the\\ntext length was vastly greater than the training length.',\n",
       " 'Current methodologies, however, have yet to\\nachieve infinite length extrapolation, causing no existing LLMs to fit for streaming applications.',\n",
       " 'Context Window Extension centers on expanding the LLMsâ€™ context window, enabling the process-\\ning of more tokens in one forward pass.',\n",
       " 'A primary line of work addresses the training efficiency\\nproblem.',\n",
       " 'Given the attention to computationâ€™s quadratic complexity during training, developing\\na long-context LLM is both a computational and memory challenge.',\n",
       " 'Solutions have ranged from\\nsystem-focused optimizations like FlashAttention (Dao et al., 2022; Dao, 2023), which accelerates\\nattention computation and reduces memory footprint, to approximate attention methods (Zaheer\\net al., 2020; Beltagy et al., 2020; Wang et al., 2020; Kitaev et al., 2020) that trade model quality for\\nefficiency.',\n",
       " 'Recently, there has been a surge of work on extending pre-trained LLMs with RoPE (Chen\\net al., 2023; kaiokendev, 2023; bloc97, 2023; Peng et al., 2023), involving position interpolation and\\nfine-tuning.',\n",
       " 'However, all the aforementioned techniques only extend LLMsâ€™ context window to a\\nlimited extent, which falls short of our paperâ€™s primary concern of handling limitless inputs.',\n",
       " 'Improving LLMsâ€™ Utilization of Long Text optimizes LLMs to better capture and employ the\\ncontent within the context rather than merely taking them as inputs.',\n",
       " 'As highlighted by Liu et al.',\n",
       " '3\\n\\n============================== End of Page 3 ==============================\\n\\nPreprint\\nDense AttentionWindow AttentionSliding Window \\nw/ Re-computationStreamingLLM\\nDense AttentionWindow AttentionStreamingLLM\\nFigure 3: Language modeling perplexity on texts with 20K tokens across various LLM.',\n",
       " 'Observations\\nreveal consistent trends: (1) Dense attention fails once the input length surpasses the pre-training\\nattention window size.',\n",
       " '(2) Window attention collapses once the input length exceeds the cache size,\\ni.e., the initial tokens are evicted.',\n",
       " '(3) StreamingLLM demonstrates stable performance, with its\\nperplexity nearly matching that of the sliding window with re-computation baseline.',\n",
       " 'and Li et al., success in the previously mentioned two directions does not necessarily translate to\\ncompetent utilization of lengthy contexts.',\n",
       " 'Addressing this effective usage of prolonged contexts\\nwithin LLMs is still a challenge.',\n",
       " 'Our work concentrates on stably harnessing the most recent tokens,\\nenabling the seamless streaming application of LLMs.',\n",
       " '3 S TREAMING LLM\\n3.1 T HEFAILURE OF WINDOW ATTENTION AND ATTENTION SINKS\\nWhile the window attention technique offers efficiency during inference, it results in an exceedingly\\nhigh language modeling perplexity.',\n",
       " 'Consequently, the modelâ€™s performance is unsuitable for deploy-\\nment in streaming applications.',\n",
       " 'In this section, we use the concept of attention sink to explain the\\nfailure of window attention, serving as the inspiration behind StreamingLLM.',\n",
       " 'Identifying the Point of Perplexity Surge.',\n",
       " 'Figure 3 shows the perplexity of language modeling on\\na 20K token text.',\n",
       " 'It is evident that perplexity spikes when the text length surpasses the cache size, led\\nby the exclusion of initial tokens.',\n",
       " 'This suggests that the initial tokens, regardless of their distance\\nfrom the tokens being predicted, are crucial for maintaining the stability of LLMs.',\n",
       " 'Why do LLMs break when removing initial tokensâ€™ KV?',\n",
       " 'We visualize attention maps from\\nall layers and heads of the Llama-2-7B and models in Figure 2.',\n",
       " 'We find that, beyond the bottom\\ntwo layers, the model consistently focuses on the initial tokens across all layers and heads.',\n",
       " 'The\\nimplication is clear: removing these initial tokensâ€™ KV will remove a considerable portion of the\\ndenominator in the SoftMax function (Equation 1) in attention computation.',\n",
       " 'This alteration leads to a\\nsignificant shift in the distribution of attention scores away from what would be expected in normal\\ninference settings.',\n",
       " 'SoftMax (x)i=exi\\nex1+PN\\nj=2exj, x 1â‰«xj, jâˆˆ2, .',\n",
       " '. . , N (1)\\nThere are two possible explanations for the importance of the initial tokens in language modeling:\\n(1) Either their semantics are crucial, or (2) the model learns a bias towards their absolute position.',\n",
       " 'To distinguish between these possibilities, we conduct experiments (Table 1), wherein the first four\\ntokens are substituted with the linebreak token â€œ\\\\n\".',\n",
       " 'The observations indicate that the model still\\nsignificantly emphasizes these initial linebreak tokens.',\n",
       " 'Furthermore, reintroducing them restores\\nthe language modeling perplexity to levels comparable to having the original initial tokens.',\n",
       " 'This\\nsuggests that the absolute position of the starting tokens, rather than their semantic value, holds\\ngreater significance.',\n",
       " 'LLMs attend to Initial Tokens as Attention Sinks.',\n",
       " 'To explain why the model disproportionately\\nfocuses on initial tokensâ€”regardless of their semantic relevance to language modeling, we introduce\\nthe concept of â€œ attention sink \".',\n",
       " 'The nature of the SoftMax function (Equation 1) prevents all attended\\ntokens from having zero values.',\n",
       " 'This requires aggregating some information from other tokens across\\nall heads in all layers, even if the current embedding has sufficient self-contained information for its\\nprediction.',\n",
       " 'Consequently, the model tends to dump unnecessary attention values to specific tokens.',\n",
       " 'A\\nsimilar observation has been made in the realm of quantization outliers (Xiao et al., 2023; Bondarenko\\net al., 2023), leading to the proposal of SoftMax-Off-by-One (Miller, 2023) as a potential remedy.',\n",
       " '4\\n\\n============================== End of Page 4 ==============================\\n\\nPreprint\\nTable 1: Window attention has poor per-\\nformance on long text.',\n",
       " 'The perplexity is\\nrestored when we reintroduce the initial\\nfour tokens alongside the recent 1020 to-\\nkens (4+1020).',\n",
       " 'Substituting the original\\nfour initial tokens with linebreak tokens\\nâ€œ\\\\n\" (4\"\\\\n\"+1020) achieves comparable per-\\nplexity restoration.',\n",
       " 'Cache config x+y de-\\nnotes adding x initial tokens with y recent\\ntokens.',\n",
       " 'Perplexities are measured on the\\nfirst book (65K tokens) in the PG19 test set.',\n",
       " 'Llama-2-13B PPL ( â†“)\\n0 + 1024 (Window) 5158.07\\n4 + 1020 5.40\\n4\"\\\\n\"+1020 5.60Table 2: Effects of reintroduced initial token numbers on\\nStreamingLLM.',\n",
       " '(1) Window attention (0+y) has a dras-\\ntic increase in perplexity.',\n",
       " '(2) Introducing one or two ini-\\ntial tokens usually doesnâ€™t suffice to fully restore model\\nperplexity, indicating that the model doesnâ€™t solely use\\nthe first token as the attention sink.',\n",
       " '(3) Introducing four\\ninitial tokens generally suffices; further additions have\\ndiminishing returns.',\n",
       " 'Cache config x+y denotes adding x\\ninitial tokens to y recent tokens.',\n",
       " 'Perplexities are evalu-\\nated on 400K tokens in the concatenated PG19 test set.',\n",
       " 'Cache Config 0+2048 1+2047 2+2046 4+2044 8+2040\\nFalcon-7B 17.90 12.12 12.12 12.12 12.12\\nMPT-7B 460.29 14.99 15.00 14.99 14.98\\nPythia-12B 21.62 11.95 12.09 12.09 12.02\\nCache Config 0+4096 1+4095 2+4094 4+4092 8+4088\\nLlama-2-7B 3359.95 11.88 10.51 9.59 9.54\\nWhy do various autoregressive LLMs, such as Llama-2, MPT, Falcon, and Pythia, consistently focus\\noninitial tokens as their attention sinks, rather than other tokens?',\n",
       " 'Our explanation is straightforward:\\nDue to the sequential nature of autoregressive language modeling, initial tokens are visible to all\\nsubsequent tokens, while later tokens are only visible to a limited set of subsequent tokens.',\n",
       " 'As a result,\\ninitial tokens are more easily trained to serve as attention sinks, capturing unnecessary attention.',\n",
       " 'Weâ€™ve noted that LLMs are typically trained to utilize multiple initial tokens as attention sinks rather\\nthan just one.',\n",
       " 'As illustrated in Figure 2, the introduction of four initial tokens, as attention sinks,\\nsuffices to restore the LLMâ€™s performance.',\n",
       " 'In contrast, adding just one or two doesnâ€™t achieve full\\nrecovery.',\n",
       " 'We believe this pattern emerges because these models didnâ€™t include a consistent starting\\ntoken across all input samples during pre-training.',\n",
       " 'Although Llama-2 does prefix each paragraph\\nwith a â€œ<s>\" token, itâ€™s applied before text chunking, resulting in a mostly random token occupying\\nthe zeroth position.',\n",
       " 'This lack of a uniform starting token leads the model to use several initial tokens\\nas attention sinks.',\n",
       " 'We hypothesize that by incorporating a stable learnable token at the start of all\\ntraining samples, it could singularly act as a committed attention sink, eliminating the need for\\nmultiple initial tokens to ensure consistent streaming.',\n",
       " 'We will validate this hypothesis in Section 3.3.',\n",
       " '3.2 R OLLING KV C ACHE WITH ATTENTION SINKS\\n01234567Generating Token 13Generating Token 14Generating Token 15\\nAttention SinksRolling KV Cache012310111213140123101112131415Evicted Tokens01234567Generating Token 7012345678Generating Token 80123456789Evicted TokensRolling KV CacheGenerating Token 9\\nFigure 4: The KV cache of StreamingLLM.To enable LLM streaming in already trained LLMs, we\\npropose a straightforward method that can recover win-\\ndow attentionâ€™s perplexity without any model finetuning.',\n",
       " 'Alongside the current sliding window tokens, we reintro-\\nduce a few starting tokensâ€™ KV in the attention computa-\\ntion.',\n",
       " 'The KV cache in StreamingLLM can be conceptually divided into two parts, as illustrated\\nin Figure 4: (1) Attention sinks (four initial tokens) stabilize the attention computation; 2) Rolling\\nKV Cache retains the most recent tokens, crucial for language modeling.',\n",
       " 'StreamingLLMâ€™ design is\\nversatile and can be seamlessly incorporated into any autoregressive language model that employs\\nrelative positional encoding, such as RoPE (Su et al., 2021) and ALiBi (Press et al., 2022).',\n",
       " 'When determining the relative distance and adding positional information to tokens, StreamingLLM\\nfocuses on positions within the cache rather than those in the original text .',\n",
       " 'This distinction is crucial\\nfor StreamingLLMâ€™s performance.',\n",
       " 'For instance, if the current cache has tokens [0, 1, 2, 3, 6, 7, 8]\\nand is in the process of decoding the 9th token, the positions assigned are [0, 1, 2, 3, 4, 5, 6, 7], rather\\nthan the positions in the original text, which would be [0, 1, 2, 3, 6, 7, 8, 9].',\n",
       " 'For encoding like RoPE, we cache the Keys of tokens prior to introducing the rotary transformation.',\n",
       " 'Then, we apply position transformation to the keys in the rolling cache at each decoding phase.',\n",
       " 'On the\\nother hand, integrating with ALiBi is more direct.',\n",
       " 'Here, the contiguous linear bias is applied instead\\nof a â€™jumpingâ€™ bias to the attention scores.',\n",
       " 'This method of assigning positional embedding within the\\ncache is crucial to StreamingLLMâ€™s functionality, ensuring that the model operates efficiently even\\nbeyond its pre-training attention window size.',\n",
       " '5\\n\\n============================== End of Page 5 ==============================\\n\\nPreprint\\n3.3 P RE-TRAINING LLM S WITH ATTENTION SINKS\\nTable 3: Comparison of vanilla attention with\\nprepending a zero token and a learnable sink token\\nduring pre-training.',\n",
       " 'To ensure stable streaming\\nperplexity, the vanilla model required several ini-\\ntial tokens.',\n",
       " 'While Zero Sink demonstrated a slight\\nimprovement, it still needed other initial tokens.',\n",
       " 'Conversely, the model trained with a learnable\\nSink Token showed stable streaming perplexity\\nwith only the sink token added.',\n",
       " 'Cache config x+y\\ndenotes adding xinitial tokens with yrecent to-\\nkens.',\n",
       " 'Perplexity is evaluated on the first sample in\\nthe PG19 test set.',\n",
       " 'Cache Config 0+1024 1+1023 2+1022 4+1020\\nVanilla 27.87 18.49 18.05 18.05\\nZero Sink 29214 19.90 18.27 18.01\\nLearnable Sink 1235 18.01 18.01 18.02As elaborated in Section 3.1, a significant reason\\nfor the modelâ€™s excessive attention to multiple\\ninitial tokens is the absence of a designated sink\\ntoken to offload excessive attention scores.',\n",
       " 'Due\\nto this, the model inadvertently designates glob-\\nally visible tokens, primarily the initial ones,\\nas attention sinks.',\n",
       " 'A potential remedy can be\\nthe intentional inclusion of a global trainable\\nattention sink token, denoted as a â€œSink Token\",\\nwhich would serve as a repository for unneces-\\nsary attention scores.',\n",
       " 'Alternatively, replacing\\nthe conventional SoftMax function with a vari-\\nant like SoftMax-off-by-One (Miller, 2023),\\nSoftMax 1(x)i=exi\\n1 +PN\\nj=1exj, (2)\\nwhich does not require the attention scores on all contextual tokens to sum up to one, might also be\\neffective.',\n",
       " 'Note that this SoftMax alternative is equivalent to using a token with an all-zero Key and\\nValue features in the attention computation.',\n",
       " 'We denote this method as â€œZero Sink\" to fit it consistently\\nin our framework.',\n",
       " 'For validation, we pre-train three language models with 160 million parameters from scratch under\\nidentical settings.',\n",
       " 'The first model utilizes the standard SoftMax attention (Vanilla), the second\\nreplaced the regular attention mechanism with SoftMax 1(Zero Sink), and one prepending a learnable\\nplaceholder token (Sink Token) in all training samples.',\n",
       " 'As shown in Table 3, while the zero sink\\nalleviates the attention sink problem to some extent, the model still relies on other initial tokens as\\nattention sinks.',\n",
       " 'Introducing a sink token is highly effective in stabilizing the attention mechanism.',\n",
       " 'Simply pairing this sink token with recent tokens sufficiently anchors the modelâ€™s performance, and\\nthe resulting evaluation perplexity is even marginally improved.',\n",
       " 'Given these findings, we recommend\\ntraining future LLMs with a sink token in all samples to optimize streaming deployment.',\n",
       " '4 E XPERIMENTS\\nWe evaluate StreamingLLM using four prominent recent model families: Llama-2 (Touvron et al.,\\n2023b), MPT (Team, 2023), PyThia (Biderman et al., 2023), and Falcon (Almazrouei et al., 2023).',\n",
       " 'Notably, Llama-2, Falcon, and Pythia incorporate RoPE (Su et al., 2021), whereas MPT employs\\nALiBi (Press et al., 2022) â€” two of the most influential position encoding techniques in recent\\nresearch.',\n",
       " 'Our diverse model selection ensures the validity and robustness of our findings.',\n",
       " 'We bench-\\nmark StreamingLLM against established baselines such as dense attention, window attention, and the\\nsliding window approach with re-computation.',\n",
       " 'In all subsequent experiments with StreamingLLM,\\nwe default to using four initial tokens as attention sinks unless stated otherwise.',\n",
       " '4.1 L ANGUAGE MODELING ON LONG TEXTS ACROSS LLM F AMILIES AND SCALES\\nWe firstly evaluate StreamingLLMâ€™s language modeling perplexity using the concatenated PG19 (Rae\\net al., 2020) test set, which contains 100 long books.',\n",
       " 'For Llama-2 models, the cache size is set at\\n2048, while for Falcon, Pythia, and MPT models, itâ€™s set at 1024.',\n",
       " 'This is half the pre-training window\\nsize chosen to enhance visualization clarity.',\n",
       " 'Figure 3 illustrates that StreamingLLM can match the oracle baseline (sliding window with re-\\ncomputation) in terms of perplexity on texts spanning 20K tokens.',\n",
       " 'Meanwhile, the dense attention\\ntechnique fails when the input length exceeds its pre-training window, and the window attention\\ntechnique struggles when the input length surpasses the cache size, leading to the eviction of the initial\\ntokens.',\n",
       " 'In Figure 5, we further substantiate that StreamingLLM can reliably handle exceptionally\\nextended texts, encompassing more than 4 million tokens, across a spectrum of model families and\\nscales.',\n",
       " 'This includes Llama-2-[7,13,70]B, Falcon-[7,40]B, Pythia-[2.8,6.9,12]B, and MPT-[7,30]B.',\n",
       " '6\\n\\n============================== End of Page 6 ==============================\\n\\nPreprint\\nDense AttentionWindow AttentionSliding Window \\nw/ Re-computationStreamingLLM\\nDense AttentionWindow AttentionStreamingLLM\\nFigure 5: Language modeling perplexity of StreamingLLM for super long texts with 4 million tokens\\nacross various LLM families and model scales.',\n",
       " 'The perplexity remains stable throughout.',\n",
       " 'We use the\\nconcatenated test set of PG19 (100 books) to perform language modeling, with perplexity fluctuations\\nattributable to the transition between books.',\n",
       " '4.2 R ESULTS OF PRE-TRAINING WITH A SINKTOKEN\\nTo validate our suggestion that introducing a sink token to all pre-training samples improves stream-\\ning LLMs, we trained two language models, each with 160 million parameters, under identical\\nconditions.',\n",
       " 'While one model adhered to the original training settings, the other incorporated a sink\\ntoken at the start of every training sample.',\n",
       " 'Our experiments employed the Pythia-160M (Bider-\\nman et al., 2023) codebase and followed its training recipe.',\n",
       " 'We train the models on an 8xA6000\\nNVIDIA GPU server using the deduplicated Pile (Gao et al., 2020) dataset.',\n",
       " 'Apart from reducing\\nthe training batch size to 256, we retained all Pythia training configurations, including learning rate\\nschedules, model initialization, and dataset permutations.',\n",
       " 'Both models were trained for 143,000 steps.',\n",
       " 'Figure 6: Pre-training loss\\ncurves of models w/ and w/o sink\\ntokens.',\n",
       " 'Two models have a simi-\\nlar convergence trend.Table 4: Zero-shot accuracy (in %) across 7 NLP benchmarks,\\nincluding ARC-[Challenge, Easy], HellaSwag, LAMBADA, Open-\\nbookQA, PIQA, and Winogrande.',\n",
       " 'The inclusion of a sink token\\nduring pre-training doesnâ€™t harm the model performance.',\n",
       " 'Methods ARC-c ARC-e HS LBD OBQA PIQA WG\\nVanilla 18.6 45.2 29.4 39.6 16.0 62.2 50.1\\n+Sink Token 19.6 45.6 29.8 39.9 16.6 62.6 50.8\\nConvergence and Normal Model Performance.',\n",
       " 'Including a sink token during pre-training has no\\nnegative impact on model convergence and subsequent performance on a range of NLP benchmarks.',\n",
       " 'As depicted in Figure 6, models trained with a sink token exhibit similar convergence dynamics\\ncompared to their vanilla counterparts.',\n",
       " 'We evaluate the two models on seven diverse NLP bench-\\nmarks, including ARC-[Challenge, Easy] (Clark et al., 2018), HellaSwag (Zellers et al., 2019),\\nLAMBADA (Paperno et al., 2016), OpenbookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020),\\nand Winogrande (Sakaguchi et al., 2019).',\n",
       " 'As shown in Table 4, the model pre-trained with a sink\\ntoken performs similarly to that trained using the vanilla approach.',\n",
       " 'Streaming Performance.',\n",
       " 'As illustrated in Table 3, the streaming perplexities differ between\\nmodels trained using traditional methods and those augmented with a sink token.',\n",
       " 'Remarkably,\\nthe vanilla model requires the addition of multiple tokens as attention sinks to maintain stable\\nstreaming perplexity.',\n",
       " 'In contrast, the model trained with a sink token achieves satisfactory streaming\\nperformance using just the sink token.',\n",
       " 'Below is a record of lines I want you to remember.',\n",
       " 'The REGISTER_CONTENT in line 0 is <8806> [omitting 9 linesâ€¦] The REGISTER_CONTENT in line 10 is <24879> [omitting 8 linesâ€¦] The REGISTER_CONTENT in line 20 is <45603> Query: The REGISTER_CONTENT in line 0 is The REGISTER_CONTENT in line 21 is <29189> [omitting 8 linesâ€¦] The REGISTER_CONTENT in line 30 is <1668> Query: The REGISTER_CONTENT in line 10 is The REGISTER_CONTENT in line 31 is <42569> [omitting 8 linesâ€¦] The REGISTER_CONTENT in line 40 is <34579> Query: The REGISTER_CONTENT in line 20 is [omitting remaining 5467 linesâ€¦]Input Content\\nDesired Output[â€œ<8806>â€, â€œ<24879>â€, â€œ<45603>â€, â€¦]\\nFigure 8: The first sample in StreamEval.Attention Visualization.',\n",
       " 'Figure 7 contrasts atten-\\ntion maps for models pre-trained with and without a\\nsink token.',\n",
       " 'The model without the sink token, similar\\nto Llama-2-7B (Figure 2), shows early-layer local\\nattention and deeper-layer focus on initial tokens.',\n",
       " 'In contrast, models trained with a sink token con-\\nsistently concentrate on the sink across layers and\\nheads, indicating an effective attention offloading\\nmechanism.',\n",
       " 'This strong focus on the sink, with re-\\nduced attention to other initial tokens, explains the\\nsink tokenâ€™s efficacy in enhancing modelâ€™s streaming\\nperformance.',\n",
       " '7\\n\\n============================== End of Page 7 ==============================\\n\\nPreprint\\nPre-Trained without Sink TokenPre-Trained with Sink Token\\nLayer 0 Head 0Layer 2 Head 0Layer 10 Head 0Layer 0 Head 0Layer 2 Head 0Layer 10 Head 0\\nFigure 7: Visualization of average attention logits over 256 sentences, each 16 tokens long, comparing models\\npre-trained with (left) and without (right) a sink token.',\n",
       " 'Both maps show the same layers and heads.',\n",
       " 'Key\\nobservations: (1) Without a sink token, models show local attention in lower layers and increased attention\\nto initial tokens in deeper layers.',\n",
       " '(2) With a sink token, there is clear attention directed at it across all layers,\\neffectively collecting redundant attention.',\n",
       " '(3) With the presence of the sink token, less attention is given to other\\ninitial tokens, supporting the benefit of designating the sink token to enhance the streaming performance.',\n",
       " 'Table 5: Accuracy (in %) on the ARC-[Easy, Challenge] datasets.',\n",
       " 'Questions were concatenated and answered\\nin a streaming manner to mimic a real-world chat setting.',\n",
       " 'The dense baseline fails due to Out-of-Memory\\n(OOM) errors.',\n",
       " 'Window attention has poor accuracy.',\n",
       " 'StreamingLLM has comparable results with the one-shot\\nsample-by-sample baseline.',\n",
       " 'Window attention and StreamingLLM use cache sizes of 1024.',\n",
       " 'Model Llama-2-7B-Chat Llama-2-13B-Chat Llama-2-70B-Chat\\nDataset Arc-E Arc-C Arc-E Arc-C Arc-E Arc-C\\nOne-shot 71.25 53.16 78.16 63.31 91.29 78.50\\nDense OOM\\nWindow 3.58 1.39 0.25 0.34 0.12 0.32\\nStreamingLLM 71.34 55.03 80.89 65.61 91.37 80.20\\n4.3 R ESULTS ON STREAMING QUESTION ANSWERING WITH INSTRUCTION -TUNED MODELS\\nTo show StreamingLLMâ€™s real-world applicability, we emulate multi-round question-answering using\\ninstruction-tuned LLMs, commonly used in real-world scenarios.',\n",
       " 'We first concatenate all question-answer pairs from the ARC-[Challenge, Easy] datasets, feed the\\ncontinuous stream to Llama-2-[7,13,70]B-Chat models, and assess model completions at each answer\\nposition using an exact match criterion.',\n",
       " 'As table 5 indicates, dense attention results in Out-of-Memory\\n(OOM) errors, showing it unsuitable for this setting.',\n",
       " 'While the window attention method works\\nefficiently, it exhibits low accuracy due to random outputs when the input length exceeds the cache\\nsize.',\n",
       " 'Conversely, StreamingLLM excels by efficiently handling the streaming format, aligning with\\nthe one-shot, sample-by-sample baseline accuracy.',\n",
       " 'Highlighting a more fitting scenario for StreamingLLM, we introduce a dataset, StreamEval, inspired\\nby the LongEval (Li et al., 2023) benchmark.',\n",
       " 'As depicted in Figure 8, diverging from LongEvalâ€™s\\nsingle query over a long-span setup, we query the model every 10 lines of new information.',\n",
       " 'Each\\nqueryâ€™s answer is consistently 20 lines prior, reflecting real-world instances where questions typically\\npertain to recent information.',\n",
       " 'As illustrated in Figure 9, LLMs employing StreamingLLM maintain\\nreasonable accuracy even as input lengths approach 120K tokens.',\n",
       " 'In contrast, both dense and window\\nattention fail at the pre-training text length and the KV cache size, respectively.',\n",
       " 'Additionally, we\\nutilize two context-extended models, LongChat-7b-v1.5-32k (Li et al., 2023) and Llama-2-7B-32K-\\nInstruct (Together, 2023), to show that StreamingLLM can complement context extension techniques.',\n",
       " 'Within StreamingLLM, context extension means broadening the maximum cache size of streaming\\nLLMs, enabling the capture of broader local information.',\n",
       " '4.4 A BLATION STUDIES\\nNumbers of Initial Tokens.',\n",
       " 'In Table 2, we ablate the effect of adding varying numbers of initial\\ntokens with recent tokens on the streaming perplexity.',\n",
       " 'The results show the insufficiency of introduc-\\ning merely one or two initial tokens, whereas a threshold of four initial tokens appears enough, with\\nsubsequent additions contributing marginal effects.',\n",
       " 'This result justifies our choice of introducing 4\\ninitial tokens as attention sinks in StreamingLLM.',\n",
       " '8\\n\\n============================== End of Page 8 ==============================\\n\\nPreprint\\nDense AttentionWindow AttentionSliding Window \\nw/ Re-computationStreamingLLM\\nDense AttentionWindow AttentionStreamingLLM\\nFigure 9: Performance on the StreamEval benchmark.',\n",
       " 'Accuracies are averaged over 100 samples.',\n",
       " 'Latency (ms)040080012001600256512102420484096\\n65\\n45\\n35\\n31\\n31\\n1411\\n523\\n223\\n103\\n63Sliding Window with Re-computationStreamingLLMMemory (GB)071421256512102420484096\\n19\\n16\\n14\\n13\\n13\\n21\\n16\\n14\\n13\\n13Latency (ms)0750150022503000256512102420484096\\n106\\n75\\n60\\n52\\n48\\n2355\\n860\\n361\\n169\\n99Memory (GB)010192938256512102420484096\\n34\\n29\\n27\\n26\\n25\\n36\\n29\\n26\\n25\\n25Llama-2-7BLlama-2-13B\\nFigure 10: Comparison of per-token decoding latency and memory usage between the sliding window\\napproach with re-computation baseline and StreamingLLM, plotted against the cache size (attention\\nwindow size) on the X-axis.',\n",
       " 'StreamingLLM delivers a remarkable speedup of up to 22.2 Ã—per token\\nand retains a memory footprint similar to the re-computation baseline.',\n",
       " 'Cache Sizes.',\n",
       " 'In Table 6, we evaluate cache sizeâ€™s impact on StreamingLLMâ€™s perplex-\\nity.',\n",
       " 'Contrary to intuition, increasing the cache size doesnâ€™t consistently lower the lan-\\nguage modeling perplexity.',\n",
       " 'This inconsistency shows a potential limitation where these mod-\\nels might not maximize the utility of the entire context they receive.',\n",
       " 'Future research ef-\\nforts should target enhancing these modelsâ€™ capabilities to utilize extensive contexts better.',\n",
       " 'Table 6: Effects of cache size on\\nStreamingLLMâ€™s performance.',\n",
       " 'Increasing\\nthe cache size in StreamingLLM doesnâ€™t con-\\nsistently yield a decrease in perplexity across\\nall models, suggesting these models may\\nnot be fully exploiting the provided context.',\n",
       " 'Cache config x+ydenotes adding xinitial\\ntokens with yrecent tokens.',\n",
       " 'Perplexity is\\nevaluated on 400K tokens in the concatenated\\nPG19 test set.',\n",
       " 'Cache 4+252 4+508 4+1020 4+2044\\nFalcon-7B 13.61 12.84 12.34 12.84\\nMPT-7B 14.12 14.25 14.33 14.99\\nPythia-12B 13.17 12.52 12.08 12.09\\nCache 4+508 4+1020 4+2044 4+4092\\nLlama-2-7B 9.73 9.32 9.08 9.594.5 E FFICENCY RESULTS\\nWe benchmark its decoding latency and memory us-\\nage against the sliding window with re-computation,\\nwhich is the only baseline with acceptable per-\\nformance.',\n",
       " 'Both methods are implemented using\\nthe Huggingface Transformers library (Wolf et al.,\\n2020) and tested on a single NVIDIA A6000 GPU\\nusing the Llama-2-7B and Llama-2-13B models.',\n",
       " 'As depicted in Figure 10, as the cache size in-\\ncreases, StreamingLLMâ€™s decoding speed demon-\\nstrates a linear growth.',\n",
       " 'The sliding window with\\nre-computation baseline has a quadratic rise in de-\\ncoding latency.',\n",
       " 'Thus, StreamingLLM achieves an\\nimpressive speedup, reaching up to 22.2 Ã—per token.',\n",
       " 'Despite its reduced latency, StreamingLLM sustains a\\nmemory footprint consistent with the re-computation\\nbaseline.',\n",
       " '5 C ONCLUSION\\nDeploying LLMs in streaming applications is urgently needed but comes with challenges due to\\nefficiency limitations and reduced performance with longer texts.',\n",
       " 'Window attention provides a partial\\nsolution, but its performance plummets when initial tokens are excluded.',\n",
       " 'Recognizing the role of\\nthese tokens as â€œattention sinks\", we introduced StreamingLLM â€”a simple and efficient framework\\nthat enables LLMs to handle unlimited texts without fine-tuning.',\n",
       " 'By adding attention sinks with\\nrecent tokens, StreamingLLM can efficiently model texts of up to 4 million tokens.',\n",
       " 'We further\\nshow that pre-training models with a dedicated sink token can improve the streaming performance.',\n",
       " 'StreamingLLM firstly decouples the LLMâ€™s pre-training window size and its actual text generation\\nlength, paving the way for the streaming deployment of LLMs.',\n",
       " '9\\n\\n============================== End of Page 9 ==============================\\n\\nPreprint\\nREFERENCES\\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Co-\\njocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic,\\nBadreddine Noune, Baptiste Pannier, and Guilherme Penedo.',\n",
       " 'Falcon-40B: an open large language\\nmodel with state-of-the-art performance.',\n",
       " '2023.',\n",
       " 'Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, and Thomas\\nHofmann.',\n",
       " 'Dynamic context pruning for efficient and interpretable autoregressive transformers,\\n2023.',\n",
       " 'Iz Beltagy, Matthew E. Peters, and Arman Cohan.',\n",
       " 'Longformer: The long-document transformer,\\n2020. arXiv:2004.05150.',\n",
       " 'Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle Oâ€™Brien, Eric Hallahan,\\nMohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron,\\nLintang Sutawika, and Oskar van der Wal.',\n",
       " 'Pythia: A suite for analyzing large language models\\nacross training and scaling, 2023.',\n",
       " 'Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi.',\n",
       " 'Piqa: Reasoning\\nabout physical commonsense in natural language.',\n",
       " 'In Thirty-Fourth AAAI Conference on Artificial\\nIntelligence , 2020.\\nbloc97.',\n",
       " 'NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size\\nwithout any fine-tuning and minimal perplexity degradation., 2023.',\n",
       " 'URL https://www.reddit.com/\\nr/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/.',\n",
       " 'Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.',\n",
       " 'Quantizable transformers: Removing\\noutliers by helping attention heads do nothing, 2023.',\n",
       " 'Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.',\n",
       " 'Language models are\\nfew-shot learners.',\n",
       " 'Advances in neural information processing systems , 33:1877â€“1901, 2020.',\n",
       " 'Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\\nChantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex Nichol, Alex Paino,\\nNikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\\nChristopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,\\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob\\nMcGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.',\n",
       " 'Evaluating\\nlarge language models trained on code, 2021.',\n",
       " 'Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.',\n",
       " 'Extending context window of\\nlarge language models via positional interpolation, 2023. arXiv: 2306.15595.',\n",
       " 'Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.',\n",
       " 'Vicuna:\\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.',\n",
       " 'URL https:\\n//lmsys.org/blog/2023-03-30-vicuna/.',\n",
       " 'Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\\nOyvind Tafjord.',\n",
       " 'Think you have solved question answering?',\n",
       " 'try arc, the ai2 reasoning challenge.',\n",
       " 'arXiv:1803.05457v1 , 2018.',\n",
       " 'Tri Dao.',\n",
       " 'FlashAttention-2: Faster attention with better parallelism and work partitioning.',\n",
       " '2023.',\n",
       " 'Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©.',\n",
       " 'FlashAttention: Fast and\\nmemory-efficient exact attention with IO-awareness, 2022. arXiv:2205.14135.',\n",
       " '10\\n\\n============================== End of Page 10 ==============================\\n\\nPreprint\\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\\nHorace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy.',\n",
       " 'The Pile: An 800gb\\ndataset of diverse text for language modeling.',\n",
       " 'arXiv preprint arXiv:2101.00027 , 2020.',\n",
       " 'Tanya Goyal and Greg Durrett.',\n",
       " 'Evaluating factuality in generation with dependency-level entailment.',\n",
       " 'InFindings of the Association for Computational Linguistics: EMNLP 2020 , Online, 2020.',\n",
       " 'Association for Computational Linguistics.',\n",
       " 'kaiokendev.',\n",
       " 'Things Iâ€™m learning while training superhot., 2023.',\n",
       " 'URL https://kaiokendev.github.io/\\ntil#extending-context-to-8k.',\n",
       " 'Ehsan Kamalloo, Nouha Dziri, Charles L. A. Clarke, and Davood Rafiei.',\n",
       " 'Evaluating open-domain\\nquestion answering in the era of large language models, 2023.',\n",
       " 'Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.',\n",
       " 'Reformer: The efficient transformer.',\n",
       " 'In 8th\\nInternational Conference on Learning Representations, ICLR 2020 .',\n",
       " 'OpenReview.net, April 2020.',\n",
       " 'Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica,\\nXuezhe Ma, , and Hao Zhang.',\n",
       " 'How long can open-source llms truly promise on context length?,\\nJune 2023.',\n",
       " 'URL https://lmsys.org/blog/2023-06-29-longchat.',\n",
       " 'Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\\nPercy Liang.',\n",
       " 'Lost in the middle: How language models use long contexts, 2023.',\n",
       " 'Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.',\n",
       " 'Can a suit of armor conduct\\nelectricity?',\n",
       " 'a new dataset for open book question answering.',\n",
       " 'In EMNLP , 2018.',\n",
       " 'Evan Miller.',\n",
       " 'Attention is off by one, 2023.',\n",
       " 'URL https://www.evanmiller.org/attention-is-off-by-one.',\n",
       " 'html.',\n",
       " 'OpenAI.',\n",
       " 'Gpt-4 technical report, 2023.',\n",
       " 'Denis Paperno, GermÃ¡n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi,\\nSandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel FernÃ¡ndez.',\n",
       " 'The LAMBADA dataset:\\nWord prediction requiring a broad discourse context.',\n",
       " 'In Proceedings of the 54th Annual Meeting\\nof the Association for Computational Linguistics (Volume 1: Long Papers) , pp.',\n",
       " '1525â€“1534, Berlin,\\nGermany, August 2016.',\n",
       " 'Association for Computational Linguistics.',\n",
       " 'doi: 10.18653/v1/P16-1144.',\n",
       " 'URL https://aclanthology.org/P16-1144.',\n",
       " 'Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.',\n",
       " 'Yarn: Efficient context window\\nextension of large language models, 2023.',\n",
       " 'Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm\\nLevskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean.',\n",
       " 'Efficiently scaling\\ntransformer inference.',\n",
       " 'arXiv preprint arXiv:2211.05102 , 2022.',\n",
       " 'Ofir Press, Noah Smith, and Mike Lewis.',\n",
       " 'Train short, test long: Attention with linear biases enables\\ninput length extrapolation.',\n",
       " 'In International Conference on Learning Representations , 2022.',\n",
       " 'URL\\nhttps://openreview.net/forum?id=R8sQPpGCv0.',\n",
       " 'Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.',\n",
       " 'Improving language\\nunderstanding by generative pre-training.',\n",
       " '2018.',\n",
       " 'Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap.',\n",
       " 'Compressive transformers for long-range sequence modelling.',\n",
       " 'In International Conference on\\nLearning Representations , 2020.',\n",
       " 'Baptiste RoziÃ¨re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\\nAdi, Jingyu Liu, Tal Remez, JÃ©rÃ©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton,\\nManish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre DÃ©fossez, Jade\\nCopet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel\\nSynnaeve.',\n",
       " 'Code Llama: Open foundation models for code, 2023.',\n",
       " '11\\n\\n============================== End of Page 11 ==============================\\n\\nPreprint\\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.',\n",
       " 'Winogrande: An\\nadversarial winograd schema challenge at scale.',\n",
       " 'arXiv preprint arXiv:1907.10641 , 2019.',\n",
       " 'John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Fe-\\nlipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al.',\n",
       " 'Chatgpt: Optimizing language\\nmodels for dialogue.',\n",
       " 'OpenAI blog , 2022.',\n",
       " 'Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu.',\n",
       " 'Roformer: Enhanced\\ntransformer with rotary position embedding.',\n",
       " 'arXiv preprint arXiv:2104.09864 , 2021.',\n",
       " 'Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\\nLiang, and Tatsunori B. Hashimoto.',\n",
       " 'Stanford alpaca: An instruction-following llama model.',\n",
       " 'https://github.com/tatsu-lab/stanford_alpaca, 2023.',\n",
       " 'MosaicML NLP Team.',\n",
       " 'Introducing mpt-7b: A new standard for open-source, commercially usable\\nllms, 2023.',\n",
       " 'URL www.mosaicml.com/blog/mpt-7b.',\n",
       " 'Accessed: 2023-05-05.',\n",
       " 'Together.',\n",
       " 'Llama-2-7b-32k-instruct â€” and fine-tuning for llama-2 models with together api, June\\n2023.',\n",
       " 'URL https://together.ai/blog/llama-2-7b-32k-instruct.',\n",
       " 'Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e\\nLacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.',\n",
       " 'Llama: Open and\\nefficient foundation language models.',\n",
       " 'arXiv preprint arXiv:2302.13971 , 2023a.',\n",
       " 'Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.',\n",
       " 'Llama 2: Open foundation\\nand fine-tuned chat models.',\n",
       " 'arXiv preprint arXiv:2307.09288 , 2023b.',\n",
       " 'Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma.',\n",
       " 'Linformer: Self-attention with\\nlinear complexity.',\n",
       " '2020.',\n",
       " 'Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\\nPierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von\\nPlaten, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama\\nDrame, Quentin Lhoest, and Alexander M. Rush.',\n",
       " 'Huggingfaceâ€™s transformers: State-of-the-art\\nnatural language processing, 2020.',\n",
       " 'Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.',\n",
       " 'SmoothQuant:\\nAccurate and efficient post-training quantization for large language models.',\n",
       " 'In Proceedings of the\\n40th International Conference on Machine Learning , 2023.',\n",
       " 'Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\\nOntaÃ±Ã³n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed.',\n",
       " 'Big bird:\\nTransformers for longer sequences.',\n",
       " 'In Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell,\\nMaria-Florina Balcan, and Hsuan-Tien Lin (eds.',\n",
       " '), Advances in Neural Information Processing\\nSystems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020 .',\n",
       " 'Curran Associates, Inc., 2020.',\n",
       " 'Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.',\n",
       " 'Hellaswag: Can a machine\\nreally finish your sentence?',\n",
       " 'CoRR , abs/1905.07830, 2019.',\n",
       " 'URL http://arxiv.org/abs/1905.07830.',\n",
       " 'Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt\\nShuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.',\n",
       " 'Opt: Open pre-trained transformer language models, 2022.',\n",
       " 'Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B.\\nHashimoto.',\n",
       " 'Benchmarking large language models for news summarization, 2023a.',\n",
       " 'Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song,\\nYuandong Tian, Christopher RÃ©, Clark Barrett, Zhangyang Wang, and Beidi Chen.',\n",
       " 'H 2o: Heavy-\\nhitter oracle for efficient generative inference of large language models, 2023b.',\n",
       " '12\\n\\n============================== End of Page 12 ==============================']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_raw.loc[df_raw.Source == 'PDF', 'sentence_tokens'][23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In the name of the Flowers, the Kobe, and the Holy Phreak\\n\\n\\nOur Father, who art in heaven this day.',\n",
       " \"We pray a Coup d'Ã©tat of our sweet NA.\",\n",
       " 'We have Palafaker and APA, Summit too!',\n",
       " 'Please, this year, let us have a World finals debut.',\n",
       " \"Let BDS scratch their heads as they blunder\\n\\n\\nBless us with throws as the East stares in wonder\\n\\n\\nWe pray for NRG and their underdog buff\\n\\n\\nHelp GGS's veterans with their mental so tough\\n\\n\\nWe pray for Cloud9, the copium cure\\n\\nTL's Pyosik has returned for a victory tour\\n\\n\\nGive APA the strength to become a god killer\\n\\n\\nThrough play-ins or groups, as they smash any pillar\\n\\n\\nFor its with NRG, TL, GGS, & C9\\n\\n\\nThis year will be different for our boys of NA\\n\\n\\nReddit, please say it with me.\",\n",
       " 'We pray.',\n",
       " 'NAmen']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_raw.loc[df_raw.Source == 'Reddit']['sentence_tokens'][11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns median sentence len in # of tokens\n",
    "def tokenize(sentence):\n",
    "    len_ls = []\n",
    "    for i in range(len(sentence)):\n",
    "        # tokenize and remove punctuations\n",
    "        tokens = word_tokenize(sentence[i])\n",
    "        new_tokens = no_punc(tokens)\n",
    "        len_ls.append(len(new_tokens))\n",
    "    \n",
    "    return round(statistics.median(len_ls))\n",
    "\n",
    "# tokenize(df_raw.sentence_tokens[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['median_sent_len'] = df_raw.sentence_tokens.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of sentence length (number of tokens)')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "sns.kdeplot(data=df_raw, x='median_sent_len', hue='Source', alpha = 0.5, fill = True)\n",
    "plt.xlabel('Length of sentence (number of tokens)')\n",
    "plt.ylabel('Number of tokens')\n",
    "plt.title('Distribution of sentence length (number of tokens)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging\n",
    "\n",
    "Randomly select 3 sentences from each dataset, and apply POS tagging. Discuss the POS tagging results. Are the results as expected? Can the POS tagger well handle the domain specific terms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google_News\n",
      "rand_doc: 3\n",
      "rand_sen: 6\n",
      "Israel says around 1,500 Hamas fighters were killed in clashes before its army regained control of the area under attack.\n",
      "==========\n",
      "Google_News\n",
      "rand_doc: 5\n",
      "rand_sen: 6\n",
      "The police on Saturday said they were \"aware of a social media post showing a man holding onto a sign relating to the Israel-Hamas conflict outside the Speakersâ€™ Corner, and investigations are ongoing\".\n",
      "==========\n",
      "Google_News\n",
      "rand_doc: 7\n",
      "rand_sen: 13\n",
      "Supporting oppressed people is human.\n",
      "==========\n",
      "Reddit\n",
      "rand_doc: 9\n",
      "rand_sen: 2\n",
      "Best time of the year to like League.\n",
      "==========\n",
      "Reddit\n",
      "rand_doc: 2\n",
      "rand_sen: 26\n",
      "At his request, BDS Coaching staff paid thousands of Euro to a pre-school teacher to give Sheo lessons about object permanence.\n",
      "==========\n",
      "Reddit\n",
      "rand_doc: 3\n",
      "rand_sen: 17\n",
      "We were given the key by the casters over and over again to decypher the message.\n",
      "==========\n",
      "PDF\n",
      "rand_doc: 3\n",
      "rand_sen: 213\n",
      "While the window attention method works\n",
      "efficiently, it exhibits low accuracy due to random outputs when the input length exceeds the cache\n",
      "size.\n",
      "==========\n",
      "PDF\n",
      "rand_doc: 2\n",
      "rand_sen: 196\n",
      "As\n",
      "we observe adjacent games contain similar clues, we use 20 games with indices 1;6;\u0001\u0001\u0001;91;96for\n",
      "testing, and games 136;141;146;151;156for prompting.\n",
      "==========\n",
      "PDF\n",
      "rand_doc: 7\n",
      "rand_sen: 284\n",
      "L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch.\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample from each dataset\n",
    "pos_dic = {'text' : [] , \n",
    "           'doc_no': [], \n",
    "           'sent_no': []}\n",
    "\n",
    "random.seed(111)\n",
    "source_cat = list(df_raw.Source.unique())\n",
    "\n",
    "for cat in source_cat:\n",
    "    data_1 = df_raw.loc[df_raw.Source == cat].reset_index(drop = True)\n",
    "    \n",
    "    max_article_count = data_1.shape[0] # get max no. doc for category\n",
    "    rand_doc_no_ls = [random.randint(0, max_article_count-1) for _ in range(3)] \n",
    "    \n",
    "    for rand_doc_no in rand_doc_no_ls:\n",
    "        data = data_1.loc[rand_doc_no, 'sentence_tokens']\n",
    "\n",
    "        max_doc_sentence = len(data) # max sentence for doc\n",
    "        rand_sent_no = random.randint(0, max_doc_sentence-1) # Which sentence\n",
    "\n",
    "        text = data[rand_sent_no]\n",
    "        \n",
    "        pos_dic['text'].append(text)\n",
    "        pos_dic['doc_no'].append(rand_doc_no)\n",
    "        pos_dic['sent_no'].append(rand_sent_no)\n",
    "        \n",
    "        print(cat)\n",
    "        print(f'rand_doc: {rand_doc_no}')\n",
    "        print(f'rand_sen: {rand_sent_no}')\n",
    "        print(text)\n",
    "        print('==========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_df = pd.DataFrame(pos_dic)\n",
    "pos_df['pos_tagging'] = pos_df.text.apply(lambda x: pos_tag(word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('police', 'NN'), ('on', 'IN'), ('Saturday', 'NNP'), ('said', 'VBD'), ('they', 'PRP'), ('were', 'VBD'), ('``', '``'), ('aware', 'JJ'), ('of', 'IN'), ('a', 'DT'), ('social', 'JJ'), ('media', 'NNS'), ('post', 'NN'), ('showing', 'VBG'), ('a', 'DT'), ('man', 'NN'), ('holding', 'VBG'), ('onto', 'IN'), ('a', 'DT'), ('sign', 'NN'), ('relating', 'VBG'), ('to', 'TO'), ('the', 'DT'), ('Israel-Hamas', 'NNP'), ('conflict', 'NN'), ('outside', 'IN'), ('the', 'DT'), ('Speakers', 'NNP'), ('â€™', 'NNP'), ('Corner', 'NNP'), (',', ','), ('and', 'CC'), ('investigations', 'NNS'), ('are', 'VBP'), ('ongoing', 'VBG'), (\"''\", \"''\"), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_df.pos_tagging[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Israel', 'NNP'), ('says', 'VBZ'), ('around', 'IN'), ('1,500', 'CD'), ('Hamas', 'NNP'), ('fighters', 'NNS'), ('were', 'VBD'), ('killed', 'VBN'), ('in', 'IN'), ('clashes', 'NNS'), ('before', 'IN'), ('its', 'PRP$'), ('army', 'NN'), ('regained', 'VBD'), ('control', 'NN'), ('of', 'IN'), ('the', 'DT'), ('area', 'NN'), ('under', 'IN'), ('attack', 'NN'), ('.', '.')]\n",
      "===============\n",
      "[('The', 'DT'), ('police', 'NN'), ('on', 'IN'), ('Saturday', 'NNP'), ('said', 'VBD'), ('they', 'PRP'), ('were', 'VBD'), ('``', '``'), ('aware', 'JJ'), ('of', 'IN'), ('a', 'DT'), ('social', 'JJ'), ('media', 'NNS'), ('post', 'NN'), ('showing', 'VBG'), ('a', 'DT'), ('man', 'NN'), ('holding', 'VBG'), ('onto', 'IN'), ('a', 'DT'), ('sign', 'NN'), ('relating', 'VBG'), ('to', 'TO'), ('the', 'DT'), ('Israel-Hamas', 'NNP'), ('conflict', 'NN'), ('outside', 'IN'), ('the', 'DT'), ('Speakers', 'NNP'), ('â€™', 'NNP'), ('Corner', 'NNP'), (',', ','), ('and', 'CC'), ('investigations', 'NNS'), ('are', 'VBP'), ('ongoing', 'VBG'), (\"''\", \"''\"), ('.', '.')]\n",
      "===============\n",
      "[('Supporting', 'VBG'), ('oppressed', 'JJ'), ('people', 'NNS'), ('is', 'VBZ'), ('human', 'JJ'), ('.', '.')]\n",
      "===============\n",
      "[('Best', 'JJS'), ('time', 'NN'), ('of', 'IN'), ('the', 'DT'), ('year', 'NN'), ('to', 'TO'), ('like', 'IN'), ('League', 'NNP'), ('.', '.')]\n",
      "===============\n",
      "[('At', 'IN'), ('his', 'PRP$'), ('request', 'NN'), (',', ','), ('BDS', 'NNP'), ('Coaching', 'NNP'), ('staff', 'NN'), ('paid', 'VBD'), ('thousands', 'NNS'), ('of', 'IN'), ('Euro', 'NNP'), ('to', 'TO'), ('a', 'DT'), ('pre-school', 'JJ'), ('teacher', 'NN'), ('to', 'TO'), ('give', 'VB'), ('Sheo', 'NNP'), ('lessons', 'NNS'), ('about', 'IN'), ('object', 'JJ'), ('permanence', 'NN'), ('.', '.')]\n",
      "===============\n",
      "[('We', 'PRP'), ('were', 'VBD'), ('given', 'VBN'), ('the', 'DT'), ('key', 'NN'), ('by', 'IN'), ('the', 'DT'), ('casters', 'NNS'), ('over', 'IN'), ('and', 'CC'), ('over', 'RB'), ('again', 'RB'), ('to', 'TO'), ('decypher', 'VB'), ('the', 'DT'), ('message', 'NN'), ('.', '.')]\n",
      "===============\n",
      "[('While', 'IN'), ('the', 'DT'), ('window', 'NN'), ('attention', 'NN'), ('method', 'NN'), ('works', 'VBZ'), ('efficiently', 'RB'), (',', ','), ('it', 'PRP'), ('exhibits', 'VBZ'), ('low', 'JJ'), ('accuracy', 'NN'), ('due', 'JJ'), ('to', 'TO'), ('random', 'VB'), ('outputs', 'NNS'), ('when', 'WRB'), ('the', 'DT'), ('input', 'NN'), ('length', 'NN'), ('exceeds', 'VBZ'), ('the', 'DT'), ('cache', 'NN'), ('size', 'NN'), ('.', '.')]\n",
      "===============\n",
      "[('As', 'IN'), ('we', 'PRP'), ('observe', 'VBP'), ('adjacent', 'JJ'), ('games', 'NNS'), ('contain', 'VBP'), ('similar', 'JJ'), ('clues', 'NNS'), (',', ','), ('we', 'PRP'), ('use', 'VBP'), ('20', 'CD'), ('games', 'NNS'), ('with', 'IN'), ('indices', 'NNS'), ('1', 'CD'), (';', ':'), ('6', 'CD'), (';', ':'), ('\\x01\\x01\\x01', 'NNP'), (';', ':'), ('91', 'CD'), (';', ':'), ('96for', 'CD'), ('testing', 'NN'), (',', ','), ('and', 'CC'), ('games', 'NNS'), ('136', 'CD'), (';', ':'), ('141', 'CD'), (';', ':'), ('146', 'CD'), (';', ':'), ('151', 'CD'), (';', ':'), ('156for', 'CD'), ('prompting', 'NN'), ('.', '.')]\n",
      "===============\n",
      "[('L.', 'NNP'), ('Chen', 'NNP'), (',', ','), ('K.', 'NNP'), ('Lu', 'NNP'), (',', ','), ('A.', 'NNP'), ('Rajeswaran', 'NNP'), (',', ','), ('K.', 'NNP'), ('Lee', 'NNP'), (',', ','), ('A.', 'NNP'), ('Grover', 'NNP'), (',', ','), ('M.', 'NNP'), ('Laskin', 'NNP'), (',', ','), ('P.', 'NNP'), ('Abbeel', 'NNP'), (',', ','), ('A.', 'NNP'), ('Srinivas', 'NNP'), (',', ','), ('and', 'CC'), ('I.', 'NNP'), ('Mordatch', 'NNP'), ('.', '.')]\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "for i in range(pos_df.shape[0]):\n",
    "    print(pos_df.pos_tagging[i])\n",
    "    print('===============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_df.to_csv('POS_Tagging.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
